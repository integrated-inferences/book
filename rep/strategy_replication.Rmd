---
title: "Case selection strategies"
date: "2/1/2025 replication"
output: 
  html_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source("../helpers/_tools.R")
library(CausalQueries)
library(tidyverse)
library(knitr)
options(mc.cores = parallel::detectCores())

output_wd   <- "../saved/"

run_test <- FALSE
run_wide_deep <- FALSE
run_selection <- FALSE
trim <- TRUE

# wide_deep controls
path_wd   <- "wd_2025"
n_large = c(100, 400, 1600) 
n_small = c(0, 50, 100)
n_sims = 50
times = 10
n_chains <- 8
n_iter   <- 5000

# For small runs
# n_large=c(5, 10) 
# n_small = c(0, 5)
# n_sims = 3
# times = 2
# n_chains <- 2
# n_iter   <- 1000

# Case selection controls
path_cs    <- "cs_2025"
cs_iter = 12000
cs_chains = 16
cs_models <- TRUE
cs_subset <- TRUE 

# For small runs
# cs_iter = 4000
# cs_chains = 4
# cs_models <- 1:2
# cs_subset <- 1:2 

```

# Helpers

## Case selection helpers

```{r}
weighted.var <- function(x, w = rep(1, length(x))) {
  # No degrees of freedom adjustment as we are dealing with population
    if(length(x)==1) return(0)
    if(all(is.na(x))) return(NA)
    w <- w/sum(w)
    sum(w*(x - sum(w*x))^2)
}

p_M <- function(model)
  model$stan_objects$event_probabilities %>% data.frame %>%
    rename(
      X0M0Y0 = contains("X0") & contains("Y0") & contains("M0"),
      X0M1Y0 = contains("X0") & contains("Y0") & contains("M1"),
      X0M0Y1 = contains("X0") & contains("Y1") & contains("M0"),
      X0M1Y1 = contains("X0") & contains("Y1") & contains("M1"),
      X1M0Y0 = contains("X1") & contains("Y0") & contains("M0"),
      X1M1Y0 = contains("X1") & contains("Y0") & contains("M1"),
      X1M0Y1 = contains("X1") & contains("Y1") & contains("M0"),
      X1M1Y1 = contains("X1") & contains("Y1") & contains("M1")) %>% 
    transmute(
      p_M_00 = X0M1Y0/(X0M0Y0+X0M1Y0),
      p_M_01 = X0M1Y1/(X0M0Y1+X0M1Y1),
      p_M_10 = X1M1Y0/(X1M0Y0+X1M1Y0),
      p_M_11 = X1M1Y1/(X1M0Y1+X1M1Y1))
  
caseload <- function(expanded_df)
  expanded_df %>% filter(!is.na(M)) %>% 
  mutate(type = paste0("p_M_", X,Y)) %>%
  group_by(type) %>%
  summarize(n = n(), m = sum(M))

prob_df <- function(df, model){
  expanded_df <- expand_data(df, model)
  cases <- caseload(expanded_df)
  probs <- p_M(model)
  
  # If no data on M
  if(mean(!is.na(expanded_df$M))==0) return(1)
  
  # Get binomial probability for each cell
  ps <- lapply(cases$type, function(cell) {
    n <- cases %>% filter(type == cell) %>% pull(n)
    m <- cases %>% filter(type == cell) %>% pull(m)
    dbinom(m, n, probs[[cell]])    
  })
  
  # Take mean of product
  Reduce(f = "*", ps) %>% mean
  }

# Get a compact data from a possible data list
grab_data <- function(possible_data, j) 
  possible_data %>% select(event, strategy, j+2) %>% rename(count = 3) |>
  mutate(count = as.integer(count))


data_description <- function(d)
  filter(d, !is.na(M)) %>% 
  mutate(type = paste0(M, "|",X,Y)) %>% pull(type) %>% 
  paste(collapse = ", ") 



# Update, implement queries and calculate probability of the M data
comparisons <- function(model, data, query,  ...) 
  
  query_model(
    update_model(model, data, data_type = "compact",  ...),
    query =  query,
    using = "posteriors") %>%
    
    mutate(prob = prob_df(data, model),
           data = data_description(expand_data(data, model))) 

# Add empty XMY rows to compact data given long XY data
augment_observed_df <- function(df_long, model)
  make_events(model, n = 0, include_strategy = TRUE) |>
  rbind(collapse_data(df_long, model)) |>
  mutate(count = as.integer(count))

```

# Dimensions

## Model list

```{r}
model_list <- list(
  
chain_model = make_model("X -> M -> Y") %>% set_priors(distribution = "jeffreys") %>%
  set_priors(node = "X", alphas = 100)  %>%
  set_prior_distribution() %>% 
  set_parameter_matrix(),

chain_model_monotonic = make_model("X -> M -> Y") %>% set_priors(distribution = "jeffreys") %>%
  set_restrictions("M[X=1] < M[X=0]") %>%
  set_restrictions("(Y[M=1] < Y[M=0])") %>%
  set_priors(node = "X", alphas = 100)  %>%
  set_prior_distribution() %>% 
  set_parameter_matrix(),

moderator_model = make_model("X -> Y <- M") %>% 
  set_priors(distribution = "jeffreys") %>%
  set_priors(node = "X", alphas = 100) %>%
  set_priors(node = "M", alphas = 100) %>%
  set_prior_distribution() %>% set_parameter_matrix(),

moderator_model_monotonic = make_model("X -> Y <- M") %>% 
  set_priors(distribution = "jeffreys") %>%
  set_priors(node = "X", alphas = 100) %>%
  set_priors(node = "M", alphas = 100) %>%
  set_restrictions("(Y[M=1] < Y[M=0])") %>%
  set_restrictions("(Y[X=1] < Y[X=0])") %>%
  set_prior_distribution() %>% set_parameter_matrix(),

two_path_model = make_model("X -> M -> Y <- X") %>% 
  set_priors(distribution = "jeffreys") %>%
  set_priors(node = "X", alphas = 100) %>%
  set_prior_distribution() %>% 
  set_parameter_matrix(),

two_path_model_monotonic = make_model("X -> M -> Y <- X") %>%
  set_priors(distribution = "jeffreys") %>%
  set_priors(node = "X", alphas = 100) %>%
  set_restrictions("(M[X=1] < M[X=0])") %>%
  set_restrictions("(Y[M=1] < Y[M=0])") %>%
  set_restrictions("(Y[X=1] < Y[X=0])") %>%
  set_prior_distribution() %>% 
  set_parameter_matrix(),

confounded_model = make_model("M -> X -> Y <- M") %>% 
  set_priors(distribution = "jeffreys") %>%
  set_priors(node = "M", alphas = 100) %>%
  set_prior_distribution() %>% 
  set_parameter_matrix(),

confounded_model_monotonic = make_model("M -> X -> Y <- M") %>%
  set_priors(distribution = "jeffreys") %>%
  set_priors(node = "M", alphas = 100) %>%
  set_restrictions("(X[M=1] < X[M=0])") %>%
  set_restrictions("(Y[M=1] < Y[M=0])") %>%
  set_restrictions("(Y[X=1] < Y[X=0])") %>%
  set_prior_distribution() %>% 
  set_parameter_matrix()

)
```

## Query lists

```{r}
query_list <- list(
  ATE    = "Y[X=1] - Y[X=0]", 
  PC_11  = "Y[X=1] - Y[X=0] :|: X == 1 & Y == 1", 
  PC_01  = "Y[X=1] - Y[X=0] :|: X == 0 & Y == 1",
  via_M  =  "Y[X=0, M=M[X=1]]==Y[X=1, M=M[X=1]] :|: (X == 1 & Y == 1) & (Y[X=1]>Y[X=0])"
  )


query_labs <- 
  c(ATE = "ATE", 
    PC_11 = "Prob. X=1 \n caused Y = 1", 
    PC_01 = "Prob. X=0 \n caused Y = 1", 
    via_M = "Probability effect of X\n on Y went via M"
)

query_names <- names(query_labs)



```


## Data strategy lists

```{r}
# For case selection

make_data_list <- function(observed, model) {
  
  list(
  
  on_11 = make_possible_data(model, observed = observed, N = 1, 
                             conditions = "X==1 & Y==1", withins = TRUE, vars = "M"),
  
  off_10 = make_possible_data(model, observed = observed, N = 1, 
                              conditions = "X==1 & Y==0", withins = TRUE, vars = "M"),
  
  on_00_11 = make_possible_data(model, observed = observed, N = list(1,1), 
                                conditions = c("X==0 & Y==0", "X==1 & Y==1"), withins = TRUE, vars = "M"),
  
  off_01_10 = make_possible_data(model, observed = observed, N = list(1,1), 
                                 conditions = c("X==0 & Y==1", "X==1 & Y==0"), withins = TRUE, vars = "M"),
  
  XY_11_11 = make_possible_data(model, observed = observed, N = 2, 
                                conditions = "X==1 & Y==1", withins = TRUE, vars = "M"),
    
  onX1_10_11 = make_possible_data(model, observed = observed, N = list(1,1), 
                                  conditions = c("X==1 & Y==0", "X==1 & Y==1"), withins = TRUE, vars = "M"),
  
  onY1_01_11 = make_possible_data(model, observed = observed, N = list(1,1), 
                                  conditions = c("X==0 & Y==1", "X==1 & Y==1"), withins = TRUE, vars = "M")
  ) |>
    # Need to ensure these are integers
    lapply(function(df) {
      df %>% mutate(across(where(~ all(. %in% c(0, 1))), as.integer))
})
}

strategy_labels = c("prior" = "prior", 
                    "off_10" = "1 off (10)", 
                    "on_11" ="1 on (11)", 
                    "off_01_10" = "2 off (01,10)", 
                    "on_00_11" = "2 on (00, 11)", 
                    "XY_11_11" = "2 pos (11, 11)",
                    "onX1_10_11" = "fix X (10, 11)", 
                    "onY1_01_11" =  "fix Y (01, 11)")

df_long <-   data.frame(X=c(1,1,1,0,0,0), Y=c(1,1,0,1,0,0))

observed_df <- collapse_data(df_long, 
                             model = make_model("X -> M -> Y"),
                             drop_family = TRUE)


# Illustration;  XY_11_11 looks inside the two 11 cases

observed_df

data_list <- make_data_list(observed_df, make_model("X -> M -> Y"))
data_list$XY_11_11[1:12,]


augmented_observed_df <- augment_observed_df(df_long, make_model("X -> M -> Y"))
augmented_observed_df
```


# Case selection

## Main function 

Take a model list and data; generate possible data from list; keep mean, sd etc for each query for each possible data realization for each model and each strategy

```{r}

inferences <-
  
  function(model_list, 
           observed_df_long = df_long,  data_function = make_data_list, 
           subset = TRUE, ...)
    
    model_list %>%
  
     lapply(function(model) {
       
      augmented_observed <- augment_observed_df(observed_df_long, model)
      observed_compact   <- collapse_data(observed_df_long, model, drop_family = TRUE)
    
      # types of possible data given model and observed data
      # these may not vary by model but could if models have restrictions 
      data_list <- data_function(observed_compact, model)[subset]
      
      print(model$statement)
      # print(data_list)

      model <- update_model(model, augmented_observed, keep_event_probabilities = TRUE, ...)
      
      c(prior = list(augmented_observed), data_list) |>
        
      lapply(function(poss_data) {
               lapply(1:(ncol(poss_data) - 2), function(j) {
                 df <- grab_data(poss_data, j)
                 comparisons(model, df, query_list,  ...) %>%
                   mutate(data_realization = j)
        }) %>%
          bind_rows()
      }) %>%
        bind_rows(.id = "strategy")
    }
  ) %>%
  bind_rows(.id = "model") 


```

## Illustration

```{r, eval = TRUE}

if(run_test) 
illustration <- inferences(model_list = list(make_model("X -> Y <- M")), 
                           subset = 1)

# smaller run
if(run_test) 
  inferences(model_list[1:2], subset = 1:2) |> 
  write_rds(paste0(path_cs, "/illustration_cs.rds"))

illustration_cs <- read_rds(paste0(path_cs, "/illustration_cs.rds"))

illustration_cs %>%
  mutate(
    Model = model, 
    mean = ifelse(label == "PC_01", -mean, mean),
    query = factor(label, names(query_labs), query_labs),
    strategy = factor(strategy, 
                      names(strategy_labels),
                      strategy_labels),
    model_group = gsub("_monotonic", "", Model),
    monotonic = ifelse(grepl("monotonic", Model), "Monotonic", "Unconstrained")) |>
  ggplot(aes(strategy, mean)) + 
  geom_point(aes(colour = prob),size = 2) +
  geom_point(shape = 1,size = 2,colour = "black") +
  ggh4x::facet_nested(query ~ model_group + monotonic, scales = "free_y") +
  xlab("Strategy")  +
    ylab("Inferences") + theme_bw()  + 
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  scale_colour_gradient(low = "white", high = "black")


```



## Case selection Implementation

```{r}

if(run_selection)
  inferences(model_list[cs_models], subset = cs_subset, iter = cs_iter, chains = cs_chains, refresh = 0)  |>
  write_rds(paste0(path_cs, "/case_selection_data.rds")) 
```



## Output

```{r results, fig.height = 9, fig.width = 8, fig.cap = "Inferences given observations"} 

all_results <- 
  read_rds(paste0(path_cs, "/case_selection_data.rds"))  


all_results <-   
all_results %>%
  mutate(
    Model = model, 
    mean = ifelse(label == "PC_01", -mean, mean),
    label = factor(label, names(query_labs), query_labs),
    strategy = factor(strategy, 
                      names(strategy_labels),
                      strategy_labels), 
    model_group = gsub("_monotonic", "", Model),
    monotonic = ifelse(grepl("monotonic", Model), "Monotonic", "Unconstrained"),
    monotonic = factor(monotonic, c("Unconstrained", "Monotonic"))) 
  

case_selection_1 <- 
all_results %>% 
ggplot(aes(strategy, mean)) + 
  geom_point(aes(colour = prob),size = 2) +
  geom_point(shape = 1,size = 2,colour = "black") +
  ggh4x::facet_nested(label ~ model_group + monotonic, scales = "free_y") +
  xlab("Strategy")  +
  ylab("Inferences") + theme_bw()  + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  scale_colour_gradient(low = "white", high = "black") + 
  theme(legend.position="bottom")

case_selection_1

write_rds(case_selection_1, paste0(output_wd, "13_case_selection_1.rds"))


```

## Variance reduction

```{r varresults, fig.height = 3, fig.width = 8, fig.cap = "Reduction in variance on ATE given strategies"} 


expected_var <- 

all_results %>% 
  group_by(label, model_group,  monotonic, strategy) %>% 
  summarize(check_prob = sum(prob), n = n(), 
            #weighted_var = Hmisc::wtd.var(mean, prob),
            weighted_var = weighted.var(mean, prob),
            expected_posterior_var = weighted.mean(sd^2, prob),
            expected_posterior_mean = weighted.mean(mean, prob), .groups = 'drop')  %>%
  group_by(model_group, monotonic, label) %>% 
  mutate(prior_var = expected_posterior_var[1]) %>% ungroup %>%
  mutate(expected_learning = 100*(1-expected_posterior_var/prior_var),
         expected_learning_2 = 100*(weighted_var/prior_var))%>%
  mutate(expected_learning = ifelse(prior_var==0, 0, expected_learning),
         expected_learning_2 = ifelse(prior_var==0, 0, expected_learning_2))

expected_var %>%
  dplyr::filter(label=="ATE" & model_group == "chain_model" & monotonic == "Unconstrained") %>% 
  ggplot(aes(expected_learning, expected_learning_2)) + 
  geom_point()
```


```{r, eval = FALSE}

expected_var %>% dplyr::filter(label == "ATE") %>%  
  select(-check_prob, -n, - label, -expected_posterior_mean) %>% kable(caption = "posterior variance on the ATE", digits = 4)

expected_var %>% 
  dplyr::filter(label == "ATE") %>%
  ggplot(aes(strategy, expected_learning_2)) + 
  geom_point() +
   facet_grid(. ~ model_group) + 
 xlab("Strategy")  +
    ylab("Expected reduction in posterior variance")  +
    theme_bw() +
  geom_hline(yintercept = 0, color = "red") + 
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))  
```


## Main reduction in variance table

```{r varresults2, fig.height = 11, fig.width = 8, fig.cap = "Reduction in variance given strategies"} 

case_selection_2 <-
  
expected_var %>% 
  ggplot(aes(strategy, expected_learning_2)) + 
  geom_point() +
   ggh4x::facet_nested(label ~ model_group + monotonic, scales = "free_y") + 
 xlab("Strategy")  +
    ylab("Reduction in variance")  +
    theme_bw() +
#  geom_hline(yintercept = 0, color = "red") + 
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))  + coord_trans(y="sqrt") + ylim(0, NA)

case_selection_2 

write_rds(case_selection_2, paste0(output_wd, "13_case_selection_2.rds"))
```



# Wide or Deep

## WD Helpers

```{r}

comparisons_wd <- function(model, data, query,  ...)
  update_model(model, data,  ...) %>%
  query_model(query = query, using = "posteriors")

make_dfs_helper <- 
  
  function(model, n_large, n_small, 
           n_sims=2, param_type = 'prior_draw') {
  
  schedule <- expand.grid(n_large, n_small)
  
  dfs <- apply(schedule, 1, function(j) 
    lapply(1:n_sims, function(i) 
      make_data(model, n_steps = j, nodes = list(c("X","Y"), "M"), 
                param_type = param_type, verbose = FALSE)))

  names(dfs) <- apply(schedule, 1, function(j) paste0("dfs_", paste(j, collapse = "-"))) 
  
  dfs
  
  }

# Function to make a distribution of data sets given some given data
# New observations drawn from posterior and combine with old observations
make_dfs <- function(model,  
                  n_large, 
                  n_small, 
                  n_sims=2, 
                  observed = NULL){
  
  if(is.null(observed)) 
    return(make_dfs_helper(model, n_large = n_large, n_small = n_small, 
                           n_sims=n_sims, param_type = 'prior_draw'))
  
  # Update model once
  model   <- update_model(model, observed) 
  
  # Take multiple new data draws
  new_dfs <- make_dfs_helper(model, n_large, n_small, n_sims, 
                             param_type = 'posterior_draw')
  
  # Add in original data
  lapply(new_dfs, function(dfs) lapply(dfs, function(df) bind_rows(df, observed)))
}


```




## Main WD function

```{r}

wide_or_deep <- 
  
  function(model, n_large, n_small, query, 
           n_sims=3, refresh = 0, iter = n_iter, 
           chains = n_chains, observed = NULL, ...){
  
  schedule <- expand.grid(n_large, n_small)

  #  Get all the data sets
  out <- 
    make_dfs(model, n_large,  n_small, n_sims, observed) %>%
    lapply(function(design) {
    lapply(design, function(df) 
      comparisons_wd(model, df, query = query,  
                     refresh = refresh, iter = iter, chains = chains
                     # , ...
                     )) %>%
        
      bind_rows(.id = "sim")}) |>  
    bind_rows(.id = "design") |> 
  mutate(
    wide_n = str_extract(design, "(?<=_)[^\\-]+"),
    deep_n = str_extract(design, "(?<=-)[0-9]+")
  )
  
  out
}



```

## Step by step illustration

```{r}
# Datas drawn from prior distribution (n_sims datasets created)
sample_datas_1 <- make_dfs(model = model_list[[1]], n_large =5, n_small = 2, n_sims=1)
sample_datas_1

# Adds to data given prior observed data; requires updating
if(run_test)
sample_datas_2 <- make_dfs(model = model_list[[1]], n_large =5, n_small = 2, n_sims=1, 
                             observed = data.frame(X=0:1, Y = 0:1))
if(run_test)
sample_datas_2

# Schedule of data sets with multiple draws:
sample_datas_schedule <- make_dfs(model = model_list[[1]], n_large =4:5, n_small = 1:2, n_sims=2)
# Output is a list of lists of simulated datasets for each data combination
df <- sample_datas_schedule$`dfs_4-1`[[2]]
df

# comparisons_wd is implemented on each of these
out <- comparisons_wd(model_list$chain_model, df, query = query_list[1],  refresh = 0)
out

if(run_test)
wide_or_deep(model = model_list$chain_model, n_large = 5, n_small = 1:2, 
             query = query_list[1:2], chains =1, iter = 4000) |>
  write_rds(paste0(path_wd, "/illustration_wd.rds"))

illustration_wd <-  read_rds(paste0(path_wd, "/illustration_wd.rds"))

```

## Implement wide-deep analysis

```{r, eval = TRUE}
model_seq <- 1:length(model_list) 
# model_seq <- 1:2

if(run_wide_deep) {
  for(k in 1:times){
  for(j in model_seq)
  wide_or_deep(model_list[[j]], 
               n_large = n_large, 
               n_small = n_small,  
               n_sims = n_sims, 
               query = query_list) %>% 
      mutate(Model = names(model_list[j])) %>%
      write_rds(paste0(path_wd, "/temp_out/", names(model_list[j]), "_", k, "_", Sys.Date(), "_wd.rds"))
  }
}

```

## Read models

```{r}

files <- list.files(path=paste0(path_wd, "/temp_out"))
#files <- files[grepl("_wd.rds", files, fixed = TRUE)]

results <-  files %>%
  lapply(function(f)     
    read_rds(paste0(paste0(path_wd, "/temp_out"), "/", f)))

names(results) <- gsub(".rds", "",  files)

results <- 
  results %>% 
  bind_rows(.id = "file") |>
  mutate(
      Model = file, 
      
      Model = gsub("_restricted", "_monotonic", Model), 

      Model = ifelse(grepl('chain_model_monotonic_', Model), 
              "Chain monotonic", Model),
      
      Model = ifelse(grepl('chain_model_', Model), 
              "Chain", Model),
  
      Model = ifelse(grepl('moderator_model_monotonic_', Model), 
              "Moderator monotonic", Model),
      
      Model = ifelse(grepl('moderator_', Model), 
              "Moderator", Model),

      Model = ifelse(grepl('confounded_monotonic_', Model), 
              "Confounded monotonic", Model),

      Model = ifelse(grepl('confounded_model_monotonic_', Model), 
              "Confounded monotonic", Model),
            
      Model = ifelse(grepl('confounded_', Model), 
              "Confounded", Model),
  
      Model = ifelse(grepl('two_path_model_monotonic_', Model), 
              "Two path monotonic", Model),

      Model = gsub("chain", "Chain", Model),

      Model = ifelse(grepl('two_path', Model), 
              "Two path", Model),

      Model = gsub("base", "Two path", Model), 
      
      Model = gsub("confounded", "Confounded", Model), 
      
      
      model_group = gsub(" monotonic", "", Model),
      
      Model_2_rows = gsub("monotonic", "\n(monotonic)", Model), 
      
      monotonic = ifelse(grepl("monotonic", Model_2_rows),
                         "Monotonic", "Unconstrained"),
      monotonic = factor(monotonic, c("Unconstrained", "Monotonic")),
      
    model_group = gsub(" monotonic", "", Model)) 

results$model_group %>% table
results$Model %>% table




# Suggests variation coming from some extremes
results %>%
   filter(Model == "Chain" & label == "ATE" & deep_n ==100 & wide_n == 1600) |>
  mutate(t = scale(sd^2)) |>
   ggplot(aes(t)) + 
   geom_histogram() + facet_wrap(file ~.)

before_trim <- dim(results)[1]
# Remove top .5% in each set
if(trim)
results <- 
  results |>
  group_by(label, wide_n, deep_n, Model) |>
  mutate(
    tops = quantile(sd^2, .995, na.rm = TRUE), 
    score = scale(sd), 
    n = n(),
    to_keep_1 = sd^2 <= tops,
    to_keep_2 = abs(score) < 5 ) |>
  ungroup()

# to keep 2 has a lot of missingness becuase of missing sds?

table(results$to_keep_1, results$to_keep_2)  
table(results$to_keep_2)  

results <- results |> filter(to_keep_1)

after_trim <- dim(results)[1]

c(before_trim, after_trim)
after_trim/before_trim
```


## Display wide-deep results


```{r, fig.width = 14, fig.height = 14}

short <- results %>% 
  mutate(
    Wide = factor(wide_n, unique(as.numeric(wide_n)) %>% sort, 
                  unique(as.numeric(wide_n)) %>% sort),
    deep_n = as.numeric(deep_n)) %>%
  group_by(model_group, monotonic, deep_n, Wide, query, label) %>% 
  dplyr::summarize(
    mean_post_var = round(mean(sd^2), digits = 6),
    se_of_post_var = round((var(sd^2)/n())^.5, digits = 6),
    lower = mean_post_var - 1.96*se_of_post_var,
    upper = mean_post_var + 1.96*se_of_post_var)  
```

```{r, fig.width = 14, fig.height = 14}

wide_deep_fig_book <- 
  short %>%  
  group_by(label, model_group, monotonic) |>
  mutate(prior_var = mean_post_var[deep_n==0 & Wide == 100]) |>
  ungroup() |>
  mutate(reduction = 1 - mean_post_var/prior_var) %>%  
  mutate(monotonic = factor(monotonic, c("Unconstrained", "Monotonic"), c("Unrestricted", "Monotonic" ))) |>  
  mutate(label = factor(
    label, query_names, query_labs)) %>%
  
  ggplot(aes(deep_n, reduction, linetype = Wide)) +
  geom_line() + 
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.05) +
  ggh4x::facet_nested(label ~ model_group + monotonic, scales = "free_y") +
  ylab("Reduction in expected posterior variance") + 
  scale_x_continuous(breaks = c(0, 50, 100)) + theme_bw() +
  xlab("Number of cases with data on M")  + 
  labs(linetype='Number of \n X, Y cases') +
  theme(legend.position="bottom") + ylim(-.02, 1)


wide_deep_fig_book

write_rds(short, paste0(path_wd, "/14_wide_deep_data.rds")) 
write_rds(wide_deep_fig_book, paste0(output_wd, "14_wide_deep.rds"))

```

