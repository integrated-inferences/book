# Mixing Models {#sec-HJC11}

:::{.callout-note title="Chapter summary"}
We show how we can integrate inferences across models. We provide four examples of situations in which, by combining models, researchers can learn more than they could from any single model. Examples include situations in which researchers seek to integrate  inferences from experimental and observational data, seek to learn across settings, or seek to integrate inferences from multiple studies.
::::



```{r}
#| label: packagesused15
#| include: false
source("_setup.R")
# run = TRUE
# library(DeclareDesign)
```

In @sec-HJC9 and @sec-HJC10, we described one form of integration that causal models can enable: the systematic combination of (what we typically think of as) qualitative and quantitative evidence for the purposes of drawing population- and case-level causal inferences. One feature of the analyses we have been considering so far is that the integration is essentially integration of inferences in the context of a single study. We are, for instance, integrating quantitative evidence for a large set of cases with qualitative evidence for a *subset* of those cases. We are, moreover, drawing inferences from the set of cases we observe to a population *within which* that sample of cases is situated. 

In this chapter, we examine how we can use causal models to integrate across studies or settings that are, in a sense, more disjointed from one another: across studies that examine different causal relationships altogether; study designs that require different assumptions about exogeneity; and contexts across which the causal quantities of interest may vary.

1. **Integrating across a model** Often, individual studies in a substantive domain examine distinct segments of a broader web of causal relationships. For instance, while one study might examine the effect of $X$ on $Y$, another might examine the effect of $Z$ on $Y$, and yet another might examine the effect of $Z$ on $K$. We show in this chapter how we can, under some conditions, integrate across such studies in ways that yield learning that we could not achieve by taking each study on its own terms. 

2. **Integrating between experimental and observational studies** One form of multi-method research that has become increasingly common is the use of both observational and experimental methods to study the same basic causal relationships. While an experiment can offer causal identification in a usually local or highly controlled setting, an observational analysis can often shed light on how the same relationships operate "in the wild," if with a greater risk of confounding. Often, observational and experimental results are presented in parallel, as separate sources of support for a causal claim. We show how, in a causal model setup, we can use experimental and observational data *jointly* to address questions that cannot be answered when the designs are considered separately.\index{Identification}

3. **Transporting knowledge across contexts** Researchers are sometimes in a situation where they can identify causal quantities in a particular setting---say, from a randomized controlled trial implemented in a specific local context---but want to know how those inferences travel to other settings. Would the intervention work differently in other countries or regions? As we will explain, we can draw inferences about causal relationships in other contexts with an appropriately specified causal model and the right data from the original context.

4. **Models in hierarchies.** Sometimes, researchers learn about the same types of processes in different settings. By thinking of the processes in each setting as deriving from a family of processes, researchers can learn from observations in one setting about causal processes in another and also learn about the nature of heterogeneity between settings. 


Before delving into the details of these strategies, we make one key qualification explicit: each of these approaches requires us to believe that setting-~or study-specific  causal model can be nested within a lower level, "encompassing," model that operates across the multiple settings that we are learning from and want to draw inferences about. Encompassing models, of course, can specifically take heterogeneity across settings into account by including in the model moderators that condition the effects of interest. But we have to believe that we have indeed captured in the model any ways in which relationships vary across the set of contexts across which we are integrating evidence or transporting inferences. 

Put differently, and perhaps more positively, we see social scientists commonly seeking to transport knowledge or combine information informally across studies and settings. Often such efforts are motivated, sometimes implicitly, by an interest in or reliance on general theoretical propositions. The approaches that we describe below ask the researcher to be *explicit* about the underlying causal beliefs that warrant that integration while also ensuring that the integration proceeds in a way that is logically consistent with stated beliefs.


## A Jigsaw Puzzle: Integrating across a Model

Generating knowledge about a causal domain often involves cumulating learning across studies that each focus on some specific part of the domain. For instance, scholars interested in the political economy of democratization might undertake studies focused on the relationship between inequality and mass protests; studies on the role of mass mobilization in generating regime change; pathways other than mass mobilization through which inequality might affect democratization; studies of the effect of international sanctions on the likelihood that autocracies will democratize; and studies of the effects of democratization on other things, such as growth or the distribution of resources. 

We can think of these studies as each analyzing data on a particular part of a broader, more encompassing causal model. In an informal way, *if* findings "hold together" reasonably intuitively, we might be able to piece together an impression of the overall relations among variables in this domain. Yet an informal approach becomes more difficult for complex models or data patterns and, more importantly, will leave opportunities for learning unexploited.

Consider the simple DAG in @fig-HJ-F-11-1, in which both $X$ and $Z$ are causes of $Y$, and $Z$ also causes $K$. 

```{r}
#| label: fig-HJ-F-11-1
#| eval :  TRUE
#| echo: false
#| fig-height:  3
#| fig-cap: "A DAG containing nodes that feature in different studies."
model <- 
  
  make_model("X -> Y <- Z -> K") |>
  
  set_parameters(statement = "(Y[X=1, Z = 1] > Y[X=0, Z = 1])", parameters = .24) |>
  set_parameters(statement = "(K[Z = 1] > K[Z = 0])", parameters = .85) 

hj_ggdag(model = model)

```

\index{Cumulation}

Now imagine three studies, all conducted in contexts in which we believe this model to hold: 

1. Study 1 is an RCT in which $X$ is randomized, with data collected on both $Y$ and $K$. $Z$ is not observed.
1. Study 2 is a factorial experiment, in which $X$ and $Z$ are independently randomized, allowing an examination of the joint effects of $X$ and $Z$ on $Y$. $K$ is not observed.
2. Study 3 is an experiment randomizing $Z$, with only $K$ observed as an outcome. $X$ and $Y$ are not observed.

Now, let's say that our primary interest is in the relationship between $X$ and $Y$. Obviously, Study 1 will, with a sufficiently large sample, perform just fine in estimating the average treatment effect of $X$ on $Y$. However, what if we are interested in a case-oriented query, such as the probability of causation: the probability, say, that $X=1$ caused $Y=1$ in a given $X=1, Y=1$ case? 

We know that within-case, process-tracing clues can sometimes provide probative value on case-level estimands like the probability of causation, and we have observed $K$ in the Study 1 cases. So what if we combine the $X$, $Y$, and $K$ data? 

```{r, echo = FALSE}

df <- make_data(model, 300, using = "parameters") |>
  
      mutate(study = rep(1:3, each = 100),
             Z = ifelse(study == 1, NA, Z),
             K = ifelse(study == 2, NA, K),
             X = ifelse(study == 3, NA, X),
             Y = ifelse(study == 3, NA, Y)
             )

```

```{r, echo = FALSE}
if(run){

updated1 <- update_model(model, filter(df, study == 1))
updated2 <- update_model(model, filter(df, study == 2))
updated3 <- update_model(model, filter(df, study == 3))
updated_all <- update_model(model, df)

subs <- list(
              "X == 1 & Y == 1 & K == 1",
              "X == 1 & Y == 1 & K == 0")
subs2 <- list(
              "X == 1 & Y == 1 & K == 1",
              "X == 1 & Y == 1 & K == 0",
              "X == 1 & Y == 1 & K == 1 & Z == 1",
              "X == 1 & Y == 1 & K == 0 & Z == 1",
              "X == 1 & Y == 1 & K == 1 & Z == 0",
              "X == 1 & Y == 1 & K == 0 & Z == 0")

# If updating done using case data only
result1 <- query_model(updated1, queries = "Y[X=0] == 0", given = subs, using = "posteriors")
result2 <- query_model(updated2, queries = "Y[X=0] == 0", given = subs, using = "posteriors")
result3 <- query_model(updated3, queries = "Y[X=0] == 0", given = subs, using = "posteriors")
result4 <- query_model(updated_all, queries = "Y[X=0] == 0", given = subs2, using = "posteriors")

write_rds(list(result1, result2, result3, result4), "saved/11_frankenstein.rds")
}

```

A simple analysis of the graph tells us that $K$ cannot help us learn about $Y$'s potential outcomes  since $K$ and $Y$ are $d$-separated by $Z$, and we have not observed $Z$ in Study 1. We see this confirmed in @tbl-HJ-T-11-1.

In the first pair of rows, we show the results of analyses in which we have simulated data from the whole model, then updated using the Study 1 observations. We give here the posterior mean on the probability of causation for an $X=Y=1$ case, conditional on each possible value that $K$ might take on. As we can see, our beliefs about the estimand remain essentially unaffected by $K$'s value, meaning that it contains no information about $X$'s effect in the case.

We see that the same thing is true for the other studies. In Study 2, we have not used $K$ to update the model, and so have not learned anything from the data about $K$'s relationship to the other variables. Thus, we have no foundation on which to ground the probative value of $K$. In Study 3, we understand the $Z,K$ relationship well, but know nothing quantitatively about how $Z$ and $X$ relate to $Y$. Thus, we have learned nothing from Study 3 about what observing $K$ might tell us about the effect of $X$ on $Y$.
 

```{r}
#| label: tbl-HJ-T-11-1
#| tbl-cap: 'Clue $K$ is uninformative in all three studies'
#| echo: false

frank <- read_rds("saved/11_frankenstein.rds")

frank <- lapply(frank, function(d) dplyr::select(d, -case_level))

kabble(
  cbind(Study = c(1, NA, 2, NA, 3, NA),
        rbind(
    frank[[1]][,-c(1,3)],
    frank[[2]][,-c(1,3)], 
    frank[[3]][,-c(1,3)])) 
)
```


However, we can do much better if we combine the data and update *jointly* across all model parameters. The results are shown in @tbl-HJ-T-11-2. Updating simultaneously across the studies allows us, in a sense, to bridge across inferences. In particular, inferences from Study 2 make $Z$ informative about $Y$'s potential outcomes under different values of $X$. Meanwhile, inferences from the data in Study 3 allow us to use information on $K$ to update on values for $Z$. As we now see in rows 1 and 2, having updated the model in an integrated fashion, $K$ now *is* informative about the probability of causation, with our posterior mean on this query changing substantially depending on the value of $K$ that we observe in a case.  

Rows 3--4 highlight that the updating works through inferences that *K* allows us to make about $Z$: We see that if $Z$ is already known (we show this for $Z=1$, but it holds for $Z=0$ as well), then there are no additional gains from the knowledge of $K$. 


```{r}
#| label: tbl-HJ-T-11-2
#| tbl-cap: 'Clue is informative after combining studies linking $K$ to $Z$ and $Z$ to $Y$'
#| echo: false
kabble(frank[[4]][1:4,-c(1,3)], digits = 2)
```


We devote @sec-HJC15 to a discussion of how we justify a model. However, we note already that in this example, we have a form of model justification. We have seen an instance in which a researcher (examining a case in Study 1) might wish to draw inferences using $K$, but does not have anything in study that justifies using $K$ for inference. owever, with access to additional data from other studies and making use of a lower level model, the researcher now has a justification for a process tracing strategy.


## Combining Observational and Experimental Data {#sec-obsexp}

\index{Models!Experimental and observational data}

Experimental studies are often understood as the "gold standard" for causal inference. This is, in particular, because of the ability of a randomized trial (given certain assumptions, such as "no spillovers") to eliminate sources of confounding. By design, an experiment removes from the situation processes that, in nature, would generate a correlation between selection into treatment and potential outcomes. An experiment thereby allows for an unbiased estimate of the average causal effect of the treatment on the outcome. 

At the same time, an interesting weakness of experimental studies is that, by dealing so effectively with selection into treatment, they limit our ability to learn about selection and its implications in the real world. Often, however, we want to know what causal effects would be specifically for units that *would* in fact, take up a treatment in a real-world, nonexperimental setting. This kind of problem is studied, for example, by @knox2019design. \index{Knox, Dean}

Consider, for instance, a policy that would make schooling subsidies available to parents, with the aim of improving educational outcomes for children. How would we know if the policy was effective? A source of confounding in an observational setting might be that those parents who apply for and take up the subsidy might also be those who are investing more in their children's education in other ways, as compared to those parents who do not apply for the subsidy. The result is that when we compare those in treatment against those not in treatment we can expect to see a gap even if the subsidies are ineffective. To eliminate this problem, we might design an experiment in which parents are randomly assigned to receive (or not receive) the subsidy and compare outcomes between children in the treatment and control groups. With a no-spillovers assumption, we can extract the ATE of the receipt of subsidies. \index{Confounding}

What this experiment cannot tell us, however, is how much the policy will boost educational outcomes outside the experiment. That is because the causal quantity of interest, for answering that question, is *not* the ATE: It is the average treatment effect for the *treated* (ATT), given real-world selection effects. \index{Selection effects} That is, the policymaker wants to know what the effect of the subsidy will be for the children of parents who *select into* treatment. One could imagine the real-world ATT being higher than the ATE if, for instance, those parents who are informed and interested enough to take up the subsidy also put the subsidy to more effective use. One could also imagine the ATT being lower than the ATE if, for instance, there are diminishing marginal returns to educational investments, and the self-selecting parents are already investing quite a lot. 

Even outside a policy context, we may be interested in the effect of a causal condition *where* that causal condition emerges. To return to our inequality and democracy example, we may want to know what would have happened to autocracies with low inequality *if* they had had high inequality---the standard average-treatment effect question. But we might also be interested in knowing how much of a difference high inequality makes *in the kinds of cases* where high inequality tends to occur---where the effect could be very different. 

With such questions, we are in a sort of bind. The experiment cannot tell us *who* would naturally select into treatment and what the effects would be for them. Yet an observational study faces the challenge of confounding. Ideally, we would like to be able to combine the best features of both: use an experiment to deal with confounding and use observational data to learn about those whom nature assigns to treatment.\index{Confounding}

We can achieve this form of integration with a causal model. We do so by creating a model in which random assignment is nested within a broader set of assignment processes. We plot the model in @fig-HJ-F-11-2

At the substantive core of this model is the $X \rightarrow Y$ relationship. However, we give $X$ a parent that is an unconfounded root node, $Z$, to capture a random-assignment process. We give $X$ a second parent, $O$, that is confounded with $Y$: $O$ here represents the observational scenario. Finally, we include a "switch" variable, $R$, that determines whether $X$ is randomly assigned or not. So when $R=1$, $X$ is determined solely by $Z$, with $X=Z$. When $R=0$, we are in an observational setting, and $X$ is determined solely by the confounded $O$, with $X=O$.

A few notes on the parameter space: Parameters allow for complete confounding between $O$ and $Y$, but $Z$ and $Y$ are unconfounded. $X$ has only one causal type since its job is to operate as a conveyor belt, simply inheriting the value of $Z$ or $O$, depending on $R$. \index{Confounding}

Note also that this model assumes the exclusion restriction that entering the experimental sample ($R$) is not related to $Y$ other than through the assignment of $X$. 


```{r}
#| label: copobsetup
#| message: false
#| warning: false
#| echo: false
if(run)
  make_model("R -> X -> Y; O -> X <- Z; O <-> Y") |>
  
	set_restrictions("(X[R=1, Z=0]!=0)") |>
	set_restrictions("(X[R=1, Z=1]!=1)") |>
	set_restrictions("(X[R=0, O=0]!=0)") |>
	set_restrictions("(X[R=0, O=1]!=1)") |>

	set_parameters(node = "Y", given = "O.0", parameters = c(.8, .2,  0,  0)) |>
	set_parameters(node = "Y", given = "O.1", parameters = c( 0,  0, .6, .4)) |>
  write_rds("saved/11_copobs.rds")


model <- read_rds("saved/11_copobs.rds")


```

```{r}
#| label: fig-HJ-F-11-2
#| message: false
#| warning: false
#| echo: false
#| fig-cap: "A model that nests an observational and an experimental study. The treatment $X$ either takes on the observational value $O$
#| or the assigned values $Z$
#| depending on whether or not the case has been randomized
#| $R$."
#| fig-height:  3
hj_ggdag(model = model)
```

```{r, echo = FALSE, include = FALSE}
P <- grab(model, "parameter_matrix")
kabble(P[,1:4])
```


Now, let us imagine true parameter values such that $X$ has a $0.2$ average effect on $Y$. However, the effect is different for those who are selected into treatment in an observational setting: it is positive ($0.6$) for cases in which $X=1$ under observational assignment, but negative ($-0.2$) for cases in which $X=0$ under observational assignment. (See Supplementary Material for complete specification.) 

When we use the model to analyze the data, we will start with flat priors on the causal types. 

The implied true values for the estimands of interest, and our priors on those estimands, are displayed in @tbl-fusionestimands.


```{r}
#| label: tbl-fusionestimands
#| tbl-cap: 'Estimands under different assignment schemes'
#| echo: false

if(run){
result <- query_model(
    model, 
    queries = list(ATE = "c(Y[X=1] - Y[X=0])"), 
    given = list(TRUE, "R==0", "R==1"),
    using = c("parameters", "priors"), 
    expand_grid = TRUE)
write_rds(result, "saved/11_fusionestimands.rds")
}

read_rds("saved/11_fusionestimands.rds") |> 
  select(-case_level) |> 
  mutate(Using = gsub("parameters", "truth", using)) |>
  kabble( digits = 2)

```


```{r, echo = FALSE}

if(run)
 make_data(model, n = 800) |>
  write_rds("saved/11_fusiondata.rds")

data <- read_rds("saved/11_fusiondata.rds")

```


Now, we generate data from the model and then update the model using these data. 


```{r}
#| label: tbl-fusiondim
#| tbl-cap: 'Inferences on the ATE from differences in means'
#| echo: false
#| warning: false
#| message: false
x <- estimatr::difference_in_means(Y~X, data = filter(data, R==0))
kabble(summary(x)[[1]], digits = 3)
```

We begin by analyzing just the observational data (cases where $R=0$) and display the results in @tbl-fusiondim. Recall that the true average effect of $X$ on $Y$ is $0.2$. Naive analysis of the observational data, taking a simple difference in means between the $X=0$ and $X=1$ cases, yields a strongly upwardly biased estimate of that effect, of `r round(x$coefficients[[1]], 2)`. 

In contrast, when we \index{CausalQueries@\texttt{CausalQueries}} update on the full causal model and use both the experimental and observational data, we get the much more accurate results shown in @tbl-fusionCQ. Moving down the rows, we show here the estimate of the unconditional ATE, the estimate for the observational context ($R=0$), and the estimate for the experimental context ($R=1$). Unsurprisingly, the estimates are identical across all three settings since, in the model, $R$ is $d$-separated from $Y$ by $X$, which is observed. And, as we see, the posterior means are very close to the right answer of $0.2$.

```{r, message = FALSE, warning = FALSE, include = FALSE}
if(run)
  update_model(model, data) |>
  write_rds("saved/11_exp_obs.rds")
  
```

```{r}
#| label: tbl-fusionCQ
#| tbl-cap: 'Estimates on the ATE for observational ($R=0$) and experimental ($R=1$) set.'
#| echo: false
updated <- read_rds("saved/11_exp_obs.rds")

# updated$step <- "dag"
# attributes(updated)$root_nodes <- attributes(updated)$endogenous_nodes

query_model(
    updated, 
    queries = list(ATE = "c(Y[X=1] - Y[X=0])"), 
    given = list(TRUE, "R==0", "R==1"),
    using = "posteriors") |>
    select(-case_level) |> 
kabble()
```




```{r, eval = FALSE, echo = FALSE}
updated_no_O <- update_model(model, dplyr::filter(data, R==1))
```


```{r, message = FALSE, warning = FALSE, include = FALSE}
if(run){
  write_rds(update_model(model, dplyr::filter(data, R==1)), "saved/11_exp_obs_2.rds")
  }
updated_no_O <- read_rds("saved/11_exp_obs_2.rds")
```

```{r}
#| label: appcombexpopp8
#| echo: false
#| include: false
result <- query_model(
    updated_no_O, 
    queries = list(ATE = "c(Y[X=1] - Y[X=0])"), 
    given = list(TRUE, "R==0", "R==1"),
    using = "posteriors") |>
    select(-case_level)
kabble(result)
```

Since the model used both the experimental and the observational data, we might wonder from where the leverage was derived: Did the observational data improve our estimates of the average treatment effect, or do our inferences emerge strictly from the experimental data? In the book's Supplementary Material, we show the results that we get when we update using experimental data only.
Comparing the two sets of results, we find there that we do indeed get a tightening of posterior variance and a more accurate result when we use both the observational and experimental data, but the experimental data alone are quite powerful, as we should expect for an estimate of the ATE. The observational data do not add a great deal to an ATE estimate, and the gains from observational data would be smaller still (and the experimental results even more accurate) if the experimental sample were larger. 

However, what we can learn about uniquely from this model and the combined observational and experimental data is *heterogeneity* in effects between those in treatment and those in control *in the observational* setting. In @tbl-combexpobsattatc, we display the results of ATT and average treatment effect for the control (ATC) queries of the updated model. To estimate the ATT we pose an ATE query while conditioning on $X=1$, while for an ATC query we pose the ATE query while conditioning on $X=0$. In the first two rows, we see that, in the experimental setting  (conditioning on $R=1$), the average effect of $X$ on $Y$ is the same in both the treated and control groups, exactly as we would expect under random assignment. In the third row, we see the estimate of $X$'s average effect for those assigned "by nature" to the control group in the observational setting, extracting a result close to the "true" value of $-0.2$. The final row shows our estimate of the treatment effect for those who are selected into treatment in the observational setting, again getting close to the answer implied by the underlying data-generating process ($0.6$). 

```{r}
#| label: tbl-combexpobsattatc
#| tbl-cap: 'Effects of $X$ conditional on $X$ for units that were randomly assigned or not.  Effects of $X$ do not depend on $X$ in the experimental group, but they do in the observational group because of self selection. '
#| echo: false
result2 <- query_model(
    updated, 
    queries = list(ATE = "c(Y[X=1] - Y[X=0])"), 
    given = list("R==1 & X==0", "R==1 & X==1", "R==0 & X==0", "R==0 & X==1"),
    using = "posteriors")

result2 |>
    select(-case_level) |>
kabble()
```

In sum, we can learn nothing about the observational ATT or ATC from the experimental data alone, where the ATT and ATC are the same quantity. And in the observational data alone, we are hobbled by confounding of unknown direction and size. What the mixed model and data, in effect, are able to do is to let us (a) learn about the ATE from experimental data, (b) use experimental inferences on the ATE to separate true effects from confounding in the observational data and thus learn about the direction and size of the confounding in those data, and (c) estimate the treatment effect for the $X=0$ group and for the $X=1$ group, respectively, in the observational data *using* knowledge about confounding in these data. By mixing the experimental and observational data, we can learn about how the treatment has affected those units that, in the "real" world of the observational setting, selected into treatment *and* about how the treatment *would* affect those that selected into control. \index{Confounding}

It is not hard to see why the observational ATT and ATC might be of great interest to decision-makers where strong causal heterogeneity is a possibility. Imagine a situation, for instance, in which the ATE was the same as in our previous example, but with the negative effects arising in the group that naturally selects into treatment and a positive effect for those that naturally do not. Based on the experimental data alone, we might conclude that the policy that makes $X=1$ available is a good bet, given its positive ATE (assuming, of course, that $Y=1$ is a valued outcome). And, of course, the observational data alone would not allow us to confidently conclude otherwise. What the integrated analysis can reveal, however, is that $X$ in fact has a *negative* mean effect on those who would be most likely to take up the treatment. The strong positive effect for the control strongly shapes the experimental results but will go unrealized in the real world. 
In a similar vein, these estimates can aid causal explanations. Seeing the positive ATE might lead us to infer that most of the $X=1, Y=1$ cases we observe in the world are ones in which $X=1$ caused $Y=1$. The observational ATT estimates would point in a very different direction, however, indicating that these are actually the cases in which $X$ is least likely to have a positive effect and, thus, where $Y=1$ was most likely generated by some other cause.

We note that the results here relate to the LATE theorem [@angrist1995identification]. Imagine using data only on (a) the experimental group in control and (b) the observational group, some of whom are in treatment. We can conceptualize our design as one in which the observational group is "encouraged" to take up treatment, allowing us to estimate the effect for the "compliers" in the observational setting: those that self-select into treatment. Conversely, we could use data only on (a) the experimental group in treatment and (b) the observational group, some of whom are in control. This is a design in which the observational group is "encouraged" to take up the control condition, allowing us to estimate the effect for the "compliers" in this group (those that self select into control). 


## Transportation of Findings across Contexts

\index{Context}
\index{Transportation}
Sometimes, we study the effect of $X$ on $Y$ in one context (a country, region, or time period, for instance) and then want to make inferences about these effects in another context (say, another country, region, or time period). We may face the challenge that effects are heterogeneous, and that conditions that vary across contexts may be related to treatment assignment, outcomes, and selection into the sample. For example, we might study the relationship between inequality and democratization in low-income countries and then want to know how those effects travel to middle-income settings. However, the level of income may have implications jointly for the level of inequality and for how likely inequality is to generate regime change, meaning that causal effects uncovered in the first context cannot be assumed to operate the same way in the second context.

This is the problem studied by @pearl2014external. In particular,  @pearl2014external identify the nodes for which data are needed to "license" external claims, given a model. \index{Bareinboim, Elias} \index{Pearl, Judea}

We illustrate with a simple model in which an observable confounder has a different distribution across contexts. In the model drawn in @fig-HJ-F-11-3, *Context* determines the distribution of the confounder, $W$. We set a restriction such that the value of $W$ in Context 1 is never less than the value of $W$ in Context 0; our priors are otherwise flat over the remaining nodal types in the model.  



```{r}
#| label: fig-HJ-F-11-3
#| echo: false
#| fig-cap: "Extrapolation when confounders have different distributions across contexts"
#| fig-height: 3

model <- make_model("Context -> W  -> X -> Y <- W") |>
  set_restrictions("W[Context = 1] < W[Context = 0]") |>
  set_parameters(statement = "X[W=1]>X[W=0]", parameters = 1/2)|>
  set_parameters(statement = complements("W", "X", "Y"), parameters = .17) |>
  set_parameters(statement = decreasing("X", "Y"), parameters = 0) 

hj_ggdag(model = model,  x_coord = 1:4, y_coord = c(1,1,0,2))


```

We show priors and true values for the estimands, drawn from a "true" set of parameter values that we have posited, in @fig-HJ-F-11-4. We see that the incidence of $W=1$ is higher in Context 1 than in Context 0, both in our priors and in the "truth" posited by the assigned parameter values. The "true" ATE of $X$ on $Y$ is also higher in Context 1, though this is not reflected in our priors. The average treatment effect conditional on $W$ is the same in both contexts, whether we work from priors or assigned parameter values, as it must be given the model. That is, in this model the ATE varies conditional on $W$---and it varies conditional *only* on $W$.  


```{r}
#| label: fig-HJ-F-11-4
#| echo: false
#| fig-cap: "Priors and true values (parameters) for three estimands: the frequency of $W$
#| the effect of $X$ on $Y$
#| and the effect conditional on $W: 1$"
#| fig-height:  4
#| fig-width:  7
#| warning: false
if(run){
appev2 <-
  query_model(model,
            queries = list(Incidence = "W==1", 
                           ATE = "Y[X=1] - Y[X=0]", 
                           CATE = "Y[X=1, W=1] - Y[X=0, W=1]"),
            given = c("Context==0", "Context==1"),
            using = c("priors", "parameters"), expand_grid = TRUE) 
write_rds(appev2, "saved/11_appev2.rds")
}



read_rds("saved/11_appev2.rds") |> 
  ggplot(aes(mean, using)) + 
  geom_point() + 
  facet_grid(given ~ query) +
  theme_bw() + 
  geom_errorbarh(aes(xmax = cred.high, xmin = cred.low), height = .1) +
  xlab("Estimate") + ylab("")


```

\index{Queries!Conditional average treatment effect (CATE)}

We now update the model using data from one context and then see if we can transport those findings to the other context. Specifically, we update using data on $X, Y,$ and $W$ from Context 0. We then use the updated beliefs to draw inferences about Context 1, using data *only* on $W$ from Context 1. In @fig-HJ-F-11-5, we show our posteriors on the queries of interest as compared to the truth, given the "true" parameter values we have posited.

We see that we have done well in recovering the effects, *both* for the context we studied (i.e., in which we observed $X$ and $Y$) and for the context we did not study.  We can think of the learning here as akin to post-stratification. We have learned from observing $X, Y$, and $W$ in Context 0 how $X$'s effect depends on $W$. Then we use those updated beliefs when confronted with a new value of $W$ in Context 1 to form a belief about $X$'s effect in this second context. Of course, getting the right answer from this procedure depends, as always, on starting with the a good model.

We can also see, in @fig-HJ-F-11-5, what would have happened if we had attempted to make the extrapolation to Context 1 without data on $W$ in that context. We see a very large  posterior variance. The high posterior variance here captures the fact that we know things could be different in Context 1, but we don't know in what way they are different. So we don't learn much, but at least we know that we don't.


```{r}
#| label: fig-HJ-F-11-5
#| echo: false
#| fig-cap: "Extrapolation when two contexts differ on $W$ and $W$ is not observable in target context. Posteriors  and true values (parameters)
#| for the average effect
#| the average effect conditional on $W$ (CATE)
#| and the incidence of $W$
#| for two contexts."
#| fig-height:  4
#| fig-width:  6
if(run){

  data <- make_data(model, n = 10000, 
                  nodes = list(c("Context", "W"), c("X", "Y")), 
                  probs = c(1,1),
                  subsets = c(TRUE, "Context == 0"))

  transport <- update_model(model, data)

  write_rds(query_model(transport,
            queries = list(Incidence = "W==1", 
                           ATE = "Y[X=1] - Y[X=0]", 
                           CATE = "Y[X=1, W=1] - Y[X=0, W=1]"),
            given = c("Context==0", "Context==1"),
            using = c("posteriors", "parameters"), expand_grid = TRUE),
            "saved/11_transport.rds")

  
  data2 <- make_data(model, n = 10000, 
                  nodes = list(c("Context"), c("W", "X", "Y")), 
                  probs = c(1,1),
                  subsets = c(TRUE, "Context == 0"))

  transport2 <- update_model(model, data2)

  query_model(transport2,
            queries = list(Incidence = "W==1", 
                           ATE = "Y[X=1] - Y[X=0]", 
                           CATE = "Y[X=1, W=1] - Y[X=0, W=1]"),
            given = c("Context==0", "Context==1"),
            using = c("posteriors", "parameters"), expand_grid = TRUE) |>
    write_rds("saved/11_transport2.rds")
}


list('Information on W in target' = read_rds("saved/11_transport.rds"),
     'No information on W in target' =  read_rds("saved/11_transport2.rds")) |>
  
  bind_rows(.id = 'Challenge') %>%
  ggplot(aes(mean, using, shape = Challenge)) + 
  geom_point(position = ggstance::position_dodgev(.5)) + 
  facet_grid(given ~ query) +
  theme_bw() + 
  geom_errorbarh(aes(xmax = cred.high, xmin = cred.low), position = ggstance::position_dodgev(.5), height = .1) +
  xlab("Estimate") + ylab("") +
  theme(legend.position = "bottom")



```
\index{Queries!Conditional average treatment effect (CATE)}


## Multilevel Models, Meta-analysis {#sec-multilevel} 

\index{Meta-analysis}

A key idea in Bayesian meta-analysis is that when we analyze multiple studies together, not only do we learn about common processes that give rise to the different results seen in different sites, but we also learn more about each study from seeing the other studies.

A classic setup is provided in @gelman2013bayesian, in which we have access to estimates of effects and uncertainty in eight sites (schools), $(b_j, se_j)_{j \in \{1,2,\ldots,8\}}$. To integrate learning across these studies we employ a  "hierarchical model"
that treats each $b_j$ as a draw from distribution $N(\beta_j, se_j)$ (and, in turn treats each  $\beta_j$ is a draw from distribution $N(\beta, \sigma)$). In that setup we want to learn about the superpopulation parameters  $\beta, \sigma$, but we also get to learn more about the study-level effects $(\beta_j)_{j \in \{1,2,\ldots,8\}}$ by studying them jointly.\index{Gelman, Andrew}

A hierarchical model like this allows us to think about the populations in our study sites as themselves drawn from a larger population ("superpopulation") of settings. And, crucially, it allows us, in turn to use data in the study sites to learn about that broader superpopulation of settings.

Although often used in the context of linear models with parameters for average causal effects, this logic works just as well with the kinds of causal models we have been using in this book. 

Let's review how our analytic setup has worked so far. At each node in a causal model, we conceptualize a given case as having a particular nodal type. The case's nodal type is drawn from a distribution of nodal types in the population of cases from which this case has been drawn. When we do process tracing, we consider that population-level distribution to be a set of fixed shares of nodal types in the population: Say, for node $Y$, we might believe that half the cases in the population are $\lambda^Y_{01}$, a quarter are $\lambda^Y_{00}$, and a quarter are $\lambda^Y_{11}$. We then use data from the case to update on the case's nodal types (or on the combination of nodal types that correspond to some case-level query), given the population-level shares. 

When we engage in population-level inference, we begin with *uncertainty* about the population-level shares of types, and we express our prior beliefs about those shares as a Dirichlet *distribution*. So, for instance, our beliefs might be centered around a $\lambda^Y_{01}=0.5, \lambda^Y_{00}=0.25, \lambda^Y_{11}=0.25$ breakdown of shares in the population; and we also express some degree of uncertainty about what the breakdown is. Now, when we analyze data on some number of cases, we can update both on those cases' types and on our beliefs about the distribution of types in the population---perhaps shifting toward a higher share of $\lambda^Y_{01}$'s (and with a change in the distribution's variance).

As in the last section, we can also build a model in which there are multiple settings, possibly differing on some population-level characteristics. Fundamentally, however, the setup in the last section still involved population-level inference in that we were assuming that the *type shares* ($\lambda$ values) are the same across settings. The settings might differ in the value of a moderating variable, but they do not differ in the shares of cases that *would* respond in any given way to the moderator (and other causal conditions). The data allow us to update on what those common, cross-setting type proportions are.

When we build a hierarchical model, each case is still understood as being embedded within a population: our cases might be citizens, say, each embedded within a country. The key difference from population-level inference is that we now conceive of there being *multiple* populations---say, multiple countries---each drawn from a population of populations, or superpopulation. Now, we think of each population (country) as having its own set of type shares for each node. In practice we think of each country's type shares as being drawn from a Dirichlet distribution of type shares (for each node) that lives at the superpopulation level. Moreover, we are *uncertain* about what that distribution at the superpopulation level *is*. We are uncertain about what type proportions the superpopulation-level distribution is centered around, and we are uncertain about how dispersed this distribution is. While the distribution's central tendency will be related to the mean type shares for countries, its variance will determine the degree of *heterogeneity* across countries in their type shares. \index{Dirichlet distribution}

To summarize, in population-level inference, we express uncertainty about the population's type shares with a Dirichlet prior, at the population level, on which we update. In the hierarchical setting, we are uncertain about the population-level type shares and the superpopulation Dirichlet from which each node's type shares are drawn. We express our uncertainty about each superpopulation Dirichlet by positing a prior distribution over the Dirichlet's $\alpha$ parameters.

Now when we observe data on citizens within countries, we can update our beliefs about the types for the particular citizens we observe, about type shares in the population of citizens within each country that we study, *and* on the parameters of the Dirichlet distribution from which population shares have been drawn. In updating on the last of these, we are learning not just about the countries we observe but also about those we do not directly observe. 

We illustrate with a simulation using a simple $X \rightarrow Y$ model. We imagine that we are studying the $X \rightarrow Y$ effect in 10 countries. Each country has a parameter distribution drawn from a common Dirichlet distribution. 


We assign a particular true set of superpopulation parameter values that, for the analytic exercise, are treated as unknown and that we would like to recover. In this true world, the probability of assignment to $X=1$ is distributed Beta(6, 4), meaning that on average, 40% of units are assigned to treatment, though there is variation in this proportion across countries (or study sites). Nodal types on $Y$ are distributed Dirichlet with parameter $\alpha^Y = (3, 4, 10, 3)$, meaning that in the average country, the treatment effect is 0.3 (i.e. $\frac{10}{20} -  \frac{4}{20}$), though this also varies across countries. Using these true parameter values, we simulate $X, Y$ data for 50 observations for $n=10$ countries. 

```{r, comment = "", include = FALSE}
model   <- make_model("X->Y")
alpha_X <- c(6, 4)*1
alpha_Y <- c(.15, .2, .5, .15)*20

true_PX =  (alpha_X/sum(alpha_X))[2]
true_ATE =  (alpha_Y/sum(alpha_Y))[3] - (alpha_Y/sum(alpha_Y))[2]
true_concentration = sum(alpha_Y)

n <- 10

if(run){
  set.seed(1)
  # Draw parameter values for each study
  lambda_X = dirmult::rdirichlet(n, alpha_X)
  lambda_Y = dirmult::rdirichlet(n, alpha_Y)
  
  # Draw data for each study
  data_events <- 
    lapply(1:n,
           function(j)
             make_data(model, n = 50,
                       parameters = c(lambda_X[j,], lambda_Y[j,])) |>
             collapse_data(model))
  
  write_rds(list(lambda_X=lambda_X, lambda_Y=lambda_Y, data = data_events), "saved/11_truth.rds")
  
  }

# Load
truth <- read_rds("saved/11_truth.rds")
data_events <- truth$data


```



```{r, include = FALSE}

# Updating

# We prepare `stan` data and update:


# Stan data is the same as for CausalQueries except we provide a data matrix instead of a vector
data_multilevel   <- CausalQueries:::prep_stan_data(model = model, data = data_events[[1]])
data_multilevel$Y <- sapply(data_events, function(j) j$count)
data_multilevel$n_studies <- ncol(data_multilevel$Y)
data_multilevel$not_P <- 1 - data_multilevel$P
data_multilevel$A <- grab(model, "ambiguities_matrix")

if(run)
  rstan::stan(file = "helpers/multilevel_dirichlet.stan",  
              data = data_multilevel, 
              iter = 12000) |>
  write_rds("saved/11_meta_pooled.rds")

if(run)
  lapply(1:length(data_events), function(s)
    update_model(model, data = data_events[[s]], 
                 data_type = "compact", iter = 4000)$posterior_distribution) |>
  write_rds("saved/11_meta_unpooled.rds")

pooled   <-  read_rds("saved/11_meta_pooled.rds")
unpooled <-  read_rds("saved/11_meta_unpooled.rds")
```



```{r}
#| label: plotalphas
#| echo: false
#| fig-cap: "Joint distributions over alpha parameters."
#| include: false

# Results

# We use a custom summary function to summarize some results from these models and provide results in a graph and table.

alphas <- rstan::extract(pooled, pars = "alpha")$alpha |>
  data.frame()

names(alphas) <- c("X0", "X1", "Y00", "Y10", "Y01", "Y11")

# ggpairs(alphas)
```

```{r, include = FALSE}
my_summary <- function(pooled, unpooled, truth){
  
  meta    <-  rstan::extract(pooled, pars = "alpha")$alpha
  studies <-  rstan::extract(pooled, pars = "lambdas")$lambdas
  
  out1 <- data.frame(
    analysis = c("Pooled", "Pooled", "Pooled"),
    study = c("Meta", "Meta", "Meta"),
    estimand = c("ATE", "Concentration", "Prob(X=1)"),
    mean = c(
      mean((meta[,5] - meta[,4])/(meta[,3] + meta[,4] + meta[,5] + meta[,6])),
      mean((meta[,3] + meta[,4] + meta[,5] + meta[,6])),
      mean((meta[,2])/(meta[,1] + meta[,2]))),
   sd = c(
      sd((meta[,5] - meta[,4])/(meta[,3] + meta[,4] + meta[,5] + meta[,6])),
      sd((meta[,3] + meta[,4] + meta[,5] + meta[,6])),
      sd((meta[,2])/(meta[,1] + meta[,2]))),
   lower = c(
      quantile((meta[,5] - meta[,4])/(meta[,3] + meta[,4] + meta[,5] + meta[,6]), .025),
      quantile((meta[,3] + meta[,4] + meta[,5] + meta[,6]), .025),
      quantile((meta[,2])/(meta[,1] + meta[,2]), .025)),
   
  upper = c(
      quantile((meta[,5] - meta[,4])/(meta[,3] + meta[,4] + meta[,5] + meta[,6]), .975),
      quantile((meta[,3] + meta[,4] + meta[,5] + meta[,6]), .975),
      quantile((meta[,2])/(meta[,1] + meta[,2]), .975)
      )
     )
  
  my_sum <- function(df, s=1, label = NA)  
    data.frame(
               analysis = c(label, label),
               study = c(paste(s),paste(s)),
               estimand = c("ATE", "Prob(X=1)"),
               mean =     c(mean(df[,5] - df[,4]), mean(df[,2])),
               sd =       c(sd(df[,5] - df[,4]), sd(df[,2])),
               lower =       c(quantile(df[,5] - df[,4], .025), quantile(df[,2], .025)),
               upper =       c(quantile(df[,5] - df[,4], .975), quantile(df[,2], .975))
               )
  
  out2 <- lapply(1:dim(studies)[3], function(s) my_sum(studies[,,s], s, label = "Pooled"))
  
  out3 <- lapply(1:length(data_events), function(s) unpooled[[s]] |> my_sum(s, "Unpooled"))

  out4 <- data.frame(
    analysis = rep("truth", nrow(truth$lambda_X)),
    study = as.character(rep(1:nrow(truth$lambda_X), 2)),
    estimand = rep(c("ATE", "Prob(X=1)"), each = nrow(truth$lambda_X)),
    mean = c(truth$lambda_Y[,3] - truth$lambda_Y[,2], truth$lambda_X[,2]),
    sd = NA)
  
  out5 <- data.frame(
    estimand = c("ATE", "Concentration", "Prob(X=1)"),
    analysis = "truth",
    study = "Meta",
    mean = c(true_ATE, true_concentration, true_PX))
  
  bind_rows(list(out3, out2, out1, out4, out5)) |>
  arrange(estimand)
  }
```


```{r, warning = FALSE, message = FALSE, echo = FALSE, fig.cap = "Estimates from individual analyses and from meta analysis", include = FALSE}
results <- my_summary(pooled, unpooled, truth)  
```


```{r, include = FALSE}

  lambda_Y = dirmult::rdirichlet(1000, alpha_Y)
  qs <- quantile(lambda_Y[, 3] - lambda_Y[, 2], c(.025, .975)) %>% round(2)

  alpha_average <- alphas |> apply(2, mean)

  lambda_Y2 = dirmult::rdirichlet(1000, alpha_average[3:6])
  qs2 <- quantile(lambda_Y2[, 3] - lambda_Y2[, 2], c(.025, .975)) %>% round(2)

```

```{r, echo = FALSE}
errors <- results |> filter(estimand == "ATE") |> select(analysis, study, mean) |> spread(analysis, mean) %>% 
  filter(study != "Meta") |>
  mutate(error_pooled = (as.numeric(truth) - as.numeric(Pooled))^2,
         error_unpooled = (as.numeric(truth) - as.numeric(Unpooled))^2) |> 
  summarize(error_pooled = mean(error_pooled)^2,
            error_unpooled = mean(error_unpooled)^2)
```

```{r}
#| label: fig-HJ-F-11-6
#| echo: false
#| fig-cap: "Updating on study level parameters from integrated analyses. Error bars indicate 95 percent credibility intervals. Studies are ordered by the size of the estimates from the unpooled analysis."
#| warning: false

order <- results |> filter(estimand == "ATE" & analysis == "Unpooled") |> 
      arrange(study == "Meta", mean) |> pull(study)

results |> filter(estimand == "ATE") |>
  mutate(analysis = factor(analysis, c("truth", "Unpooled", "Pooled") |> rev(), c("Truth", "Unpooled", "Pooled") |> rev()),
         study = factor(study, order)) |>
ggplot(aes(mean, study, shape = analysis))  + 
  geom_errorbarh(aes(xmax = upper, xmin = lower), position = ggstance::position_dodgev(height = .6))  + 
  geom_point(position = ggstance::position_dodgev(height = .6)) + theme_bw() +
  theme(legend.position = "bottom")
```

We now need to specify uncertainty over the ($\alpha$) parameters of the Dirichlet distribution at the superpopulation level. In practice, we use an inverse gamma distribution for this, the critical feature being that we presuppose positive numbers but otherwise keep priors on these parameters dispersed. When we update now, we update simultaneously over the $\alpha$ parameters and the $\lambda$ parameters *for each country*.

In @fig-HJ-F-11-6 we plot the results. We focus on the average treatment effects and show a comparison of three values for each country: the unpooled estimate, or the estimate we get for each country using only data from that country; the pooled estimate, or the estimate we get for each country using data from *all* countries to inform that country's parameter estimate; and the truth as posited for this simulation. As we can see, the pooled estimates are generally closer to the center than the unpooled estimates: this is because we are effectively using data from all countries to discount extreme features of the data observed in a given country. Put differently, the pooled data serve somewhat like a prior when it comes to drawing inferences about a single country: our inference is a compromise between the data from that country and the beliefs we have formed from the pooled data. We can also see that, for most countries, the pooling helps: The regularization provided by the pooling often (but not always) gives us an estimate closer to the truth for most of the settings. This is especially true when the unpooled estimates are unusually low. Across cases we have a reduction in root-mean-squared error of `r 100*(1 - errors[1]/errors[2]) |> round(2)`%.

Of course, we also get an estimate for the meta-estimand, the average effect in the superpopulation. This lies close to the correct answer and has a relatively tight credibility interval. 

Finally, we can extract estimates of the *variation* in treatment effects across cases---a quantity distinct from our *uncertainty* about average effects. We can, for instance,  think of a concentration parameter, operationalized as the sum of the $\alpha^j$ terms for a node, with a higher value representing lower overall heterogeneity. In this example, the "true" $\alpha^Y$ terms that we assigned summed to 20. This "true" $\alpha^Y$ vector implies that treatment effects lie, 95% of the time, between `r qs[1]` and `r qs[2]`. Our estimate of this concentration parameter is `r (results |> filter(estimand == "Concentration" & analysis == "Pooled"))$mean |> round(2)`. With this particular data draw, we thus underestimate heterogeneity. . Though this concentration parameter estimate still implies considerable heterogeneity in effects.^[There is of course also posterior variation around this estimate.]  Our posteriors imply that we expect treatment effects to lie, with 95% probability, between `r qs2[1]` and `r qs2[2]`.^[Implied variation at the mean estimates of $\alpha$.]

