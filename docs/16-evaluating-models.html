<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>16&nbsp; Evaluating Models – Integrated Inferences</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./17-conclusion.html" rel="next">
<link href="./15-justifying-models.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script><link rel="stylesheet" href="style.css">
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./15-justifying-models.html">IV Models in Question</a></li><li class="breadcrumb-item"><a href="./16-evaluating-models.html"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Evaluating Models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Integrated Inferences</a> 
        <div class="sidebar-tools-main">
    <a href="./Integrated-Inferences.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Start</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Front matter</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quick guide</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">I Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-causal-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Causal Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-illustrating-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Illustrating Causal Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-causal-questions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Causal Queries</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-being-Bayesian.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Bayesian Answers</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-theory-as-causal-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Theories as Causal Models</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">II Model-based Causal Inference</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-process-tracing-with-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Process Tracing with Causal Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-PT-application.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Process Tracing Applications</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-mixing-methods.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Integrated Inferences</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-mixed-application.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Integrated Inferences Applications</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-fusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Mixing Models</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">III Design choices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-clue-selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Clue Selection as a Decision Problem</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-case-selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Case Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-wide-or-deep.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Going Wide, Going Deep</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">IV Models in Question</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-justifying-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Justifying Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-evaluating-models.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Evaluating Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Final Words</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">End matter</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-errata.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Errata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#four-strategies" id="toc-four-strategies" class="nav-link active" data-scroll-target="#four-strategies"><span class="header-section-number">16.1</span> Four Strategies</a>
  <ul class="collapse">
<li><a href="#check-conditional-independence" id="toc-check-conditional-independence" class="nav-link" data-scroll-target="#check-conditional-independence"><span class="header-section-number">16.1.1</span> Check Conditional Independence</a></li>
  <li><a href="#bayesian-p-value-are-the-data-unexpected-given-your-model" id="toc-bayesian-p-value-are-the-data-unexpected-given-your-model" class="nav-link" data-scroll-target="#bayesian-p-value-are-the-data-unexpected-given-your-model"><span class="header-section-number">16.1.2</span> Bayesian <em>p</em>-Value: Are the Data Unexpected Given Your Model?</a></li>
  <li><a href="#leave-one-out-likelihoods" id="toc-leave-one-out-likelihoods" class="nav-link" data-scroll-target="#leave-one-out-likelihoods"><span class="header-section-number">16.1.3</span> Leave-One-Out Likelihoods</a></li>
  <li><a href="#sensitivity" id="toc-sensitivity" class="nav-link" data-scroll-target="#sensitivity"><span class="header-section-number">16.1.4</span> Sensitivity</a></li>
  </ul>
</li>
  <li>
<a href="#evaluating-the-democracy-inequality-model" id="toc-evaluating-the-democracy-inequality-model" class="nav-link" data-scroll-target="#evaluating-the-democracy-inequality-model"><span class="header-section-number">16.2</span> Evaluating the Democracy-Inequality Model</a>
  <ul class="collapse">
<li><a href="#check-assumptions-of-conditional-independence" id="toc-check-assumptions-of-conditional-independence" class="nav-link" data-scroll-target="#check-assumptions-of-conditional-independence"><span class="header-section-number">16.2.1</span> Check Assumptions of Conditional Independence</a></li>
  <li><a href="#bayesian-p-value" id="toc-bayesian-p-value" class="nav-link" data-scroll-target="#bayesian-p-value"><span class="header-section-number">16.2.2</span> Bayesian <em>p</em>-Value</a></li>
  <li><a href="#leave-one-out-likelihoods-1" id="toc-leave-one-out-likelihoods-1" class="nav-link" data-scroll-target="#leave-one-out-likelihoods-1"><span class="header-section-number">16.2.3</span> Leave-One-Out Likelihoods</a></li>
  <li><a href="#sensitivity-to-priors" id="toc-sensitivity-to-priors" class="nav-link" data-scroll-target="#sensitivity-to-priors"><span class="header-section-number">16.2.4</span> Sensitivity to Priors</a></li>
  </ul>
</li>
  <li>
<a href="#evaluating-the-institutions-growth-model" id="toc-evaluating-the-institutions-growth-model" class="nav-link" data-scroll-target="#evaluating-the-institutions-growth-model"><span class="header-section-number">16.3</span> Evaluating the Institutions-Growth Model</a>
  <ul class="collapse">
<li><a href="#check-assumptions-of-conditional-independence-1" id="toc-check-assumptions-of-conditional-independence-1" class="nav-link" data-scroll-target="#check-assumptions-of-conditional-independence-1"><span class="header-section-number">16.3.1</span> Check Assumptions of Conditional Independence</a></li>
  <li><a href="#bayesian-p-value-1" id="toc-bayesian-p-value-1" class="nav-link" data-scroll-target="#bayesian-p-value-1"><span class="header-section-number">16.3.2</span> Bayesian P-Value</a></li>
  <li><a href="#leave-one-out-loo-cross-validation" id="toc-leave-one-out-loo-cross-validation" class="nav-link" data-scroll-target="#leave-one-out-loo-cross-validation"><span class="header-section-number">16.3.3</span> Leave-One-Out (LOO) Cross-validation</a></li>
  <li><a href="#sensitivity-to-priors-1" id="toc-sensitivity-to-priors-1" class="nav-link" data-scroll-target="#sensitivity-to-priors-1"><span class="header-section-number">16.3.4</span> Sensitivity to Priors</a></li>
  </ul>
</li>
  <li><a href="#appendix" id="toc-appendix" class="nav-link" data-scroll-target="#appendix"><span class="header-section-number">16.4</span> Appendix</a></li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./15-justifying-models.html">IV Models in Question</a></li><li class="breadcrumb-item"><a href="./16-evaluating-models.html"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Evaluating Models</span></a></li></ol></nav><div class="quarto-title">
<h1 class="title"><span id="sec-HJC16" class="quarto-section-identifier"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Evaluating Models</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><div class="callout callout-style-default callout-note callout-titled" title="Chapter summary">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter summary
</div>
</div>
<div class="callout-body-container callout-body">
<p>We describe strategies for figuring out whether a model is likely doing more harm than good and for comparing the performance of different models to one another.</p>
</div>
</div>
<p>Throughout this book, we have maintained the conceit that you believe your model. But it is also obvious that even the most nonparametric-seeming models depend on substantive assumptions and that these are almost certainly wrong. The question then is not how much you believe your model (or whether you really believe what you say you believe) but whether your model is useful in some sense. How can we evaluate the usefulness of our models?</p>
<section id="four-strategies" class="level2" data-number="16.1"><h2 data-number="16.1" class="anchored" data-anchor-id="four-strategies">
<span class="header-section-number">16.1</span> Four Strategies</h2>
<p>In this chapter, we will describe four strategies and show them at work for a running example in which we know a model poorly captures an assumed causal process. We will then use turn the four strategies loose on the two models that we examined in Chapters <a href="08-PT-application.html" class="quarto-xref"><span>Chapter 8</span></a> and <a href="10-mixed-application.html" class="quarto-xref"><span>Chapter 10</span></a>.</p>
<p>Here’s our running example. Imagine a true causal process involving <span class="math inline">\(X\)</span>, <span class="math inline">\(M\)</span>, and <span class="math inline">\(Y\)</span>. Say that <span class="math inline">\(X\)</span> affects <span class="math inline">\(Y\)</span> <em>directly</em>, <span class="math inline">\(M\)</span> never has a negative effect on <span class="math inline">\(Y\)</span>, and <span class="math inline">\(X\)</span> has no effect on <span class="math inline">\(M\)</span> (and so there is no indirect effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> via <span class="math inline">\(M\)</span>). But imagine that researchers wrongly suppose that the effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> runs entirely through <span class="math inline">\(M\)</span>, positing a model of the form <span class="math inline">\(X \rightarrow M \rightarrow Y\)</span>.</p>
<p>The problem with the posited model, then, is that it represents overly strong beliefs about independence relations: It does not allow for a direct effect that is in fact operating.</p>
<p>We are perfectly able to update using this too-strong <span class="math inline">\(X \rightarrow M \rightarrow Y\)</span> model and data—but the updated model can produce wildly misleading causal inferences. We show this using a set of 200 observations simulated from a model that has direct effects only and an average effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> of <span class="math inline">\(1/3\)</span>.</p>
<p>In the left panel of <a href="#fig-HJ-F-16-1" class="quarto-xref">Figure&nbsp;<span>16.1</span></a>, we show the estimated average treatment effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> when using these data to update the <span class="math inline">\(X \rightarrow M \rightarrow Y\)</span> model. In the right panel, we show the inferences we would make using the same data but using a model that makes weaker assumptions by allowing for direct effects: An <span class="math inline">\(X \rightarrow M \rightarrow Y \leftarrow X\)</span> model. With both models, we start with flat priors over nodal types.</p>
<p>We represent the (stipulated) true average effect with the vertical line in each graph.</p>
<p>As we can see, the weaker (i.e., more permissive) model performs alright: The true effect falls within the posterior distribution on the ATE. However, the stronger model, which excludes direct effects, generates a tight posterior distribution that essentially excludes the right answer. So, if we go into the analysis with the stronger model, we have a problem.</p>
<p>But can we <em>know</em> we have a problem?</p>
<p>In the remainder of this section, we explore a range of diagnostics that researchers can undertake to evaluate the usefulness of their models or to compare models with one another: Checking assumptions of conditional independence built into a model; checking the model’s fit; using “leave-one-out” cross-validation; and assessing model sensitivity. </p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-HJ-F-16-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-HJ-F-16-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="16-evaluating-models_files/figure-html/fig-HJ-F-16-1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-HJ-F-16-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.1: A restricted model yields a credibility interval that does not contain the actual average effect.
</figcaption></figure>
</div>
</div>
</div>
<section id="check-conditional-independence" class="level3" data-number="16.1.1"><h3 data-number="16.1.1" class="anchored" data-anchor-id="check-conditional-independence">
<span class="header-section-number">16.1.1</span> Check Conditional Independence</h3>
<p></p>
<p>First, even before engaging in updating, we can look to see whether the data we have are consistent with the causal model we postulate. In particular, we can check whether there are inconsistencies with the Markov condition that we introduced in <a href="02-causal-models.html" class="quarto-xref"><span>Chapter 2</span></a>: That every node is <em>conditionally independent</em> of its nondescendants, given its parents.</p>
<p>In this case, if the stronger model is right, then given <span class="math inline">\(M\)</span>, <span class="math inline">\(Y\)</span> should be independent of <span class="math inline">\(X\)</span>.</p>
<p>Is it?</p>
<p>One way to check is to assess the covariance of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> given <span class="math inline">\(M\)</span> in the data. Specifically, we regress <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span> for each value of <span class="math inline">\(M\)</span>, once for <span class="math inline">\(M=1\)</span> and again for <span class="math inline">\(M=0\)</span>; a correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> at either value of <span class="math inline">\(M\)</span> would be problematic for the conditional independence assumption embedded in the stronger model.</p>
<p>Note that this form of diagnostic test is a classical one in the frequentist sense: We start by hypothesizing that our model is correct and then ask whether the data were unlikely given the model.</p>
<div class="cell" data-layout-align="center">
<div id="tbl-HJT-r16" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-HJT-r16-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;16.1: Regression coefficient on <span class="math inline">\(X\)</span> given <span class="math inline">\(M=0\)</span> and <span class="math inline">\(M=1\)</span>
</figcaption><div aria-describedby="tbl-HJT-r16-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<thead><tr class="header">
<th style="text-align: right;">M</th>
<th style="text-align: right;">estimate</th>
<th style="text-align: right;">std.error</th>
<th style="text-align: right;">p.value</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0</td>
<td style="text-align: right;">0.345</td>
<td style="text-align: right;">0.093</td>
<td style="text-align: right;">0.000</td>
</tr>
<tr class="even">
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.341</td>
<td style="text-align: right;">0.096</td>
<td style="text-align: right;">0.001</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>We report the regression coefficients on <span class="math inline">\(X\)</span> in <a href="#tbl-HJT-r16" class="quarto-xref">Table&nbsp;<span>16.1</span></a>. It is immediately apparent that we have a problem. At both values of <span class="math inline">\(M\)</span>, and especially when <span class="math inline">\(M=0\)</span>, there is a strong correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, evidence of a violation of the Markov condition implied by the stronger model.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>Identifying the full set of conditional independence assumptions in a causal model can be difficult. There are however well developed algorithms for identifying what sets, if any, we need to condition on to ensure conditional independence between two nodes given a DAG.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
</section><section id="bayesian-p-value-are-the-data-unexpected-given-your-model" class="level3" data-number="16.1.2"><h3 data-number="16.1.2" class="anchored" data-anchor-id="bayesian-p-value-are-the-data-unexpected-given-your-model">
<span class="header-section-number">16.1.2</span> Bayesian <em>p</em>-Value: Are the Data Unexpected Given Your Model?</h3>
<p> A second—though clearly related–approach asks whether features of the data we observe are in some sense unusual given our updated model, or more unusual given our model than another model. For instance, if one model assumed no adverse effects of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> and no confounding, then a strong negative correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> would be unusual, even for the model updated with this data; and this negative correlation would be <em>more</em> unusual for this model than for a model that allowed for adverse effects.</p>
<p>This approach is also quite classical: We are looking to see whether we should “reject” our model in light of inconsistencies between the data we have and the data we expect to see given our updated model. The idea is not to figure out whether the model is false—we know it is—but whether it is unacceptably inconsistent with data patterns in the world <span class="citation" data-cites="gelman2013two">(<a href="20-references.html#ref-gelman2013two" role="doc-biblioref">Gelman 2013</a>)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-HJ-F-16-2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-HJ-F-16-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="16-evaluating-models_files/figure-html/fig-HJ-F-16-2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="960">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-HJ-F-16-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.2: Bayesian <span class="math inline">\(p\)</span> values are not calibrated.
</figcaption></figure>
</div>
</div>
</div>
<p>An approach for doing this using simulated data from the posterior predictive distribution is described in <span class="citation" data-cites="gabry2019visualization">Gabry et al. (<a href="20-references.html#ref-gabry2019visualization" role="doc-biblioref">2019</a>)</span>.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> The basic procedure we employ here is to:</p>
<ol type="1">
<li>Draw a parameter vector from the posterior.</li>
<li>Draw data using the parameter vector.</li>
<li>Calculate a test statistic using these data.</li>
<li>Repeat 1 - 3 to build up a distribution of test statistics.</li>
<li>Calculate the same test statistic using the real data (the observed statistic).</li>
<li>Assess how extreme the observed statistic is relative to the distribution of statistics generated from the posterior (e.g.&nbsp;the probability of getting a test statistic as large or larger than the observed statistic).</li>
</ol>
<p>Note that in this straightforward calculation we assess the probability of the data given the same model that generated the data; approaches could also be used that seek out-of-sample estimates of the probability of observing the observed data.</p>
<p>We note that the <span class="math inline">\(p\)</span> values generated in this way are not necessarily “calibrated” in the sense that given a true vector <span class="math inline">\(\theta\)</span>, the distribution of the <span class="math inline">\(p\)</span> value is not uniform <span class="citation" data-cites="bayarri2000p">(<a href="20-references.html#ref-bayarri2000p" role="doc-biblioref">Bayarri and Berger 2000</a>)</span>.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> It nevertheless gives an indication of whether the data are unusual given our model. As an illustration, imagine a simple <span class="math inline">\(X\rightarrow Y\)</span> model and imagine that in truth the effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> were 1. Say we observe <span class="math inline">\(N\)</span> cases in which <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are indeed perfectly correlated; we update our model and then draw data from this updated model. What are the chances that the data we draw would also be perfectly correlated, like the data we put into the model? In fact, surprisingly, the answer is “low,” and, moreover <em>how</em> low depends on <span class="math inline">\(N\)</span>.See this result plotted in <a href="#fig-HJ-F-16-2" class="quarto-xref">Figure&nbsp;<span>16.2</span></a>. In other words, the extreme data we see can seem extreme to us—even after we have updated using the right model and extreme data.</p>
<p>Returning to our running example, we consider two test statistics and compare performance for the stronger and weaker model (<a href="#fig-HJ-F-16-3" class="quarto-xref">Figure&nbsp;<span>16.3</span></a>. First, we look just at the distribution of the outcome <span class="math inline">\(Y\)</span> to see how the actual distribution in the data compares to the predicted distribution from the updated model. Second, we look at the actual correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> and see how this compares to the predicted distribution. In both cases we calculate a two-sided <span class="math inline">\(p\)</span>-value by assessing the chances of such an extreme outcome as what we observe. If the observed data were at the mean of the predictive distribution, then we would have a <span class="math inline">\(p\)</span>-value of 1. If they were at the 95th percentile (and the distribution of test statistics under the model were symmetric) we would have a <span class="math inline">\(p\)</span>-value of 0.10.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-HJ-F-16-3" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-HJ-F-16-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="16-evaluating-models_files/figure-html/fig-HJ-F-16-3-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-HJ-F-16-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.3: Bayesian <span class="math inline">\(p\)</span>-values for different test statistics and models.
</figcaption></figure>
</div>
</div>
</div>
<p>For the first test, we see that the predicted distribution of the outcome <span class="math inline">\(Y\)</span> is similar for both updated models; and the actual mean outcome is within the distribution of predicted mean outcomes. The <span class="math inline">\(p\)</span>-values for the stronger (1) and weaker models (0.87) suggest that the observed mean <span class="math inline">\(Y\)</span> value is not unusual for either model. No clues there. This is a fairly “easy” test in the sense that many models should have little difficulty producing a reasonable distribution for <span class="math inline">\(Y\)</span> even if they are problematic in other ways.</p>
<p>When it comes to the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, however, the two models perform very differently. The posterior predictive distribution from the stronger model is centered around a <span class="math inline">\(0\)</span> correlation and does not even extend out as far as the observed correlation. The resulting <span class="math inline">\(p\)</span>-value is 0, meaning that from the perspective of the stronger model the <span class="math inline">\(X,Y\)</span> correlation in the data is entirely unexpected. A frequentist looking at the observed correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> should feel comfortable rejecting the stronger model. The updated weaker model, in contrast, predicts a strong correlation, and the observed correlation is comfortably within the posterior predictive distribution, with a <span class="math inline">\(p\)</span>-value of 0.41.</p>
<p>At first blush, the abysmal performance of the stronger model may seem surprising. Even after this model has <em>seen</em> the <span class="math inline">\(X,Y\)</span> correlations in the data, the model still finds those correlations highly surprising. The <span class="math inline">\(X \rightarrow M \rightarrow Y\)</span> model fails to learn, however, because the strong assumptions on independence do not provide the flexibility it needs to capture the complex relations between <span class="math inline">\(X\)</span>, <span class="math inline">\(M\)</span>, and <span class="math inline">\(Y\)</span>. The problem is that <span class="math inline">\(M\)</span> is uncorrelated with <span class="math inline">\(X\)</span> in the true data-generating process, so the stronger model learns that there is no indirect effect. But, at the same time, this model does not <em>allow</em> for a direct effect. Despite what would seem to be overwhelming evidence of a systematic <span class="math inline">\(X,Y\)</span> correlation, a causal relationship connecting <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span> remains extremely unlikely given the <span class="math inline">\(X,M\)</span> data pattern and the impossibility of direct effects. The stronger model just can’t handle the truth. The weaker model, on the other hand, readily learns about the direct <span class="math inline">\(X \rightarrow Y\)</span> effect.</p>
</section><section id="leave-one-out-likelihoods" class="level3" data-number="16.1.3"><h3 data-number="16.1.3" class="anchored" data-anchor-id="leave-one-out-likelihoods">
<span class="header-section-number">16.1.3</span> Leave-One-Out Likelihoods</h3>
<p> A further class of model-validation methods involves cross-validation. Rather than asking how well the updated model predicts the data used to update it, cross-validation uses the data at hand to estimate how well the model is likely to predict new data that have not yet been seen.</p>
<p>One way to do this is to split the available data, using one subsample to update and then assessing predictions using the other subsample. We focus here, however, on a “leave-one-out” (LOO) approach that uses <em>all</em> of the available data to estimate out-of-sample predictive performance.</p>
<p> In the LOO approach, we update the model using all data points except for one and then ask how well the model performs in predicting the left-out observation. We repeat this for every data point in the dataset to assess how well we can predict the entire dataset.</p>
<p>Often, the LOO approach is used to predict a particular outcome variable. We, however, are interested in predictions over the joint realization of all nodes. Thus, we calculate the posterior probability of each data point, using the model updated with all of the other observations.</p>
<p>The LOO estimate of out-of-sample predictive fit, for a dataset with <span class="math inline">\(n\)</span> observations, is then:</p>
<p><span class="math display">\[\prod_1^np(y_i|y_{-i}, \text{model})\]</span> where <span class="math inline">\(y_{-i}\)</span> is the data pattern with observation <span class="math inline">\(y_i\)</span> left out, and <span class="math inline">\(y_i\)</span> represents the values of all nodes of interest for observation <span class="math inline">\(i\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-HJ-F-16-4" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-HJ-F-16-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="16-evaluating-models_files/figure-html/fig-HJ-F-16-4-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-HJ-F-16-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.4: LOO likelihoods for different models
</figcaption></figure>
</div>
</div>
</div>
<p>We implement LOO cross-validation of the stronger and weaker models using 200 observations generated from the same data-generating model employed above. We find that the LOO likelihood of the data under the stronger model is 2.12e-183 while the likelihood is 9.01e-179 under the weaker model. Thus, the weaker model represents an estimated improvement in out-of-sample prediction on the order of 4.24e+04<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<p>We can visualize the pattern in <a href="#fig-HJ-F-16-4" class="quarto-xref">Figure&nbsp;<span>16.4</span></a>, where we plot the likelihood of each possible data type under the stronger model against the likelihood of that data type under the weaker model. The distribution is much more compressed on the horizontal axis than on the vertical axis indicating how the stronger model is not able to differentiate as much across the data types as the weaker.</p>
<p>Notably, the stronger model is not able to “learn” from the data about the (<em>in fact</em>, operative) relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The positive correlation arises because both models have “learned” from chance correlations in the data that different values <span class="math inline">\(X,M\)</span> combinations are differentially likely. The weaker model, however, also succeeds in dividing the data types into two groups: Those with a positive <span class="math inline">\(X,Y\)</span> correlation and those with a negative <span class="math inline">\(X,Y\)</span> correlation and has correctly (given the true model) learned that the former is more likely than the latter. The stronger model is not successful in separating these sets out in this way. <!-- Flag: two typos in this paragraph in published book --></p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-HJ-F-16-5" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-HJ-F-16-5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="16-evaluating-models_files/figure-html/fig-HJ-F-16-5-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-HJ-F-16-5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.5: LOO likelihoods and data type counts for the stronger and weaker models
</figcaption></figure>
</div>
</div>
</div>
<p>In <a href="#fig-HJ-F-16-6" class="quarto-xref">Figure&nbsp;<span>16.6</span></a>, we then see how the likelihoods of each data type line up with the actual count of each data type. As we can see, the weaker model updates to likelihoods that fit the actual data pattern well while the stronger model does not; in particular the stronger model underpredicts cases that are on the diagonal and over predicts cases that are off it.</p>
<p>We can also turn the tables and imagine that the <em>stronger</em> model represents the true data-generating process. We implement LOO cross-validation of the two models using 200 data points generated from the stronger model. In <a href="#fig-HJ-F-16-6" class="quarto-xref">Figure&nbsp;<span>16.6</span></a>, we see a comparison of the likelihoods of the data types under the two updated models and note that they are extremely similar. This represents an important asymmetry: The model that makes weaker assumptions performs far better in handling data generated by a “stronger” true model than does the stronger model in learning about a process that violates one of its assumptions. Since the weaker model allows for both direct and indirect effects, the weaker <em>can</em> learn about the parameters of the true process in the first situation; but the strong model cannot do so in the second situation because it has by assumption ruled out a key feature of that process (the direct effect).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-HJ-F-16-6" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-HJ-F-16-6-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="16-evaluating-models_files/figure-html/fig-HJ-F-16-6-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-HJ-F-16-6-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.6: Data prediction of a restricted and unrestricted model when in fact the data are generated by the <del>un</del>restricted (stronger) model
</figcaption></figure>
</div>
</div>
</div>
<p>While it is difficult to see this in <a href="#fig-HJ-F-16-6" class="quarto-xref">Figure&nbsp;<span>16.6</span></a>, the stronger model performs better here than the weaker model. The likelihood of the data under the stronger model is now 1.51e-120, compared to the likelihood of 1.13e-125 under the weaker model. Thus, the weaker model represents an estimated loss to out-of-sample prediction on the order of 7.46e-06. This is not surprising insofar as the stronger model <em>precisely</em> models the data-generating process while the extra parameters in the weaker model allow for “learning” from chance features of the data.</p>
<p>These examples display features of estimation of out-of-sample prediction accuracy familiar from a regression context. In a regression framework, adding parameters to a model may improve fit to sample—generating gains to out-of-sample prediction accuracy when the new parameters pick up systematic features of the data-generating process—but run a risk of over-fitting to chance patterns in the data. Similarly, in a causal models framework, for a model with weaker assumptions and more parameters. We saw that the weaker model performed much better when the true process involved direct effects since the extra parameters, allowing for direct effects, captured something “real” going on. But the same model performed slightly worse than the stronger model when there were no direct effects to pick up, such that the extra parameter could only model noise.</p>
</section><section id="sensitivity" class="level3" data-number="16.1.4"><h3 data-number="16.1.4" class="anchored" data-anchor-id="sensitivity">
<span class="header-section-number">16.1.4</span> Sensitivity</h3>
<p> The last approach we consider brackets the question of which model is better and asks, instead: How much do your conclusions depend on the model? You can worry less about your assumptions if the conclusions are not strongly dependent on them.</p>
<p>For the running example we already saw in <a href="#fig-HJ-F-16-1" class="quarto-xref">Figure&nbsp;<span>16.1</span></a> that conclusions can depend dramatically on the model used. This alone is reason to be worried.</p>
<p>To illustrate how to think about sensitivity for a process tracing example, consider a situation in which we are unsure about posited parameter values: That is, about the probability of particular effects at particular nodes. It is likely to be the case in many research situations that we are considerably uncertain about how to quantify intuitive or theoretically informed beliefs about the relative likelihood of different effects.</p>
<p>Suppose, for instance, that we begin with an <span class="math inline">\(X \rightarrow M \rightarrow Y\)</span> model. And suppose, further, that we believe that it is unlikely that <span class="math inline">\(M\)</span> has an adverse effect on <span class="math inline">\(Y\)</span>. But we are not sure <em>how</em> unlikely that adverse effect is. (We assume all other nodal types are equally likely.) Finally, say that we want to use the observation of <span class="math inline">\(M\)</span> to draw an inference about whether <span class="math inline">\(X=1\)</span> caused <span class="math inline">\(Y=1\)</span> in an <span class="math inline">\(X=Y=1\)</span> case.</p>
<p>How much does our inference regarding <span class="math inline">\(X\)</span>’s effect on <span class="math inline">\(Y\)</span>—when we see <span class="math inline">\(M=0\)</span> or <span class="math inline">\(M=1\)</span>—depend on this second stage assumption about the probability of a negative <span class="math inline">\(M \rightarrow Y\)</span> effect?</p>
<p>We answer the question by looking at posterior beliefs for a range of possible values for the relevant parameter, <span class="math inline">\(\lambda^Y_{10}\)</span>. In <a href="#tbl-ch16ranges" class="quarto-xref">Table&nbsp;<span>16.2</span></a>, we examine a range of values for <span class="math inline">\(\lambda^Y_{10}\)</span>, from 0 to 0.25 (full parity with other types). For each parameter value, we first show the resulting prior belief about the probability that <span class="math inline">\(X=1\)</span> caused <span class="math inline">\(Y=1\)</span>. We can see that, before we observe <span class="math inline">\(M\)</span>, we think that a positive <span class="math inline">\(X \rightarrow Y\)</span> effect is more likely as a negative <span class="math inline">\(M \rightarrow Y\)</span> effect becomes more likely. This stands to reason since a negative second-stage effect is one possible process through which a positive <span class="math inline">\(X \rightarrow Y\)</span> effect might occur. And higher values for <span class="math inline">\(\lambda^Y_{10}\)</span> come disproportionately at the expense of types under which <span class="math inline">\(X\)</span> cannot affect <span class="math inline">\(Y\)</span>.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<p>In the next two columns, we show the posterior belief we arrive at when we observe <span class="math inline">\(M=0\)</span> and then <span class="math inline">\(M=1\)</span>, for each <span class="math inline">\(\lambda^Y_{10}\)</span> assumption. Looking at the last column first, we see that our inference from <span class="math inline">\(M=1\)</span> does <em>not</em> depend at all on our beliefs about adverse <span class="math inline">\(M \rightarrow Y\)</span> effects. The reason is that, if we see <span class="math inline">\(M=1\)</span>, we already know that <span class="math inline">\(M\)</span> did not have a negative effect on <span class="math inline">\(Y\)</span>, given that we also know <span class="math inline">\(Y=1\)</span>. Our beliefs are purely a function of the probability that there are positive effects at both stages as compared to the probability of other causal types that could yield <span class="math inline">\(X=M=Y=1\)</span>, a comparison unaffected by the probability of a negative <span class="math inline">\(M \rightarrow Y\)</span> effect.</p>
<p>Our inferences when <span class="math inline">\(M=0\)</span>, on the other hand, do depend on <span class="math inline">\(\lambda^Y_{10}\)</span>: When we see <span class="math inline">\(M=0\)</span>, our belief about a positive <span class="math inline">\(X \rightarrow Y\)</span> effect depends on the likelihood of <em>negative</em> effects at both stages. We see, then, that the likelier we think negative effects are at the second stage, the higher our posterior confidence in a positive <span class="math inline">\(X \rightarrow Y\)</span> effect when we see <span class="math inline">\(M=0\)</span>.</p>
<div class="cell" data-layout-align="center">
<div id="tbl-ch16ranges" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ch16ranges-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;16.2: Inferences on the probability that <span class="math inline">\(X\)</span> caused <span class="math inline">\(Y\)</span> upon seeing <span class="math inline">\(M=0\)</span> or <span class="math inline">\(M=1\)</span> for a range of possible values of <span class="math inline">\(\lambda^Y_{10}\)</span>
</figcaption><div aria-describedby="tbl-ch16ranges-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<thead><tr class="header">
<th style="text-align: right;"><span class="math inline">\(\lambda^Y_{10}\)</span></th>
<th style="text-align: right;">Prior</th>
<th style="text-align: right;"><span class="math inline">\(M=0\)</span></th>
<th style="text-align: right;"><span class="math inline">\(M=1\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">0.167</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.05</td>
<td style="text-align: right;">0.183</td>
<td style="text-align: right;">0.068</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.10</td>
<td style="text-align: right;">0.200</td>
<td style="text-align: right;">0.125</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.15</td>
<td style="text-align: right;">0.217</td>
<td style="text-align: right;">0.173</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.20</td>
<td style="text-align: right;">0.233</td>
<td style="text-align: right;">0.214</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.25</td>
<td style="text-align: right;">0.250</td>
<td style="text-align: right;">0.250</td>
<td style="text-align: right;">0.25</td>
</tr>
</tbody>
</table>
<p>Possible inferences on <code>X caused Y</code> given observation of M: 0 and M: 1 given different possible models. The highest inference when M: 0 is lower than the lowest inference when M: 1</p>
</div>
</div>
</figure>
</div>
</div>
<p>Even though our inferences given <span class="math inline">\(M=1\)</span> do not depend on <span class="math inline">\(\lambda^Y_{10}\)</span>, the amount that we <em>update</em> if we see <span class="math inline">\(M=1\)</span> <em>does</em> depend on <span class="math inline">\(\lambda^Y_{10}\)</span>. This is because <span class="math inline">\(\lambda^Y_{10}\)</span> affects our belief, prior to seeing <span class="math inline">\(M\)</span>, that <span class="math inline">\(X=1\)</span> caused <span class="math inline">\(Y=1\)</span>. Working with a low <span class="math inline">\(\lambda^Y_{10}\)</span> value, we start out less confident that <span class="math inline">\(X=1\)</span> caused <span class="math inline">\(Y=1\)</span>, and thus our beliefs make a bigger jump if we do see <span class="math inline">\(M=1\)</span> than if we had worked with a <span class="math inline">\(\lambda^Y_{10}\)</span> higher value.</p>
<p>However, to the extent that we want to know how our assumptions affect our <em>conclusions</em>, the interesting feature of this illustration is that sensitivity depends on what we find. The answer to our query is sensitive to the <span class="math inline">\(\lambda^Y_{10}\)</span> assumption if we find <span class="math inline">\(M=0\)</span>, but not if we find <span class="math inline">\(M=1\)</span>. It is also worth noting that, even if we observe <span class="math inline">\(M=0\)</span>, the sensitivity is limited across the range of parameter values tested. In particular, for all <span class="math inline">\(\lambda^Y_{10}\)</span> values below parity (0.25), seeing <span class="math inline">\(M=0\)</span> moves our beliefs <em>in the same direction.</em></p>
<p>We can use the same basic approach to examine how our conclusions change if we relax assumptions about nodal-type restrictions, about confounds, or about causal structure.</p>
<p>We also note that, in cases in which we cannot quantify uncertainty about parameters, we might still be able to engage in a form of “qualitative inference.” There is a literature on probabilistic causal models that assesses the scope for inferences when researchers provide ranges of plausible values for parameters (perhaps intervals, perhaps only signs, positive, negative, zero), rather than specifying a probability distribution. For a comprehensive treatment of qualitative algebras, see <span class="citation" data-cites="parsons2001qualitative">Parsons (<a href="20-references.html#ref-parsons2001qualitative" role="doc-biblioref">2001</a>)</span>. Under this kind of approach, a researcher might willing to say that they think some probability <span class="math inline">\(p\)</span> is not plausibly greater than .5, but unwilling to make a statement about their beliefs about where in the <span class="math inline">\(0\)</span> to <span class="math inline">\(0.5\)</span> range it lies. Such incomplete statements can be enough to rule out classes of conclusion.</p>
</section></section><section id="evaluating-the-democracy-inequality-model" class="level2" data-number="16.2"><h2 data-number="16.2" class="anchored" data-anchor-id="evaluating-the-democracy-inequality-model">
<span class="header-section-number">16.2</span> Evaluating the Democracy-Inequality Model</h2>
<p></p>
<p>We now turn to consider how well our model of democracy and inequality from <a href="08-PT-application.html" class="quarto-xref"><span>Chapter 8</span></a> and <a href="10-mixed-application.html" class="quarto-xref"><span>Chapter 10</span></a> fares when put to these four tests.</p>
<section id="check-assumptions-of-conditional-independence" class="level3" data-number="16.2.1"><h3 data-number="16.2.1" class="anchored" data-anchor-id="check-assumptions-of-conditional-independence">
<span class="header-section-number">16.2.1</span> Check Assumptions of Conditional Independence</h3>
<p>Our model presupposes that <span class="math inline">\(P\)</span> and <span class="math inline">\(I\)</span> are independent and that <span class="math inline">\(P\)</span> and <span class="math inline">\(M\)</span> are independent. Note that the model is consistent with the possibility that, conditional on <span class="math inline">\(D\)</span>, there is a correlation between <span class="math inline">\(M\)</span> and <span class="math inline">\(P\)</span> or between <span class="math inline">\(I\)</span> and <span class="math inline">\(P\)</span>, as <span class="math inline">\(D\)</span> acts as a collider for these pairs of nodes.</p>
<p>To test these assumptions, we in fact need to depart from the dataset drawn from <span class="citation" data-cites="haggard2012distributive">Haggard, Kaufman, and Teo (<a href="20-references.html#ref-haggard2012distributive" role="doc-biblioref">2012</a>)</span> because these authors only examined cases in which <span class="math inline">\(D=1\)</span>, those that democratized. Thus, we cannot use these data to assess the relationships not conditional on <span class="math inline">\(D\)</span> or conditional on <span class="math inline">\(D=0\)</span>. We generate observations on all four nodes for a broader set of cases by pulling together measures from multiple sources, with the aim of modeling democratization that occurred between 1990 and 2000.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> We describe this data in more detail in Appendix.</p>
<p>We can check some of the model’s assumptions in relation to conditional independencies through a set of simple regression models, with results displayed in <a href="#tbl-ch16cipimd" class="quarto-xref">Table&nbsp;<span>16.3</span></a>. In the first two rows, we examine the simple correlation between <span class="math inline">\(P\)</span> and <span class="math inline">\(I\)</span> and between <span class="math inline">\(P\)</span> and <span class="math inline">\(M\)</span>, respectively. We can see from the estimates in the first row that the data pattern is consistent with our assumption of unconditional independence of <span class="math inline">\(I\)</span> and <span class="math inline">\(P\)</span>. However, we also see that there <em>is</em> evidence of an unconditional correlation between <span class="math inline">\(P\)</span> and <span class="math inline">\(M\)</span>, something that is excluded by our model.</p>
<div class="cell" data-layout-align="center">
<div id="tbl-ch16cipimd" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ch16cipimd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;16.3: Regression coefficients to assess conditional independence
</figcaption><div aria-describedby="tbl-ch16cipimd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<thead><tr class="header">
<th style="text-align: left;">Correlation</th>
<th style="text-align: left;">Given</th>
<th style="text-align: right;">estimate</th>
<th style="text-align: right;">std.error</th>
<th style="text-align: right;">p.value</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">P,I</td>
<td style="text-align: left;">-</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.114</td>
<td style="text-align: right;">1.000</td>
</tr>
<tr class="even">
<td style="text-align: left;">P,M</td>
<td style="text-align: left;">-</td>
<td style="text-align: right;">0.291</td>
<td style="text-align: right;">0.131</td>
<td style="text-align: right;">0.029</td>
</tr>
<tr class="odd">
<td style="text-align: left;">P,I</td>
<td style="text-align: left;">M = 0</td>
<td style="text-align: right;">-0.220</td>
<td style="text-align: right;">0.111</td>
<td style="text-align: right;">0.053</td>
</tr>
<tr class="even">
<td style="text-align: left;">P,I</td>
<td style="text-align: left;">M = 1</td>
<td style="text-align: right;">0.467</td>
<td style="text-align: right;">0.260</td>
<td style="text-align: right;">0.098</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>We can dig a little deeper, however. The model <em>also</em> implies that <span class="math inline">\(P\)</span> should be independent of <span class="math inline">\(I\)</span> given <span class="math inline">\(M\)</span>—since <span class="math inline">\(D\)</span> blocks all paths between <span class="math inline">\(P\)</span> and either <span class="math inline">\(I\)</span> or <span class="math inline">\(M\)</span>. We test this assumption in rows 3 and 4 of the table, where we examine the conditional independence of <span class="math inline">\(P\)</span> and <span class="math inline">\(I\)</span> given <span class="math inline">\(M=0\)</span> and given <span class="math inline">\(M=1\)</span>. Here, the evidence is also troubling for our model, as we see a relatively strong negative correlation between <span class="math inline">\(P\)</span> and <span class="math inline">\(I\)</span> when <span class="math inline">\(M=0\)</span>, and positive correlation when <span class="math inline">\(M=1\)</span>.</p>
<p>While we cannot identify the correct model from this data pattern, one possible explanation could be that pressure has a direct effect on mobilization, making mobilization the product of inequality and pressure jointly.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> A model with an arrow running from <span class="math inline">\(P\)</span> to <span class="math inline">\(M\)</span> would make the model consistent with the unconditional correlation between these two variables, the conditional correlation between <span class="math inline">\(P\)</span> and <span class="math inline">\(I\)</span> given <span class="math inline">\(M\)</span> (since <span class="math inline">\(M\)</span> would now be a collider for <span class="math inline">\(I\)</span> and <span class="math inline">\(P\)</span>), as well as the unconditional independence of <span class="math inline">\(I\)</span> and <span class="math inline">\(P\)</span>. A possible way forward—which we do not pursue here—would be to now amend the model and evaluate the revised model against an independent set of data.</p>
</section><section id="bayesian-p-value" class="level3" data-number="16.2.2"><h3 data-number="16.2.2" class="anchored" data-anchor-id="bayesian-p-value">
<span class="header-section-number">16.2.2</span> Bayesian <em>p</em>-Value</h3>
<p></p>
<p>We turn next to evaluating the democratization model using the Bayesian <span class="math inline">\(p-\)</span>value approach, and for this purpose can return to the data that we coded from <span class="citation" data-cites="haggard2012distributive">Haggard, Kaufman, and Teo (<a href="20-references.html#ref-haggard2012distributive" role="doc-biblioref">2012</a>)</span>’s qualitative vignettes. In the two panels of <a href="#fig-HJ-F-16-7" class="quarto-xref">Figure&nbsp;<span>16.7</span></a>, we plot the posterior predictive distributions from our updated model for three quantities of interest: The outcome <span class="math inline">\(D\)</span>, the correlation between <span class="math inline">\(I\)</span> and <span class="math inline">\(M\)</span>, and the correlation between <span class="math inline">\(I\)</span> and <span class="math inline">\(D\)</span>. In each graph, we indicate with a vertical line the mean value for these quantities for the data at hand and report the <span class="math inline">\(p-\)</span>value: The probability of the observed data conditional on our model.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-HJ-F-16-7" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-HJ-F-16-7-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="16-evaluating-models_files/figure-html/fig-HJ-F-16-7-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-HJ-F-16-7-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.7: Three tests for the Inequality and Democracy model
</figcaption></figure>
</div>
</div>
</div>
<p>As we can see, both visually and from the <span class="math inline">\(p-\)</span>values, the model performs well (or at least, does not signal issues) in the sense that the data that we observe are not unexpected under the model.</p>
</section><section id="leave-one-out-likelihoods-1" class="level3" data-number="16.2.3"><h3 data-number="16.2.3" class="anchored" data-anchor-id="leave-one-out-likelihoods-1">
<span class="header-section-number">16.2.3</span> Leave-One-Out Likelihoods</h3>
<p></p>
<p>Turning to “leave one out” model assessment, we now consider comparing our base model (the “restricted model”) to two models that make weaker assumptions. In one (the “partially restricted” model), we drop the assumption of monotonicity of <span class="math inline">\(M\)</span> in <span class="math inline">\(I\)</span>. In a second alternative (“unrestricted model”), we make no monotonicity assumptions for any of the causal relations</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-HJ-F-16-8" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-HJ-F-16-8-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="16-evaluating-models_files/figure-html/fig-HJ-F-16-8-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="864">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-HJ-F-16-8-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.8: LOO data predictions for three versions of the Inequality and Democracy model
</figcaption></figure>
</div>
</div>
</div>
<p><a href="#fig-HJ-F-16-8" class="quarto-xref">Figure&nbsp;<span>16.8</span></a>, shows the relationship, for each model, of the likelihood of each data type against the number of cases of that data type in the data. A data type here is defined as a possible combination of realized values on all nodes (<span class="math inline">\(I, P, M\)</span>, and <span class="math inline">\(D\)</span>). In each plot, the diagonal line represents equality between the proportion of expected cases under the model and the proportion of actual cases. Just eyeballing the relationships, you can see that the plots are very similar. The unrestricted model has, however, somewhat more compressed (and so, less sensitive) predictions. If we were to fit a line on the graphs we would have an adjusted <span class="math inline">\(R^2\)</span> of 0.93 for the unrestricted model and 0.97 for the partially restricted and unrestricted models, respectively.</p>
<p>More formally, we calculate the LOO likelihood for each model as 1.68e-74 for the restricted model, 2.53e-75 for the partially restricted model, and 1.53e-73 for the unrestricted model. In other words, we see that the most restricted model performs best on this criterion, though the differences between the models are not large.</p>
</section><section id="sensitivity-to-priors" class="level3" data-number="16.2.4"><h3 data-number="16.2.4" class="anchored" data-anchor-id="sensitivity-to-priors">
<span class="header-section-number">16.2.4</span> Sensitivity to Priors</h3>
<p>In our base model we assume a set of monotonicity relations among nodes. How much do conclusions depend on these restrictions? We answer the question by comparing our conclusion with these restrictions to what we would conclude without this assumption. As above, we compare the fully restricted model, to a partially restricted model and a fully unrestricted model.</p>
<p>We first show results for population inference from a mixed methods analysis. As seen in <a href="#fig-HJ-F-16-9" class="quarto-xref">Figure&nbsp;<span>16.9</span></a>, our inferences regarding the overall effect of <span class="math inline">\(I\)</span> on <span class="math inline">\(D\)</span> are not very sensitive to the monotonicity assumption at <span class="math inline">\(M\)</span>. However, they are extremely sensitive to the other monotonicity assumptions made in the model: As we can see, the effect goes from around <span class="math inline">\(-0.25\)</span> to <span class="math inline">\(0\)</span> when we remove all restrictions.</p>
<p>Our conditional inferences about the share of <span class="math inline">\(I=0\)</span>, <span class="math inline">\(D=1\)</span> cases in which inequality mattered are not sensitive to the monotonicity assumptions. In particular, in cases with <span class="math inline">\(I=0, D=1\)</span> we are about equally likely to think that democratization was due to low inequality given any of the models. However, inferences conditional on <span class="math inline">\(M\)</span> are highly sensitive to the restrictions. When we see that in fact there was no mobilization, our attribution increases in the restricted model but decreases in the unrestricted model. In the fully unrestricted model our inferences are not affected at all by observation of <span class="math inline">\(M=0\)</span>.</p>
<p>Why is this? In the partially restricted model, we entertain the possibility that low inequality mattered not just directly but also, perhaps, by inducing protests. However, we when you observe no protests, we rule out this possible pathway. In the restricted model, we do not think that democratization could have been produced by low inequality via demonstrations—but nevertheless entertain the possibility of mobilization that is <em>not</em> due to inequality, which could nevertheless be the cause of democratization. In this case, observing no mobilization removes a rival cause of democratization, not a second channel.</p>
<p>In all, we judge the conditional inferences as very sensitive to the monotonicity assumptions we put in place. Defending a particular set of claims requires a stronger defense of the model employed than would be needed if this were not the case.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Using the `size` aesthetic with geom_path was deprecated in ggplot2 3.4.0.
ℹ Please use the `linewidth` aesthetic instead.</code></pre>
</div>
<div class="cell-output-display">
<div id="fig-HJ-F-16-9" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-HJ-F-16-9-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="16-evaluating-models_files/figure-html/fig-HJ-F-16-9-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-HJ-F-16-9-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.9: ATE of <span class="math inline">\(I\)</span> on <span class="math inline">\(D\)</span> for three models under different conditions
</figcaption></figure>
</div>
</div>
</div>
<p>We now consider a process-tracing analysis in which we stipulate the probabilities of nodal types rather than learning about them from the data. For this setup, we compare our restricted model (<span class="math inline">\(M_1\)</span>) to an alternative model (<span class="math inline">\(M_2\)</span>) in which we allow for negative effects of <span class="math inline">\(I\)</span> on <span class="math inline">\(M\)</span>, but consider them to be <em>unlikely</em> rather than impossible (with null and positive effects somewhat likely). We refer to these priors as “quantitative priors” in the sense that they place a numerical value on beliefs rather than a logical restriction. Specifically, we define model <span class="math inline">\(M_2\)</span> with prior probabilities on the elements of <span class="math inline">\(\theta^M\)</span> as: <span class="math inline">\(p(\theta^M=\theta^M_{10})=0.1\)</span>, <span class="math inline">\(p(\theta^M=\theta^M_{00})=0.3\)</span>, <span class="math inline">\(p(\theta^M=\theta^M_{11})=0.3\)</span>, and <span class="math inline">\(p(\theta^M=\theta^M_{01})=0.3\)</span>. This is in comparison to the 0, 1/3,1/3,1/3 distribution implied by the fully restricted model, <span class="math inline">\(M_1\)</span>.</p>
<p>In <a href="#fig-HJ-F-16-10" class="quarto-xref">Figure&nbsp;<span>16.10</span></a> we compare findings for a set of cases with different data realizations.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-HJ-F-16-10" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-HJ-F-16-10-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="16-evaluating-models_files/figure-html/fig-HJ-F-16-10-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-HJ-F-16-10-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.10: Inferences under alternative quantitative priors
</figcaption></figure>
</div>
</div>
</div>
<p>The results differ in various modest ways. For cases with <span class="math inline">\(I=0, D=1\)</span> we ask whether the low inequality caused democratization. There are some differences here when we are looking for negative effects of inequality, though the ordering of inferences does not change. The differences appear in the cases of Albania and Nicaragua, where <span class="math inline">\(M=1\)</span>. Under priors fully constrained to monotonic causal effects, we see that observing <span class="math inline">\(M=1\)</span> makes us think low inequality was less likely to have caused democracy because <span class="math inline">\(M=1\)</span> represents an alternative cause and because low inequality cannot cause democratization via <span class="math inline">\(M\)</span> if <span class="math inline">\(I \rightarrow M\)</span> effects cannot be negative. However, if we allow for a negative effect of <span class="math inline">\(I\)</span> on <span class="math inline">\(M\)</span>, even while believing it to be unlikely, we now believe a negative effect of inequality on democratization, conditional on mobilization, to be more likely since now that effect <em>can</em> run from <span class="math inline">\(I=0\)</span> to <span class="math inline">\(M=1\)</span> to <span class="math inline">\(D=1\)</span>. Thus, our estimate for Albania and Nicaragua goes up under <span class="math inline">\(M2\)</span> relative to <span class="math inline">\(M1\)</span>. We see, likewise, that mobilization, <span class="math inline">\(M\)</span>, becomes less <em>informative</em> about the effect, as the estimates for Albania (<span class="math inline">\(M=1, P=0\)</span>) are more similar to those for Mexico (<span class="math inline">\(M=0, P=0\)</span>), and those for Nicaragua (<span class="math inline">\(M=1, P=1\)</span>) to those for Taiwan (<span class="math inline">\(M=0, P=1\)</span>).</p>
<p>Turning to cases with high inequality and democratization, inferences about the probability of positive causation are unaffected by the assumption about the effect of <span class="math inline">\(I\)</span> on <span class="math inline">\(M\)</span>. The reason is that, since we still maintain a monotonicity assumption for the direct effect of <span class="math inline">\(I\)</span> on <span class="math inline">\(D\)</span> (no positive effects), the only question is whether there was an indirect effect. Since we maintain the assumption of a monotonic effect of <span class="math inline">\(M\)</span> on <span class="math inline">\(D\)</span>, it remains the case in both models that observing <span class="math inline">\(M=0\)</span> rules out a positive indirect effect. If however <span class="math inline">\(M=1\)</span>, then <span class="math inline">\(I\)</span> did not have a negative effect on <span class="math inline">\(M\)</span> and the only question is whether <span class="math inline">\(M=1\)</span> because of <span class="math inline">\(I\)</span> or independent of it—which depends only on the relative sizes of <span class="math inline">\(\theta^M_{11}\)</span> and <span class="math inline">\(\theta^M_{01}\)</span>. These remain the same (and equal to one another) in both models.</p>
<p>Overall the evaluation of the democracy and inequality model paints a mixed picture. Although the model is able to recreate data patterns consistent with observations, the inferences from within case observations discussed in <a href="08-PT-application.html" class="quarto-xref"><span>Chapter 8</span></a> depended on assumptions about processes that, while theoretically compelling, can <em>not</em> be justified from observation of broader data patterns even under relatively heroic assumptions on causal identification.</p>
</section></section><section id="evaluating-the-institutions-growth-model" class="level2" data-number="16.3"><h2 data-number="16.3" class="anchored" data-anchor-id="evaluating-the-institutions-growth-model">
<span class="header-section-number">16.3</span> Evaluating the Institutions-Growth Model</h2>
<p>Now we use these four techniques on our second application studying institutional quality and economic growth. Recall that we used data from <span class="citation" data-cites="rodrik2004institutions">Rodrik, Subramanian, and Trebbi (<a href="20-references.html#ref-rodrik2004institutions" role="doc-biblioref">2004</a>)</span> to assess the causes of economic growth, focusing specifically on the effects of institutions and of geography.</p>
<section id="check-assumptions-of-conditional-independence-1" class="level3" data-number="16.3.1"><h3 data-number="16.3.1" class="anchored" data-anchor-id="check-assumptions-of-conditional-independence-1">
<span class="header-section-number">16.3.1</span> Check Assumptions of Conditional Independence</h3>
<p></p>
<p>Our model presupposes unconditional independence between <span class="math inline">\(M\)</span> and <span class="math inline">\(D\)</span> and between <span class="math inline">\(R\)</span> and <span class="math inline">\(D\)</span>. We can see from the simple unconditional regressions reported in <a href="#tbl-HJ-T-16-4" class="quarto-xref">Table&nbsp;<span>16.4</span></a> that a dependence exists that is not allowed for in our model. Mortality and distance are related, as are distance from the equator and institutions.</p>
<div class="cell" data-layout-align="center">
<div id="tbl-HJ-T-16-4" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-HJ-T-16-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;16.4: Regression coefficients for <span class="math inline">\(M\)</span> and <span class="math inline">\(R\)</span> on <span class="math inline">\(D\)</span> (estimated separately).
</figcaption><div aria-describedby="tbl-HJ-T-16-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<thead><tr class="header">
<th style="text-align: left;">Relation</th>
<th style="text-align: right;">estimate</th>
<th style="text-align: right;">std.error</th>
<th style="text-align: right;">p.value</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">M~D</td>
<td style="text-align: right;">-0.373</td>
<td style="text-align: right;">0.105</td>
<td style="text-align: right;">0.001</td>
</tr>
<tr class="even">
<td style="text-align: left;">R~D</td>
<td style="text-align: right;">0.241</td>
<td style="text-align: right;">0.111</td>
<td style="text-align: right;">0.033</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>We might consider then a model that allows for an arrow from <span class="math inline">\(D\)</span> to <span class="math inline">\(M\)</span>. In this case we have a violation of the exclusion restriction. Even still, one might expect that taking account of possible dependencies might not greatly alter analysis since we in effect block on each variable when assessing the effect of another. We will revisit this question when we assess model sensitivity.</p>
</section><section id="bayesian-p-value-1" class="level3" data-number="16.3.2"><h3 data-number="16.3.2" class="anchored" data-anchor-id="bayesian-p-value-1">
<span class="header-section-number">16.3.2</span> Bayesian P-Value</h3>
<p></p>
<p>We turn next to evaluating the institutions and growth model using the Bayesian <span class="math inline">\(p-\)</span>value approach, and for this purpose can return to the data that we coded from Rodrik, Subramanian and Trebbi. In the two panels of <a href="#fig-HJ-F-16-11" class="quarto-xref">Figure&nbsp;<span>16.11</span></a>, we plot the posterior predictive distributions from our updated model for two quantities of interest: The outcome <span class="math inline">\(Y\)</span>, the size of a country’s economy and the correlation between <span class="math inline">\(M\)</span>, the settler mortality rate and <span class="math inline">\(R\)</span>, the quality of institutions. In each graph, we indicate with a vertical line the mean value for these quantities for the data at hand and report the <span class="math inline">\(p-\)</span>value: The probability of the observed data conditional on our model. This shows the odds of observing the data we see if we assume our model is true.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-HJ-F-16-11" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-HJ-F-16-11-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="16-evaluating-models_files/figure-html/fig-HJ-F-16-11-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-HJ-F-16-11-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.11: Bayesian <span class="math inline">\(p\)</span> values for the Institutions and Growth model
</figcaption></figure>
</div>
</div>
</div>
<p>This distribution has a low <span class="math inline">\(p\)</span> value, suggesting that the model does not update on the correlation between <span class="math inline">\(R\)</span> and <span class="math inline">\(Y\)</span> sufficiently; even after observing the data we remain surprised how strong this relation is. The Figure looks essentially identical if we instead use the weaker model in which we allow a <span class="math inline">\(D\)</span> to <span class="math inline">\(M\)</span> link.</p>
<p>If we repeat the exercise but imagining that our database were 10 times larger than it is (we replicate it 10 times), the model will have more scope to learn, returning a some somewhat better <span class="math inline">\(p\)</span> value of 10%. This suggests that the problem may not be with the structure of the model so much as the limited confidence we have regarding causal relations.</p>
</section><section id="leave-one-out-loo-cross-validation" class="level3" data-number="16.3.3"><h3 data-number="16.3.3" class="anchored" data-anchor-id="leave-one-out-loo-cross-validation">
<span class="header-section-number">16.3.3</span> Leave-One-Out (LOO) Cross-validation</h3>
<p></p>
<p><a href="#fig-HJ-F-16-12" class="quarto-xref">Figure&nbsp;<span>16.12</span></a> shows the LOO likelihoods for the models with and without a <span class="math inline">\(D\)</span> to <span class="math inline">\(M\)</span> path.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-HJ-F-16-12" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-HJ-F-16-12-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="16-evaluating-models_files/figure-html/fig-HJ-F-16-12-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-HJ-F-16-12-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.12: LOO data predictions for alternative models
</figcaption></figure>
</div>
</div>
</div>
<p>We can see here that the LOO likelihoods are relatively similar for the different models. This suggests our monotonicity restrictions are not having an enormous impact on the plausibility of the model through this test. The LOO likelihood is 1.19e-92 for the base model and 1.61e-92 for the model that allows a <span class="math inline">\(D \rightarrow M\)</span> path.</p>
<p>The points off the 45 degree line in <a href="#fig-HJ-F-16-10" class="quarto-xref">Figure&nbsp;<span>16.10</span></a> confirm, and provide greater detail about, the weakness in the model that we uncovered in our analysis of Bayesian <span class="math inline">\(p\)</span> values. We can see that we are systematically under-predicting cases in which <span class="math inline">\(Y=R= 1-M = 1-D\)</span>, which is why the model finds the true <span class="math inline">\(Y\)</span>, <span class="math inline">\(R\)</span> correlation “surprising”.</p>
</section><section id="sensitivity-to-priors-1" class="level3" data-number="16.3.4"><h3 data-number="16.3.4" class="anchored" data-anchor-id="sensitivity-to-priors-1">
<span class="header-section-number">16.3.4</span> Sensitivity to Priors</h3>
<p>We test for sensitivity to three features of the base model: The assumption of a monotonic effect of mortality on institutions, the exclusion restriction (no direct <span class="math inline">\(M\)</span> to <span class="math inline">\(Y\)</span> path), and the exclusion of a <span class="math inline">\(D\)</span> to <span class="math inline">\(M\)</span> path. How much do conclusions depend on these assumptions? We answer this question by comparing our conclusions with these assumptions to what we would conclude by relaxing them. As above, we compare the baseline model in which all three assumptions are embedded to a one-by-one relaxation of each assumption.</p>
<p>We first show results for population inference from a mixed-methods analysis, in <a href="#fig-HJ-F-16-13" class="quarto-xref">Figure&nbsp;<span>16.13</span></a>. As we can see, our inferences are reasonably stable across models, whether we are estimating the average effect of <span class="math inline">\(R\)</span> on <span class="math inline">\(Y\)</span>, the share of <span class="math inline">\(R=1, Y=1\)</span> cases in which institutions mattered, or the share of <span class="math inline">\(R=0, Y=0\)</span> cases in which institutions mattered. The most consequential model assumption appears to be that of monotonicity for the effect of <span class="math inline">\(R\)</span> on <span class="math inline">\(Y\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-HJ-F-16-13" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-HJ-F-16-13-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="16-evaluating-models_files/figure-html/fig-HJ-F-16-13-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-HJ-F-16-13-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.13: Sensitivity of population-level inferences to different model assumptions
</figcaption></figure>
</div>
</div>
</div>
<p>We now consider the consequences of the same model assumptions for case-level queries.Whereas for the democratization model we explored case-level queries under different assumed nodal-type probabilities, here we draw case-level inquiries from the updated model and use the “uninformative-case” query procedure (see Section 9.3.2.1). <a href="#fig-HJ-F-16-14" class="quarto-xref">Figure&nbsp;<span>16.14</span></a> shows the inferences we make given the same four different models for four types of cases. We focus the analysis here on cases with weak institutions and poor growth, but with differing values for <span class="math inline">\(M\)</span> and <span class="math inline">\(D\)</span>. We can see, in terms of the substantive conclusions we would draw, that patterns of inference for all cases are similar across the first three models. For instance, learning that there was high mortality makes you more likely to think that Nigeria did poorly because of poor institutions, regardless of whether we require monotonicity in the <span class="math inline">\(R\)</span> to <span class="math inline">\(Y\)</span> relationship or exclude a <span class="math inline">\(D\)</span> to <span class="math inline">\(M\)</span> path. The inferences are strongest in the case in which monotonicity is not imposed, but qualitatively similar across the first three rows. Case-level inference looks very different—indeed, becomes impossible—if we allow an arbitrary violation of the exclusion restriction: We gain nothing at all from observation of <span class="math inline">\(M\)</span> and <span class="math inline">\(D\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-HJ-F-16-14" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-HJ-F-16-14-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="16-evaluating-models_files/figure-html/fig-HJ-F-16-14-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1056">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-HJ-F-16-14-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.14: Sensitivity of probative value to three model assumptions
</figcaption></figure>
</div>
</div>
</div>
<p>Overall, the evaluation of the institutions and growth model in <a href="#fig-HJ-F-16-14" class="quarto-xref">Figure&nbsp;<span>16.14</span></a> suggests reasonably robust data-based case-level inferences. However, these case-level queries do depend critically on the plausibility of the exclusion restriction to identify the relation between institutions and growth. </p>
<p>In summary, neither model emerges with a spotless bill of health. In both setups, our probing points to areas where the models’ assumptions appear to be weighing on conclusions. Ultimately, however, the sensitivity of conclusions to model assumptions seems greater for the inequality model where monotonicity assumptions appear quite consequential.</p>
</section></section><section id="appendix" class="level2" data-number="16.4"><h2 data-number="16.4" class="anchored" data-anchor-id="appendix">
<span class="header-section-number">16.4</span> Appendix</h2>
<p>Data sources for the expanded inequality and democratization data set.</p>
<ul>
<li><p><strong>Inequality</strong>: We measure inequality, <span class="math inline">\(I\)</span>, using the Gini estimates from the University of Texas Inequality Project (<span class="citation" data-cites="galbraith2019inequality">Galbraith (<a href="20-references.html#ref-galbraith2019inequality" role="doc-biblioref">2016</a>)</span>). As we want to measure inequality at the beginning of the period, we take the Gini measure for each country that is closest in time to the year 1989. We then dichotomize the variable using the median value for the period as a cutoff.</p></li>
<li><p><strong>Mobilization</strong>: We measure <span class="math inline">\(M\)</span> using the Mass Mobilization Protest Data from <span class="citation" data-cites="clark2016mobilization">Clark and Regan (<a href="20-references.html#ref-clark2016mobilization" role="doc-biblioref">2016</a>)</span>. To capture the kinds of mobilization on which redistributive theories of democratization focus, we restrict our focus to protests in the demand categories “land farm issue,” “labor wage dispute,” “price increases, tax policy,” and “political behavior, process.” We also include only those gatherings with a size of at least 1000 protesters. We code a country case as <span class="math inline">\(M=1\)</span> if and only if, during the 1990s, it experienced at least one protest that meets both the demand-type and size criteria. </p></li>
<li><p><strong>Pressure</strong>: We draw on the GIGA Sanctions Dataset to measure international pressure, <span class="math inline">\(P\)</span>. Specifically, we code a country case as <span class="math inline">\(P=1\)</span> if and only if the country was the target of democratization-focused sanctions during the 1990-2000 period.</p></li>
<li><p><strong>Democratization</strong>: We use dichotomous democracy measures from <span class="citation" data-cites="cheibub2010democracy">Cheibub, Gandhi, and Vreeland (<a href="20-references.html#ref-cheibub2010democracy" role="doc-biblioref">2010</a>)</span>, in two ways. First, we filter countries such that our sample includes only those that were not democracies in 1990 (<span class="math inline">\(N=77\)</span>). We then use the democracy measure for the year 2000 to determine which countries democratized, coding as <span class="math inline">\(D=1\)</span> those and only those cases that Cheibub et al.&nbsp;code as democracies in that year.</p></li>
</ul>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-bayarri2000p" class="csl-entry" role="listitem">
Bayarri, MJ, and James O Berger. 2000. <span>“P Values for Composite Null Models.”</span> <em>Journal of the American Statistical Association</em> 95 (452): 1127–42.
</div>
<div id="ref-cheibub2010democracy" class="csl-entry" role="listitem">
Cheibub, José Antonio, Jennifer Gandhi, and James Raymond Vreeland. 2010. <span>“Democracy and Dictatorship Revisited.”</span> <em>Public Choice</em> 143 (1-2): 67–101.
</div>
<div id="ref-clark2016mobilization" class="csl-entry" role="listitem">
Clark, David, and Patrick Regan. 2016. <span>“Mass Mobilization Protest Data. 2016.”</span> <em>Harvard Dataverse</em>. <a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/HTTWYL">https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/HTTWYL</a>.
</div>
<div id="ref-gabry2019visualization" class="csl-entry" role="listitem">
Gabry, Jonah, Daniel Simpson, Aki Vehtari, Michael Betancourt, and Andrew Gelman. 2019. <span>“Visualization in Bayesian Workflow.”</span> <em>Journal of the Royal Statistical Society: Series A (Statistics in Society)</em> 182 (2): 389–402.
</div>
<div id="ref-galbraith2019inequality" class="csl-entry" role="listitem">
Galbraith, James. 2016. <span>“University of Texas Inequality Project.”</span> <a href="https://utip.lbj.utexas.edu/default.html">https://utip.lbj.utexas.edu/default.html</a>.
</div>
<div id="ref-gelman2013two" class="csl-entry" role="listitem">
Gelman, Andrew. 2013. <span>“Two Simple Examples for Understanding Posterior p-Values Whose Distributions Are Far from Uniform.”</span> <em>Electronic Journal of Statistics</em> 7: 2595–2602.
</div>
<div id="ref-haggard2012distributive" class="csl-entry" role="listitem">
Haggard, Stephan, Robert R Kaufman, and Terence Teo. 2012. <span>“Distributive Conflict and Regime Change: A Qualitative Dataset.”</span> <em>Coding Document to Accompany Haggard and Kaufman</em>.
</div>
<div id="ref-parsons2001qualitative" class="csl-entry" role="listitem">
Parsons, Simon. 2001. <em>Qualitative Methods for Reasoning Under Uncertainty</em>. Vol. 13. Mit Press.
</div>
<div id="ref-rodrik2004institutions" class="csl-entry" role="listitem">
Rodrik, Dani, Arvind Subramanian, and Francesco Trebbi. 2004. <span>“Institutions Rule: The Primacy of Institutions over Geography and Integration in Economic Development.”</span> <em>Journal of Economic Growth</em> 9 (2): 131–65.
</div>
</div>
</section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr>
<ol>
<li id="fn1"><p>In applying the Markov condition, we also need to take into account any unobserved confounding. For instance, suppose that there was an unobserved confounder of the relationship between <span class="math inline">\(M\)</span> and <span class="math inline">\(Y\)</span> in the <span class="math inline">\(X \rightarrow M \rightarrow Y\)</span> model. Then we would <em>not</em> expect <span class="math inline">\(Y\)</span> to be independent of <span class="math inline">\(X\)</span> conditional on <span class="math inline">\(M\)</span>. In this case <span class="math inline">\(M\)</span> acts as a collider between <span class="math inline">\(X\)</span> and another unobserved cause of <span class="math inline">\(Y\)</span>; so conditioning on <span class="math inline">\(M\)</span> introduces a correlation between <span class="math inline">\(X\)</span> and this unobserved cause, and thus between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><code>R</code> users can quickly access such results using the <code>impliedConditionalIndependencies</code> function in the <code>dagitty</code> package.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Tools in the <code>bayesplot</code> package can be used to show how typical the data we observe is for different models<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>A uniform distribution has the property that the probability of getting a value of <span class="math inline">\(p\)</span> or less under the model is <span class="math inline">\(p\)</span>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>These numbers (and later numbers) change from simulation to simulation based on the particular data we draw from the model.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Increasing weight on <span class="math inline">\(\lambda^Y_{10}\)</span> is drawn equally from <span class="math inline">\(\lambda^Y_{00}\)</span>, <span class="math inline">\(\lambda^Y_{11}\)</span>, and <span class="math inline">\(\lambda^Y_{10}\)</span>, with the first two of these three representing null effects.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>The data that we use to measure mobilization, from <span class="citation" data-cites="clark2016mobilization">Clark and Regan (<a href="20-references.html#ref-clark2016mobilization" role="doc-biblioref">2016</a>)</span>, cover only the 1990s.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>There is, in fact, also a strong positive interaction between <span class="math inline">\(I\)</span> and <span class="math inline">\(P\)</span> in a linear model of <span class="math inline">\(M\)</span>.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./15-justifying-models.html" class="pagination-link" aria-label="Justifying Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Justifying Models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./17-conclusion.html" class="pagination-link" aria-label="Final Words">
        <span class="nav-page-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Final Words</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>