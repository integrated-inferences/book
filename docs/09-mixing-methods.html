<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>9&nbsp; Integrated Inferences – Integrated Inferences</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./10-mixed-application.html" rel="next">
<link href="./08-PT-application.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script><link rel="stylesheet" href="style.css">
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./07-process-tracing-with-models.html">II Model-based Causal Inference</a></li><li class="breadcrumb-item"><a href="./09-mixing-methods.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Integrated Inferences</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Integrated Inferences</a> 
        <div class="sidebar-tools-main">
    <a href="./Integrated-Inferences.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Start</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Front matter</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quick guide</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">I Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-causal-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Causal Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-illustrating-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Illustrating Causal Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-causal-questions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Causal Queries</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-being-Bayesian.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Bayesian Answers</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-theory-as-causal-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Theories as Causal Models</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">II Model-based Causal Inference</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-process-tracing-with-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Process Tracing with Causal Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-PT-application.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Process Tracing Applications</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-mixing-methods.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Integrated Inferences</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-mixed-application.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Integrated Inferences Applications</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-fusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Mixing Models</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">III Design choices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-clue-selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Clue Selection as a Decision Problem</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-case-selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Case Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-wide-or-deep.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Going Wide, Going Deep</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">IV Models in Question</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-justifying-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Justifying Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-evaluating-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Evaluating Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Final Words</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">End matter</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-errata.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Errata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar"><div class="quarto-margin-header"><div class="margin-header-item">
<p><img src=".\figures/cover_smaller.jpg" class="img-fluid"></p>
</div></div>
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#from-one-unit-to-many" id="toc-from-one-unit-to-many" class="nav-link active" data-scroll-target="#from-one-unit-to-many"><span class="header-section-number">9.1</span> From One Unit to Many</a></li>
  <li>
<a href="#general-procedure" id="toc-general-procedure" class="nav-link" data-scroll-target="#general-procedure"><span class="header-section-number">9.2</span> General Procedure</a>
  <ul class="collapse">
<li><a href="#setup" id="toc-setup" class="nav-link" data-scroll-target="#setup"><span class="header-section-number">9.2.1</span> Setup</a></li>
  <li><a href="#inference" id="toc-inference" class="nav-link" data-scroll-target="#inference"><span class="header-section-number">9.2.2</span> Inference</a></li>
  <li><a href="#wrinkles" id="toc-wrinkles" class="nav-link" data-scroll-target="#wrinkles"><span class="header-section-number">9.2.3</span> Wrinkles</a></li>
  </ul>
</li>
  <li>
<a href="#payoffs" id="toc-payoffs" class="nav-link" data-scroll-target="#payoffs"><span class="header-section-number">9.3</span> Payoffs</a>
  <ul class="collapse">
<li><a href="#mmpayoff" id="toc-mmpayoff" class="nav-link" data-scroll-target="#mmpayoff"><span class="header-section-number">9.3.1</span> Mixing Methods</a></li>
  <li><a href="#sec-casepop" id="toc-sec-casepop" class="nav-link" data-scroll-target="#sec-casepop"><span class="header-section-number">9.3.2</span> Deriving Probative Value from the Data</a></li>
  <li><a href="#learning-without-identification" id="toc-learning-without-identification" class="nav-link" data-scroll-target="#learning-without-identification"><span class="header-section-number">9.3.3</span> Learning without Identification</a></li>
  </ul>
</li>
  <li>
<a href="#extensions" id="toc-extensions" class="nav-link" data-scroll-target="#extensions"><span class="header-section-number">9.4</span> Extensions</a>
  <ul class="collapse">
<li><a href="#beyond-binary-data" id="toc-beyond-binary-data" class="nav-link" data-scroll-target="#beyond-binary-data"><span class="header-section-number">9.4.1</span> Beyond Binary Data</a></li>
  <li><a href="#measurement-error" id="toc-measurement-error" class="nav-link" data-scroll-target="#measurement-error"><span class="header-section-number">9.4.2</span> Measurement Error</a></li>
  <li><a href="#spillovers" id="toc-spillovers" class="nav-link" data-scroll-target="#spillovers"><span class="header-section-number">9.4.3</span> Spillovers</a></li>
  </ul>
</li>
  <li>
<a href="#chapter-appendix-mixing-methods-with-causalqueries" id="toc-chapter-appendix-mixing-methods-with-causalqueries" class="nav-link" data-scroll-target="#chapter-appendix-mixing-methods-with-causalqueries"><span class="header-section-number">9.5</span> Chapter Appendix: Mixing Methods with <code>CausalQueries</code></a>
  <ul class="collapse">
<li><a href="#an-illustration-in-code" id="toc-an-illustration-in-code" class="nav-link" data-scroll-target="#an-illustration-in-code"><span class="header-section-number">9.5.1</span> An Illustration in Code</a></li>
  <li><a href="#replication-of-chickering1996clinician-lipid-analysis." id="toc-replication-of-chickering1996clinician-lipid-analysis." class="nav-link" data-scroll-target="#replication-of-chickering1996clinician-lipid-analysis."><span class="header-section-number">9.5.2</span> Replication of <span class="citation" data-cites="chickering1996clinician">Chickering and Pearl (1996)</span> Lipid Analysis.</a></li>
  <li><a href="#sec-pvpopdag" id="toc-sec-pvpopdag" class="nav-link" data-scroll-target="#sec-pvpopdag"><span class="header-section-number">9.5.3</span> Probative value arising from correlations in the posterior distribution over parameters</a></li>
  </ul>
</li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./07-process-tracing-with-models.html">II Model-based Causal Inference</a></li><li class="breadcrumb-item"><a href="./09-mixing-methods.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Integrated Inferences</span></a></li></ol></nav><div class="quarto-title">
<h1 class="title"><span id="sec-HJC9" class="quarto-section-identifier"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Integrated Inferences</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><div class="callout callout-style-default callout-note callout-titled" title="Chapter summary">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter summary
</div>
</div>
<div class="callout-body-container callout-body">
<p>We extend the analysis from <a href="07-process-tracing-with-models.html" class="quarto-xref"><span>Chapter 7</span></a> to multi-case settings and demonstrate how we can use the approach for mixed-method analysis. When analyzing multiple cases, we update our theory from the evidence and can then <em>use</em> our updated theory to draw both population- and case-level inferences. While single-case process tracing is entirely theory-informed, mixed-data inference is thus also <em>data</em>-informed. The approach can integrate information across any arbitrary mix of data structures, such as “thin” data on causes and outcomes in many cases and “thicker” process evidence on a subset of those cases.</p>
</div>
</div>
<p><br></p>
<p>We now extend the approach introduced in <a href="07-process-tracing-with-models.html" class="quarto-xref"><span>Chapter 7</span></a> to show how we can undertake causal-model-based causal inference using data on multiple cases.</p>
<p>In the single-case process-tracing setup, we start with a set of beliefs about causal effects at each node (i.e., about the distribution of nodal types in the population) and <em>apply</em> those beliefs, in combination with case-specific evidence, to the case at hand. The model itself remains static in single-case process tracing. When we draw on data from multiple cases, in contrast, we can use these data to <em>update</em> the model—to learn about the distribution of causal effects in the population. We can then use this updated, or trained, model to answer questions about causal relationships at the population level. We can also use this updated model at the <em>case</em> level—to undertake process-tracing on a given case with a model informed by observations of a wider set of cases. This means that, rather than the probative value of process-tracing evidence being supported only by our theoretical assumptions, probative value can emerge from the data itself.</p>
<p>Moreover, as we will show, causal models offer a powerful approach for <em>mixing</em> methods: that is, for integrating information drawn from different kinds of data strategies. We can readily update a causal model with a dataset that includes, for instance, data on only the explanatory variable and the outcome for a large set of cases and intensive data on causal processes for a subset of those cases.</p>
<p>We start the chapter with a conceptual point: As we demonstrate in the next section, the inferential logic introduced in <a href="07-process-tracing-with-models.html" class="quarto-xref"><span>Chapter 7</span></a> for single-case analysis can be used <em>as is</em> for multi-case analysis. Thus, the conceptual work for mixed-methods inference from causal models has, in a sense, been done already. We then show how we can deploy the same machinery, under assumptions regarding independence across cases, to learn about general causal processes. We explore the main payoffs of the approach: the ways in which it allows us to mix methods, integrate population- and case-level inference, and learn about causality in the absence of causal identification. And then, in the chapter’s final section, we illustrate several ways in which the baseline approach can be extended—to the analysis of nonbinary data and to modeling measurement error and spillovers.</p>
<section id="from-one-unit-to-many" class="level2" data-number="9.1"><h2 data-number="9.1" class="anchored" data-anchor-id="from-one-unit-to-many">
<span class="header-section-number">9.1</span> From One Unit to Many</h2>
<p></p>
<p>Conceptualized correctly, there is no deep difference between the logic of inference used in single-case and in multi-case studies. To be clear, our claim here is not that any single “case” can be disaggregated into many “cases,” thereby allowing for large-<span class="math inline">\(n\)</span> analysis of single units <span class="citation" data-cites="king1994designing">(<a href="20-references.html#ref-king1994designing" role="doc-biblioref">King, Keohane, and Verba 1994</a>)</span>. Our point is, rather, the opposite: Fundamentally, model-based inference always involves comparing <em>a</em> pattern of data with the logic of the model. Studies with multiple cases can, in fact, be conceptualized as single-case studies: We always draw our inferences from a single <em>collection</em> of clues, whether those clues have come from one or from many units.</p>
<p>In practice, when we move from a causal model with one observation to a causal model with multiple observations, we can use the structure we introduced in <a href="07-process-tracing-with-models.html" class="quarto-xref"><span>Chapter 7</span></a> by simply replacing nodes that have a single value (i.e., scalars) with nodes containing multiple values (i.e., vectors) drawn from multiple cases. We then make inferences about causal relations between nodes from observation of the values of multiple nodes’ vectors.</p>
<p>To illustrate, consider the following situation. Suppose that our model includes a binary treatment, <span class="math inline">\(X\)</span>, assigned to 1 with probability 0.5; an outcome, <span class="math inline">\(Y\)</span>; and a third “clue” variable, <span class="math inline">\(K\)</span>, all observable. We posit an unobserved variable <span class="math inline">\(\theta^Y\)</span>, representing <span class="math inline">\(Y\)</span>’s nodal type, with <span class="math inline">\(\theta^Y\)</span> taking on values in <span class="math inline">\(\{a,b,c,d\}\)</span> with equal probability. (We interpret the types in <span class="math inline">\(\{a,b,c,d\}\)</span> as defined in <a href="02-causal-models.html#sec-counterfactualmodel" class="quarto-xref"><span>Section 2.1</span></a>.) In addition to pointing into <span class="math inline">\(Y\)</span>, moreover, <span class="math inline">\(\theta^Y\)</span> affects <span class="math inline">\(K\)</span>—in a rather convenient way. In particular, <span class="math inline">\(K=1\)</span> whenever <span class="math inline">\(X\)</span> has an effect on <span class="math inline">\(Y\)</span>, while <span class="math inline">\(K=1\)</span> with a 50% probability otherwise. In other words, our clue <span class="math inline">\(K\)</span> is informative about <span class="math inline">\(\theta^Y\)</span>, a unit’s nodal type for <span class="math inline">\(Y\)</span>. As familiar from <a href="07-process-tracing-with-models.html" class="quarto-xref"><span>Chapter 7</span></a> and <a href="08-PT-application.html" class="quarto-xref"><span>Chapter 8</span></a>, when we observe <span class="math inline">\(K\)</span> in a case, we can update on <span class="math inline">\(X\)</span>’s effect on <span class="math inline">\(Y\)</span> within the case since that <span class="math inline">\(K\)</span> value will have different likelihoods under different values of <span class="math inline">\(\theta^Y\)</span>.</p>
<p>So far, we have described the problem at the unit level. Let’s now consider a two-case version of this setup. We do this by exchanging scalar nodes for vectors:</p>
<ul>
<li>We have a treatment node, <span class="math inline">\(\mathbf X\)</span>, that can take on one of four values, <span class="math inline">\((0,0), (0,1), (1,0), (1,1)\)</span> with equal probability. The value (0,0) simply means that <span class="math inline">\(X=0\)</span> in both cases; the value (0,1) means that <span class="math inline">\(X\)</span> is <span class="math inline">\(0\)</span> in the first case and <span class="math inline">\(1\)</span> in the second case; and so on.</li>
<li>
<span class="math inline">\(\mathbf \theta^Y\)</span> is now also a vector with two elements, one for each case. <span class="math inline">\(\mathbf \theta^Y\)</span> can thus take on one of 16 values <span class="math inline">\((a,a), (a,b),\dots, (d,d)\)</span>. We interpret <span class="math inline">\(\theta^Y=(a,b)\)</span>, for instance, to mean that <span class="math inline">\(Y\)</span>’s nodal type is <span class="math inline">\(a\)</span> for the first case and <span class="math inline">\(b\)</span> for the second case. Let us set a uniform prior distribution over these 16 possible values.</li>
<li>
<span class="math inline">\(\mathbf Y\)</span> is a vector that is generated by <span class="math inline">\(\mathbf \theta^Y\)</span> and <span class="math inline">\(\mathbf X\)</span> in an obvious way. For instance, <span class="math inline">\(\mathbf X=(0,0), \theta^Y=(a,b)\)</span> generate outcomes <span class="math inline">\(\mathbf Y=(1,0)\)</span>.</li>
<li>The vector <span class="math inline">\(\mathbf K\)</span> has the same domain as <span class="math inline">\(\mathbf X\)</span> and <span class="math inline">\(\mathbf Y\)</span>. Consistent with the setup above, for any case <span class="math inline">\(j\)</span>, the element <span class="math inline">\(K_j=1\)</span> with probability <span class="math inline">\(1.0\)</span> if <span class="math inline">\(\mathbf\theta^Y_{j}=b\)</span> and with probability <span class="math inline">\(0.5\)</span> if <span class="math inline">\(\mathbf\theta^Y_{j} \neq b\)</span>.</li>
</ul>
<p>Now consider a causal estimand. In a single-case setup, we might ask whether <span class="math inline">\(X\)</span> has an effect on <span class="math inline">\(Y\)</span> in the case. For a multi-case setup, we might ask what the Sample Average Treatment Effect (SATE), <span class="math inline">\(\tau\)</span>, is. Note a subtle difference in the nature of the answers we seek in these two situations. In the first (single-case) instance, our estimand is binary—of the form: “Is the case a <span class="math inline">\(b\)</span> type?”—and our answer is a probability. In multi-case estimation of the SATE, our estimand is categorical, and our answer will be a probability distribution: We are asking, “what is the probability that <span class="math inline">\(\tau\)</span> is 0.5?,” “What is the probability that <span class="math inline">\(\tau\)</span> is 0.5?,” and so on. </p>
<p>While the estimand shifts, we can use the tools introduced for single-case process tracing in Chapters <a href="07-process-tracing-with-models.html" class="quarto-xref"><span>Chapter 7</span></a> and <a href="08-PT-application.html" class="quarto-xref"><span>Chapter 8</span></a> to analyze these data from multiple cases. Consider the probability that <span class="math inline">\(\tau=1\)</span>. A SATE of <span class="math inline">\(1\)</span> would require that <span class="math inline">\(X\)</span> have a positive effect on <span class="math inline">\(Y\)</span> in both cases, that is, that <span class="math inline">\(\theta^Y = (b,b)\)</span>. Under our uniform priors, this has just a 1 in 16 probability.</p>
<p>Now suppose that we observe that, for both units, <span class="math inline">\(X=1\)</span> and <span class="math inline">\(Y=1\)</span>. This data pattern is consistent with only four possible <span class="math inline">\(\theta\)</span> vectors: <span class="math inline">\((b,b), (d,d), (b, d)\)</span>, and <span class="math inline">\((d,b)\)</span>. Moreover, each of these four is equally likely to produce the data pattern we see, though only one of them gets us <span class="math inline">\(\tau=1\)</span>. So our belief that <span class="math inline">\(\tau=1\)</span> now shifts from 1 in 16 to 1 in 4.</p>
<p>Next, suppose that we further look at <span class="math inline">\(K\)</span> in both cases and observe the data pattern <span class="math inline">\(\mathbf K = (1,1)\)</span>. The probability of this pattern for <span class="math inline">\(\theta\)</span> vector <span class="math inline">\((b,b)\)</span> (<span class="math inline">\(\tau = 1\)</span>) is 1. For the other three possible type vectors <span class="math inline">\((d,d), (b, d), (d,b)\)</span>, the probability of this <span class="math inline">\(\mathbf K\)</span> pattern is <span class="math inline">\(0.25, 0.5,\)</span> and <span class="math inline">\(0.5\)</span>, respectively. We apply Bayes’ rule now simply by dividing the probability of observing the <span class="math inline">\(K\)</span> data pattern if the hypothesis (<span class="math inline">\(\tau = 1\)</span>) is true by the (unnormalized) sum of the probabilities of the <span class="math inline">\(K\)</span> data pattern for <em>all</em> four <span class="math inline">\(\theta\)</span> vectors consistent with <span class="math inline">\(X=1, Y=1\)</span>: so <span class="math inline">\(1/(1 + 0.25 + 0.5 + 0.5) = 4/9\)</span>.</p>
<p>We can similarly figure out the posterior probability on any possible value of <span class="math inline">\(\tau\)</span> and build up a full posterior distribution. And we can do so given any <span class="math inline">\(K\)</span> pattern (i.e., <span class="math inline">\(\mathbf K\)</span> realization) across the cases. Thus, if we observe <span class="math inline">\(\mathbf K = (0,1)\)</span>, the probability of this pattern for type vector <span class="math inline">\((b,b)\)</span> (<span class="math inline">\(\tau = 1\)</span>) is 0. For the type vectors <span class="math inline">\((d,d), (b, d), (d,b)\)</span>, it is <span class="math inline">\(0.25, 0, 0.5\)</span>, respectively. <a href="#tbl-ch9patterns" class="quarto-xref">Table&nbsp;<span>9.1</span></a> shows the posterior distribution over a set of discrete SATE values given different <span class="math inline">\(K\)</span> patterns observed.</p>
<div id="tbl-ch9patterns" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ch9patterns-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;9.1: Bayesian inferences for Sample Average Treatment Effects given different data patterns across two cases.
</figcaption><div aria-describedby="tbl-ch9patterns-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 13%">
<col style="width: 11%">
<col style="width: 12%">
<col style="width: 11%">
</colgroup>
<thead><tr class="header">
<th>
<span class="math inline">\(X\)</span> pattern</th>
<th>
<span class="math inline">\(Y\)</span> pattern</th>
<th>
<span class="math inline">\(K\)</span> pattern</th>
<th><span class="math inline">\(\tau = -1\)</span></th>
<th><span class="math inline">\(\tau = -.5\)</span></th>
<th><span class="math inline">\(\tau = 0\)</span></th>
<th><span class="math inline">\(\tau = .5\)</span></th>
<th><span class="math inline">\(\tau = 1\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>(1,1)</td>
<td>(1,1)</td>
<td>(1,1)</td>
<td>0</td>
<td>0</td>
<td>1/9</td>
<td>4/9</td>
<td>4/9</td>
</tr>
<tr class="even">
<td>(1,1)</td>
<td>(1,1)</td>
<td>(1,0)</td>
<td>0</td>
<td>0</td>
<td>1/3</td>
<td>2/3</td>
<td>0</td>
</tr>
<tr class="odd">
<td>(1,1)</td>
<td>(1,1)</td>
<td>(0,0)</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The conceptual point is that the general logic of inference with multiple units is the same as that with one unit. In both situations, we work out the likelihood of any given data <em>pattern</em> for each possible set of values of model parameters, and then update our beliefs about those parameters accordingly. From our posterior distribution over fundamental model parameters, we can then derive a posterior distribution over the possible answers to any causal query, such as the values of <span class="math inline">\(\tau\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-HJ-F-9-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-HJ-F-9-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="09-mixing-methods_files/figure-html/fig-HJ-F-9-1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-HJ-F-9-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.1: Multiple units as vector valued nodes on a single DAG.
</figcaption></figure>
</div>
</div>
</div>
<p>However, while conceptually simple, thinking of nodes on a DAG as representing outcomes for all units implies models of extraordinary complexity, whose complexity rises rapidly with the number of cases. For instance, consider the model in <a href="#fig-HJ-F-9-1" class="quarto-xref">Figure&nbsp;<span>9.1</span></a> in which <span class="math inline">\(X = (X_1, X_2)\)</span> has a direct effect on <span class="math inline">\(Y = (Y_1, Y_2)\)</span> as well as an indirect effect via <span class="math inline">\(M = (M_1, M_2)\)</span>. The implied <span class="math inline">\(\theta^X\)</span> vector has four possible values. The <span class="math inline">\(\theta^M\)</span> vector has <span class="math inline">\(4^4 = 256\)</span> possibilities, and <span class="math inline">\(\theta^Y\)</span> has <span class="math inline">\(4^{4\times4} =\)</span> 4,294,967,296. Together, this means about 5 billion causal types for just two binary units. The mind boggles.</p>
<p>Fortunately, we can use a different approach.</p>
</section><section id="general-procedure" class="level2" data-number="9.2"><h2 data-number="9.2" class="anchored" data-anchor-id="general-procedure">
<span class="header-section-number">9.2</span> General Procedure</h2>
<p></p>
<p>To illustrate the logical parallel between single-case and multi-case inference, we have worked through a problem of sample-level inference: In the last section, we imagined that we were trying to estimate the causal effect for the specific set of cases that we were observing. However, for the remainder of this chapter, and for the rest of the book, when we discuss multi-case analysis, we will set our sights primarily on learning about models that describe realizations of <em>general</em> processes. That is, we will seek to learn about <em>populations</em>. We will then use our updated, general models at two levels: To address queries about the population and to address queries about specific cases. This means that we will bracket sample-level inference: That is, studying a set of cases in order to estimate some causal quantity for that sample. It is entirely possible to pose sample-level queries within the framework, but this will not be our focus.</p>
<p>There are two reasons motivating our focus on general models applicable to a population. The first is that we are interested in learning across cases. Our strategy for learning across cases is to learn about population-level parameters. We will use data on a set of cases to update our beliefs about a general model that we think is of relevance for other cases drawn from the same population.</p>
<p>The second reason for structuring learning around populations is more practical. If we can think of units as draws from a large population, and then invoke independence assumptions across types, then we can greatly reduce the kind of complexity we discussed at the end of the last section. In the two-case example above, the vector <span class="math inline">\(\theta^Y\)</span> could take on any of 16 values (<span class="math inline">\((a,a), (a,b),\ldots (d,d)\)</span>). In any given case, however, <span class="math inline">\(\theta^Y\)</span> can take on only four possible values (<span class="math inline">\(\{a,b,c,d\}\)</span>). So here is the simplifying move we make: Rather than trying to learn about the probabilities of 16 possible vector values for the two cases we’re studying (or of the 1,048,576 values for 10 cases), we instead turn this into a problem of learning about how the population is divvied up among just four nodal types. And if we know about the relative proportions of these types in a population, we are then in a position to estimate the probability that any case drawn from this population is of a given type.</p>
<p>Thinking about inference in this way simplifies the problem by greatly reducing the parameter space, but we do not get this payoff for free. It requires invoking the assumption that (potential) outcomes in one unit are independent of (potential) outcomes in all other units. If we cannot stand by that assumption, then we will need to build independence failures into our models, in ways we discuss later in this chapter.</p>
<p>As we move to population-level inference, we will continue to use simple DAGs to describe causal structures. When working with populations, however, we will now think of a DAG as standing in for a more complex multi-case structure. We can think of each individual unit as having an identical unit-level DAG and as being connected to one another via nodes that are common across the population. <a href="#fig-HJ-F-9-2" class="quarto-xref">Figure&nbsp;<span>9.2</span></a> illustrates (see also Figure 3 in <span class="citation" data-cites="chickering1996clinician">Chickering and Pearl (<a href="20-references.html#ref-chickering1996clinician" role="doc-biblioref">1996</a>)</span> for an example of a similar graph for a DAG with unobserved confounding). Here we replicate, twice, a simple unit-level DAG involving direct effects from <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span> as well as indirect effects via <span class="math inline">\(M\)</span>. We now subscript the unit-level substantive nodes to indicate the different values they can take on for each of the two units. Each unit also has separate, subscripted <span class="math inline">\(\theta\)</span> terms, implying that nodal types can vary across units, too. The unit-level <span class="math inline">\(\theta\)</span> terms are linked, however, through dependence on common <span class="math inline">\(\lambda\)</span>s, representing for each node the population-level shares of its nodal types. Thus, since <span class="math inline">\(\lambda^X\)</span> represents the population-level distribution of <span class="math inline">\(\theta^X\)</span>, <span class="math inline">\(\lambda^X\)</span> matters for the values that both <span class="math inline">\(\theta^{X_1}\)</span> and <span class="math inline">\(\theta^{X_2}\)</span> will take on and in turn, for the values that <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> will take on. We conceptualize the relationships similarly for the other nodes.</p>
<p>Critically, we can deploy the principles of conditional independence on a DAG, explored in <a href="07-process-tracing-with-models.html" class="quarto-xref"><span>Chapter 7</span></a>, to articulate how we can learn about one case from another. We can learn about <span class="math inline">\(\theta^{M_2}\)</span> and about <span class="math inline">\(M_2\)</span> from observing <span class="math inline">\(M_1\)</span>, for instance, because they are all descendants of the population-level node <span class="math inline">\(\lambda^M\)</span>—and we know that information flows across a “forked path.” The DAG elucidates less obvious possibilities, too. For instance, we can learn about <span class="math inline">\(\theta^{Y_2}\)</span> from observing <span class="math inline">\(M_1\)</span> if we also know <span class="math inline">\(Y_1\)</span>, since <span class="math inline">\(Y_1\)</span> acts as a collider for <span class="math inline">\(M_1\)</span> and <span class="math inline">\(\lambda^Y\)</span>; thus, observing <span class="math inline">\(Y_1\)</span> opens a path between <span class="math inline">\(M_1\)</span> and <span class="math inline">\(\theta^{Y_2}\)</span>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Moreover, all of this cross-case learning depends on the <span class="math inline">\(\lambda\)</span>s being (at least somewhat) <em>unknown</em>: If the <span class="math inline">\(\lambda\)</span>s are known, then the path between unit DAGs is blocked, so there can be no learning across cases. Put more intuitively, we can transfer knowledge across cases if we can learn <em>from</em> (some) cases about a population to which other cases also belong—and this strategy depends on the fact that we don’t already know all there is to know about the population.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-HJ-F-9-2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-HJ-F-9-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="09-mixing-methods_files/figure-html/fig-HJ-F-9-2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-HJ-F-9-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.2: A population DAG with multiple units from the same population
</figcaption></figure>
</div>
</div>
</div>
<p>We now outline the general procedure for multi-case inference. The core steps in this procedure are:</p>
<ul>
<li>to figure out all possible causal types implied by a DAG</li>
<li>to describe a set of distributions over these causal types</li>
<li>for any distribution over causal types, figure out the likelihood of any data pattern.</li>
</ul>
<p>With this likelihood in hand, we have enough to update our beliefs over distributions of causal types once we encounter the data. With updated beliefs about the distribution of causal types, we are ready, in turn, to pose any causal query of interest. This procedure can be seen as a generalization of the analysis used in <span class="citation" data-cites="chickering1996clinician">Chickering and Pearl (<a href="20-references.html#ref-chickering1996clinician" role="doc-biblioref">1996</a>)</span> to study compliance. We use the same basic logic here, but now for arbitrary DAGs, data structures, and queries. Appendix shows how to implement all steps in code and provides a replication of the analysis in <span class="citation" data-cites="chickering1996clinician">Chickering and Pearl (<a href="20-references.html#ref-chickering1996clinician" role="doc-biblioref">1996</a>)</span>. </p>
<section id="setup" class="level3" data-number="9.2.1"><h3 data-number="9.2.1" class="anchored" data-anchor-id="setup">
<span class="header-section-number">9.2.1</span> Setup</h3>
<p>We now describe the procedure in more detail. The key steps are as follows.</p>
<ol type="1">
<li><p><strong>A DAG</strong>. As with process tracing, we begin with a graphical causal model specifying possible causal linkages between nodes. Our “chain” model for instance has DAG: <span class="math inline">\(X \rightarrow M \rightarrow Y\)</span>. As described above, we now imagine this DAG standing in for a larger (“extended”) DAG in which this DAG is replicated for each unit and connected to other unit DAGs by population-level parameters (<span class="math inline">\(\lambda\)</span>s).</p></li>
<li><p><strong>Nodal types</strong>. Just as in process tracing, the DAG and variable ranges define the set of possible nodal types in the model—the possible ways in which each variable is assigned (if a root node) or determined by its parents (otherwise). For the <span class="math inline">\(X \rightarrow M \rightarrow Y\)</span> model, there are two types for <span class="math inline">\(\theta^X\)</span>, four for <span class="math inline">\(\theta^M\)</span>, and four for <span class="math inline">\(\theta^Y\)</span>. </p></li>
<li><p><strong>Causal types</strong>. A full set of nodal types gives rise to a full set of causal types, encompassing all possible combinations of nodal types across all nodes in the model. We let <span class="math inline">\(\theta\)</span> denote an arbitrary causal type. For an <span class="math inline">\(X \rightarrow M \rightarrow Y\)</span> model, one possible causal type would be <span class="math inline">\(\theta = (\theta^X_1, \theta^M_{01}, \theta^Y_{01})\)</span>.</p></li>
<li><p><strong>Parameters.</strong> As before, we use <span class="math inline">\(\lambda^j\)</span> to denote the population shares of the elements of <span class="math inline">\(\theta^j\)</span> (the nodal types) for a given node, <span class="math inline">\(j\)</span>. Recall that in process tracing, we sought to learn about <span class="math inline">\(\theta\)</span>, and our priors were given by <span class="math inline">\(\lambda\)</span>. When we shift to multi-case inference, <span class="math inline">\(\lambda\)</span> becomes the parameter we want to learn about: we seek to learn about the <em>shares</em> of types in a population (or, equivalently, about the probability of different types arising in cases drawn from that population).</p></li>
<li><p><strong>Priors</strong>. In the process tracing setup, we treat <span class="math inline">\(\lambda\)</span> as given: We do not seek to learn about <span class="math inline">\(\lambda\)</span>, and uncertainty over <span class="math inline">\(\lambda\)</span> plays no role. When we get to observe data on multiple cases, however, we have the opportunity to learn <em>both</em> about the cases at hand <em>and</em> about the population. Moreover, our level of uncertainty about population-level parameters will shape our inferences. We thus want our parameters (the <span class="math inline">\(\lambda\)</span>’s) to be drawn from a prior <em>distribution</em> — a distribution that expresses our uncertainty and over which we can update once we see the data. While different distributions may be appropriate to the task in general, uncertainty over proportions (of cases, events, etc.) falling into a set of discrete categories is usefully described by a Dirichlet distribution, as discussed in <a href="05-being-Bayesian.html" class="quarto-xref"><span>Chapter 5</span></a>. Recall that the parameters of a Dirichlet distribution (the <span class="math inline">\(\alpha\)</span>’s) can be thought of as conveying both the relative expected proportions in each category and our degree of uncertainty. </p></li>
</ol>
<p>With some abuse of graphical representation—we illustrate for only one replicate of the unit-level DAG—<a href="#fig-HJ-F-9-3" class="quarto-xref">Figure&nbsp;<span>9.3</span></a> displays the relationship between the case and population levels, together with an indication of distributions on different quantities.</p>
<ul>
<li>
<span class="math inline">\(\theta\)</span> denotes the case-level type with a categorical distribution. That distribution is described by the parameter vector <span class="math inline">\(\lambda\)</span>.</li>
<li>
<span class="math inline">\(\lambda\)</span> denotes the population-level shares of types. Uncertainty over <span class="math inline">\(\lambda\)</span> itself is characterized by a Dirichlet distribution, described by parameter vector <span class="math inline">\(\alpha\)</span>.</li>
<li>
<span class="math inline">\(\alpha\)</span> captures our priors on the distribution of <span class="math inline">\(\lambda\)</span>; in “multilevel” applications we might think of the <span class="math inline">\(\alpha\)</span> terms as parameters that we want to learn about, in which case we should provide a prior for <span class="math inline">\(\alpha\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>
</li>
</ul></section><section id="inference" class="level3" data-number="9.2.2"><h3 data-number="9.2.2" class="anchored" data-anchor-id="inference">
<span class="header-section-number">9.2.2</span> Inference</h3>
<p>Inference then works by figuring out the probability of the data given different possible parameter vectors, <span class="math inline">\(\lambda\)</span>s, and then applying Bayes rule. In practice, we proceed as follows. </p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-HJ-F-9-3" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-HJ-F-9-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="09-mixing-methods_files/figure-html/fig-HJ-F-9-3-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-HJ-F-9-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.3: Types parameters and priors
</figcaption></figure>
</div>
</div>
</div>
<section id="distributions-over-causal-types" class="level4" data-number="9.2.2.1"><h4 data-number="9.2.2.1" class="anchored" data-anchor-id="distributions-over-causal-types">
<span class="header-section-number">9.2.2.1</span> Distributions over Causal Types</h4>
<p>We first need to characterize our beliefs over causal types given any possible parameter vector, <span class="math inline">\(\lambda\)</span>. Imagine a draw of one possible value of <span class="math inline">\(\lambda\)</span> from the prior. This <span class="math inline">\(\lambda\)</span> vector implies a set of nodal type shares for all nodes. That set of nodal type shares implies, in turn, a distribution over <em>causal</em> types (<span class="math inline">\(\theta\)</span>), which are just combinations of nodal types. If nodal types are independent of each other, then causal type shares are a simple matter of multiplying nodal-type shares. For instance, the probability of causal type <span class="math inline">\(\theta = (\theta^X_1, \theta^M_{01}, \theta^Y_{01})\)</span> is simply <span class="math inline">\(p(\theta|\lambda)=\lambda^X_1\lambda^M_{01}\lambda^Y_{01}\)</span>. More generally:</p>
<p><span class="math display">\[p(\theta|\lambda) = \prod_{k,j:\theta^j_k\in\theta}\lambda^j_k\]</span></p>
</section><section id="data-probabilities" class="level4" data-number="9.2.2.2"><h4 data-number="9.2.2.2" class="anchored" data-anchor-id="data-probabilities">
<span class="header-section-number">9.2.2.2</span> Data Probabilities</h4>
<p>Each causal type, in turn, implies a single data realization or data type. For instance, <span class="math inline">\(\theta = (\theta^X_1, \theta^M_{01}, \theta^Y_{01})\)</span> implies data <span class="math inline">\(X=1, M=1, Y=1\)</span> (and <em>only</em> that data type). Let <span class="math inline">\(D(\theta)\)</span> denote the data type implied by causal type <span class="math inline">\(\theta\)</span>.</p>
<p>A single data type, however, may be implied by multiple causal types. We use <span class="math inline">\(\Theta(d)\)</span> to denote the set of causal types that imply a given data type:</p>
<p><span class="math display">\[\Theta(d) = \{\theta| D(\theta) = d \}\]</span></p>
<p>Let <span class="math inline">\(w_d\)</span> be the probability of a given data type <span class="math inline">\(d\)</span> (the “event probability”). The probability of a given data type is the sum of the probabilities of all causal types that imply it (given <span class="math inline">\(\lambda\)</span>). So we have:</p>
<p><span class="math display">\[w_d = \sum_{\theta \in \Theta(d)}p(\theta|\lambda)\]</span></p>
<p>We use <span class="math inline">\(\mathbf w\)</span> to denote the vector of event probabilities over <em>all</em> possible data types.</p>
<p>To illustrate, a data type <span class="math inline">\(d = (X=1, M =1, Y=1)\)</span> is consistent with four different causal types in the <span class="math inline">\(X\rightarrow M\rightarrow Y\)</span> model: <span class="math inline">\(\Theta(d) = \{(\theta^X_0, \theta^M_{01}, \theta^Y_{01}), (\theta^X_0, \theta^M_{11}, \theta^Y_{01}), (\theta^X_0, \theta^M_{01}, \theta^Y_{11}), (\theta^X_0, \theta^M_{11}, \theta^Y_{11})\}\)</span>. The probability of the data type is then calculated by summing up the probabilities of each causal type that implies the event. We can write this as: <span class="math inline">\(w_{111}:=\lambda^X_1(\lambda^M_{01} + \lambda^M_{11}))(\lambda^Y_{01} + \lambda^Y_{11})\)</span>.</p>
<p>In practice, calculating the full <span class="math inline">\(\mathbf w\)</span> vector is made easier by the construction of an “ambiguities matrix,” just as for process tracing, which tells us which causal types are consistent with a particular data type, as well as a “parameter matrix,” which tells us which parameters determine the probability of a causal type. </p>
<p>We use Tables <a href="#tbl-ambigmatrixmix" class="quarto-xref">Table&nbsp;<span>9.2</span></a> and <a href="#tbl-parammmatrixmix" class="quarto-xref">Table&nbsp;<span>9.3</span></a> to illustrate how to calculate the event probability for each data type for a given parameter vector <span class="math inline">\(\lambda\)</span>, here using a simple <span class="math inline">\(X \rightarrow Y\)</span> model. Starting with data type <span class="math inline">\(X=0, Y=0\)</span> (first column of the ambiguities matrix), we see that the consistent causal types are (<span class="math inline">\(\theta^X_0, \theta^Y_{00}\)</span>) and (<span class="math inline">\(\theta^X_0, \theta^Y_{01}\)</span>), in rows 1 and 5. We then turn to columns 1 and 5 of the parameter matrix to read off the probability of each of these causal types—which, for each, is given by the probability of the nodal types out of which it is formed. So for <span class="math inline">\(\theta^X_0, \theta^Y_{00}\)</span>, the probability is <span class="math inline">\(0.4 \times 0.3\)</span>, and for <span class="math inline">\(\theta^X_0, \theta^Y_{01}\)</span>, the probability is <span class="math inline">\(0.4\times 0.2\)</span>—giving a total probability of <span class="math inline">\(0.2\)</span> for the <span class="math inline">\(X=0, Y=0\)</span> data event. All four event probabilities, for the four possible data types, are then calculated in the same way.</p>
<p>In practice, within the <code>CausalQueries</code> package, these calculations are done using matrix operations.</p>
<div class="cell" data-layout-align="center">
<div id="tbl-ambigmatrixmix" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ambigmatrixmix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;9.2: An ambiguities matrix for a simple <span class="math inline">\(X \rightarrow Y\)</span> model (with no unobserved confounding). Row labels indicate causal types, column labels indicate data types.
</figcaption><div aria-describedby="tbl-ambigmatrixmix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<thead><tr class="header">
<th></th>
<th style="text-align: right;">X0Y0</th>
<th style="text-align: right;">X1Y0</th>
<th style="text-align: right;">X0Y1</th>
<th style="text-align: right;">X1Y1</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>X0Y00</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td>X1Y00</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="odd">
<td>X0Y10</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td>X1Y10</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="odd">
<td>X0Y01</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td>X1Y01</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
</tr>
<tr class="odd">
<td>X0Y11</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td>X1Y11</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<div class="cell" data-layout-align="center">
<div id="tbl-parammmatrixmix" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-parammmatrixmix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;9.3: A parameter matrix for a simple <span class="math inline">\(X \rightarrow Y\)</span> model (with no unobserved confounding), indicating a single draw of <span class="math inline">\(\lambda\)</span> values from the prior distribution.
</figcaption><div aria-describedby="tbl-parammmatrixmix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<thead><tr class="header">
<th></th>
<th style="text-align: center;">X0.Y00</th>
<th style="text-align: center;">X1.Y00</th>
<th style="text-align: center;">X0.Y10</th>
<th style="text-align: center;">X1.Y10</th>
<th style="text-align: center;">X0.Y01</th>
<th style="text-align: center;">X1.Y01</th>
<th style="text-align: center;">X0.Y11</th>
<th style="text-align: center;">X1.Y11</th>
<th style="text-align: center;"><span class="math inline">\(\lambda\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>X.0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.4</td>
</tr>
<tr class="even">
<td>X.1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.6</td>
</tr>
<tr class="odd">
<td>Y.00</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.3</td>
</tr>
<tr class="even">
<td>Y.10</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.2</td>
</tr>
<tr class="odd">
<td>Y.01</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.2</td>
</tr>
<tr class="even">
<td>Y.11</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.3</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
</section><section id="likelihood" class="level4" data-number="9.2.2.3"><h4 data-number="9.2.2.3" class="anchored" data-anchor-id="likelihood">
<span class="header-section-number">9.2.2.3</span> Likelihood</h4>
<p></p>
<p>Now we know the probability of observing each data pattern in a <em>single</em> case given <span class="math inline">\(\lambda\)</span>. We can use these case-level event probabilities to aggregate up to the likelihood of observing a data pattern across multiple cases (given <span class="math inline">\(\lambda\)</span>). For this aggregation, we make use of an independence assumption: that each unit is independently drawn from a common population-level distribution. Doing so lets us move from a categorical distribution that gives the probability that a single case has a particular data type to a <em>multinomial</em> distribution that gives the probability of seeing an arbitrary data pattern across any number of cases. </p>
<p>Specifically, with discrete variables, we can think of a given multiple-case data pattern simply as a set of counts across categories. This allows us to represent a multi-case data pattern in compact form. For, say, <span class="math inline">\(X, Y\)</span> data, we will observe a certain number of <span class="math inline">\(X=0, Y=0\)</span> cases (which we notate as <span class="math inline">\(n_{00}\)</span>), a certain number of <span class="math inline">\(X=1, Y=0\)</span> cases (<span class="math inline">\(n_{10}\)</span>), a certain number of <span class="math inline">\(X=0, Y=1\)</span> cases (<span class="math inline">\(n_{01}\)</span>), and a certain number of <span class="math inline">\(X=1, Y=1\)</span> cases (<span class="math inline">\(n_{11}\)</span>). A data pattern, given a particular set of variables observed (a search strategy), thus has a multinomial distribution. The likelihood of a data pattern under a given search strategy, in turn, takes the form of a multinomial distribution conditional on the number of cases observed, <span class="math inline">\(n\)</span>, and the probability of each data type, given a particular <span class="math inline">\(\lambda\)</span>. More formally, we write:</p>
<p><span class="math display">\[D \sim \text{Multinomial}(n, w(\lambda))\]</span></p>
<p>To illustrate, for a three-node model, with <span class="math inline">\(X, Y\)</span>, and <span class="math inline">\(M\)</span>—all binary—let <span class="math inline">\(n_{XYM}\)</span> denote an eight-element vector recording the number of cases in a sample displaying each possible combination of <span class="math inline">\(X,Y,M\)</span> data. Thus, the data <span class="math inline">\(d\)</span> can be summarized with a vector of counts of the form <span class="math inline">\(\mathbf n_{XYM}:=(n_{000},n_{001},n_{100},\ldots ,n_{111})\)</span>. The elements of <span class="math inline">\(n_{XYM}\)</span> sum to <span class="math inline">\(n\)</span>, the total number of cases studied. Likewise, let the event probabilities for data types given <span class="math inline">\(\lambda\)</span> be registered in a vector, <span class="math inline">\(w_{XYM}=(w_{000},w_{001},w_{100},\ldots ,w_{111})\)</span>. The likelihood of a data pattern, <span class="math inline">\(d\)</span>, given <span class="math inline">\(\lambda\)</span> is then:</p>
<p><span class="math display">\[
p(d|\lambda) = p(n_{XYM}|\lambda) =
  \text{Multinom}\left(n_{XYM}| \sum n_{XYM}, w_{XYM}(\lambda)\right)  \\
\]</span></p>
</section><section id="estimation" class="level4" data-number="9.2.2.4"><h4 data-number="9.2.2.4" class="anchored" data-anchor-id="estimation">
<span class="header-section-number">9.2.2.4</span> Estimation</h4>
<p> We now have all the components for updating on <span class="math inline">\(\lambda\)</span>. Applying Bayes rule (see <a href="05-being-Bayesian.html" class="quarto-xref"><span>Chapter 5</span></a>), we have:</p>
<p><span class="math display">\[p(\lambda | d) = \frac{p(d | \lambda)p(\lambda)}{\int_{\lambda'}{p(d | \lambda')p(\lambda')}}\]</span></p>
<p>In the <code>CausalQueries</code> package this updating is implemented in <code>stan</code>, and the result of the updating is a dataframe that contains a collection of draws from the posterior distribution for <span class="math inline">\(\lambda\)</span>. <a href="#tbl-posteriortable" class="quarto-xref">Table&nbsp;<span>9.4</span></a> illustrates what such a dataframe might look like for an <span class="math inline">\(X\rightarrow M \rightarrow Y\)</span> model. Each row represents a single draw from the posterior distribution, <span class="math inline">\(p(\lambda|d)\)</span>. The 10 columns correspond to the model’s 10 parameters: Each draw from <span class="math inline">\(\lambda\)</span>’s posterior distribution contains a set of population-level shares for each of the 10 nodal types in the model. </p>
<p>So, for instance, in the first row, we have one draw from our posterior distribution over <span class="math inline">\(\lambda\)</span>. In this draw, we have a world in which the shares of cases with nodal types <span class="math inline">\(\theta^X_0\)</span> and <span class="math inline">\(\theta^X_1\)</span> are 47% and 53%, respectively; the shares with <span class="math inline">\(\theta^M_{00}, \theta^M_{10}, \theta^M_{01}\)</span>, and <span class="math inline">\(\theta^M_{11}\)</span> are 21%, 7%, 17%, and 55%, respectively; and the shares for <span class="math inline">\(\theta^Y_{00}, \theta^Y_{10}, \theta^Y_{01}\)</span>, and <span class="math inline">\(\theta^Y_{11}\)</span> are 20%, 23%, 15%, and 41%, respectively. For each draw of <span class="math inline">\(\lambda\)</span>, these shares differ. <code>Stan</code> typically carries out thousands of draws to characterize the full joint posterior distribution over all parameters.</p>
<div class="cell" data-layout-align="center">
<div id="tbl-posteriortable" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-posteriortable-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;9.4: An illustration of a posterior distribution for a <span class="math inline">\(X \rightarrow M \rightarrow Y\)</span> model. Each row is a draw from <span class="math inline">\(p(\lambda|d))\)</span>. Such a posterior would typically have thousands of rows and capture the full joint posterior distribution over all parameters.
</figcaption><div aria-describedby="tbl-posteriortable-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<thead><tr class="header">
<th style="text-align: right;">X.0</th>
<th style="text-align: right;">X.1</th>
<th style="text-align: right;">M.00</th>
<th style="text-align: right;">M.10</th>
<th style="text-align: right;">M.01</th>
<th style="text-align: right;">M.11</th>
<th style="text-align: right;">Y.00</th>
<th style="text-align: right;">Y.10</th>
<th style="text-align: right;">Y.01</th>
<th style="text-align: right;">Y.11</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.78</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.07</td>
<td style="text-align: right;">0.61</td>
<td style="text-align: right;">0.10</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.11</td>
<td style="text-align: right;">0.81</td>
<td style="text-align: right;">0.07</td>
<td style="text-align: right;">0.01</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.54</td>
<td style="text-align: right;">0.46</td>
<td style="text-align: right;">0.09</td>
<td style="text-align: right;">0.05</td>
<td style="text-align: right;">0.09</td>
<td style="text-align: right;">0.78</td>
<td style="text-align: right;">0.33</td>
<td style="text-align: right;">0.03</td>
<td style="text-align: right;">0.02</td>
<td style="text-align: right;">0.61</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.28</td>
<td style="text-align: right;">0.72</td>
<td style="text-align: right;">0.61</td>
<td style="text-align: right;">0.01</td>
<td style="text-align: right;">0.10</td>
<td style="text-align: right;">0.28</td>
<td style="text-align: right;">0.44</td>
<td style="text-align: right;">0.35</td>
<td style="text-align: right;">0.05</td>
<td style="text-align: right;">0.16</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.42</td>
<td style="text-align: right;">0.58</td>
<td style="text-align: right;">0.50</td>
<td style="text-align: right;">0.24</td>
<td style="text-align: right;">0.16</td>
<td style="text-align: right;">0.10</td>
<td style="text-align: right;">0.23</td>
<td style="text-align: right;">0.37</td>
<td style="text-align: right;">0.29</td>
<td style="text-align: right;">0.11</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.92</td>
<td style="text-align: right;">0.08</td>
<td style="text-align: right;">0.10</td>
<td style="text-align: right;">0.09</td>
<td style="text-align: right;">0.74</td>
<td style="text-align: right;">0.07</td>
<td style="text-align: right;">0.17</td>
<td style="text-align: right;">0.18</td>
<td style="text-align: right;">0.51</td>
<td style="text-align: right;">0.14</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.88</td>
<td style="text-align: right;">0.12</td>
<td style="text-align: right;">0.04</td>
<td style="text-align: right;">0.27</td>
<td style="text-align: right;">0.28</td>
<td style="text-align: right;">0.40</td>
<td style="text-align: right;">0.10</td>
<td style="text-align: right;">0.20</td>
<td style="text-align: right;">0.67</td>
<td style="text-align: right;">0.03</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
</section><section id="sec-Qch9" class="level4" data-number="9.2.2.5"><h4 data-number="9.2.2.5" class="anchored" data-anchor-id="sec-Qch9">
<span class="header-section-number">9.2.2.5</span> Querying</h4>
<p>Once we have generated a posterior distribution for <span class="math inline">\(\lambda\)</span>, we can then query that distribution. The simplest queries relate to values of <span class="math inline">\(\lambda\)</span> itself. For instance, if we are interested in the probability that <span class="math inline">\(M\)</span> has a positive effect on <span class="math inline">\(Y\)</span>, given an updated <span class="math inline">\(X \rightarrow M \rightarrow Y\)</span> model, we want to know about the distribution of <span class="math inline">\(\lambda^Y_{01}\)</span>. This distribution can be read directly from column 9 (<span class="math inline">\(Y.01\)</span>) of Table <a href="#tbl-posteriortable" class="quarto-xref">Table&nbsp;<span>9.4</span></a>.</p>
<p>More complex queries can all be described as summaries of combinations of these columns. For instance, the query, “What is the average effect of <span class="math inline">\(M\)</span> on <span class="math inline">\(Y\)</span>” is a question about the distribution of <span class="math inline">\(\lambda^Y_{01} -  \lambda^Y_{10}\)</span>, which is given by the difference between columns 9 and 8 of <a href="#tbl-posteriortable" class="quarto-xref">Table&nbsp;<span>9.4</span></a>. This is a linear summary of parameters and is easily calculated.</p>
<p>Still more complex queries might ask about conditional quantities. Let <span class="math inline">\(\pi(Q|D)\)</span> denote the share of cases for which <span class="math inline">\(Q\)</span> is true, <em>among</em> those that have features <span class="math inline">\(D\)</span>. For instance, we could ask about the share of cases <em>among those that display <span class="math inline">\(M=1, Y=1\)</span></em> for which <span class="math inline">\(M\)</span> causes <span class="math inline">\(Y\)</span>. The condition <span class="math inline">\(D\)</span> could even be a causal quantity and is not necessarily observable: For instance, we might be interested in the share of cases <em>among those for which <span class="math inline">\(M\)</span> has a positive effect on <span class="math inline">\(Y\)</span></em> for which <span class="math inline">\(X\)</span> also has a positive effect on <span class="math inline">\(M\)</span>. Though more complex, we proceed in the same way for such “conditional queries,” calculating the value of the query for each possible value of <span class="math inline">\(\lambda\)</span> that we entertain and then taking the distribution over these values as given by our posterior of <span class="math inline">\(\lambda\)</span> itself.</p>
<p>Let <span class="math inline">\(\pi(Q|D, \lambda_i)\)</span> denote the <em>share</em> of cases for which our query, <span class="math inline">\(Q\)</span>, is satisfied, among those with condition <span class="math inline">\(D\)</span>, given a specific parameter draw, <span class="math inline">\(\lambda_i\)</span>. This could be, for instance, the share of cases with <span class="math inline">\(M=1, Y=1\)</span> for which <span class="math inline">\(M\)</span> causes <span class="math inline">\(Y\)</span>, under a single draw of <span class="math inline">\(\lambda\)</span> from its posterior. Similarly, let <span class="math inline">\(\pi(Q \&amp; D | \lambda_i)\)</span> denote the <em>share</em> of cases for which the query is satisfied and condition <span class="math inline">\(D\)</span> is present, given <span class="math inline">\(\lambda_i\)</span>. And let <span class="math inline">\(\pi(D | \lambda_i)\)</span> denote the <em>share</em> of cases with <span class="math inline">\(D\)</span> given <span class="math inline">\(\lambda_i\)</span>. Then, using the law of conditional probability, our conditional query under a single draw <span class="math inline">\(\lambda_i\)</span> is: <span class="math display">\[\pi(Q|D, \lambda_i) = \frac{\pi(Q  \&amp; D | \lambda_i)}{\pi(D | \lambda_i)}\]</span></p>
<p>For the conditional query about <em>the share of cases with <span class="math inline">\(M=1, Y=1\)</span> for which <span class="math inline">\(M\)</span> causes <span class="math inline">\(Y\)</span></em>, we can read this from <a href="#tbl-posteriortable" class="quarto-xref">Table&nbsp;<span>9.4</span></a> as the ratio of the second-to-last column to the sum of the last two columns: <span class="math inline">\(\frac{\lambda^Y_{01}}{\lambda^Y_{01} + \lambda^Y_{11}}\)</span>.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>We then have a posterior probability distribution over the query induced by our posterior distribution over <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(p(\lambda)\)</span>. We can calculate the <em>expected value</em> of our query’s posterior distribution as:</p>
<p><span class="math display">\[\hat{\pi}(Q|D, p) := \int \frac{\pi(Q  \&amp; D | \lambda_i)}{\pi(D | \lambda_i)}  p(\lambda_i)d\lambda_i\]</span></p>
<p>Here, we are essentially taking a weighted average of the different answers to our query across the different possible values of <span class="math inline">\(\lambda\)</span>, weighting each answer by the probability of the <span class="math inline">\(\lambda_i\)</span> from which it is derived.</p>
<p>Still, more complex queries may require keeping some nodes constant while varying others. For instance, we might imagine the impact of a change in <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> while keeping constant a mediator <span class="math inline">\(M\)</span> that lies on a path from <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span> (where there is a second, direct path from <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span>). Complex as such queries might be, they too can be calculated as summaries of the combinations of columns of the posterior distribution, following the rules described in <a href="04-causal-questions.html" class="quarto-xref"><span>Chapter 4</span></a>.</p>
<p>In all situations, once we have a distribution over the queries, we can calculate not just the expected value but also quantities such as the standard deviation of our posterior or the credibility intervals—a range of values over which 95% of our posterior probability mass lies.</p>
</section><section id="illustration" class="level4" data-number="9.2.2.6"><h4 data-number="9.2.2.6" class="anchored" data-anchor-id="illustration">
<span class="header-section-number">9.2.2.6</span> Illustration</h4>
<p><a href="#fig-HJ-F-9-4" class="quarto-xref">Figure&nbsp;<span>9.4</span></a> shows examples of a full mapping from data to posteriors for different data structures and queries. We begin with a simple chain model of the form <span class="math inline">\(X\rightarrow M \rightarrow Y\)</span>, with flat priors over all nodal types.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> In each column, we report inferences for a different query; and in each row, we report inferences for different data structures. For all data structures, we assume (for the sake of illustration) that we in fact observe a perfect positive correlation between <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>, and <span class="math inline">\(M\)</span>. However, across the rows, we vary which nodes and for how many cases we observe data.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-HJ-F-9-4" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-HJ-F-9-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="09-mixing-methods_files/figure-html/fig-HJ-F-9-4-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="960">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-HJ-F-9-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.4: Posterior means and credibility intervals for a range of causal queries given different data for a chain model.
</figcaption></figure>
</div>
</div>
</div>
<p>In the first four columns, we report queries about the shares of <span class="math inline">\(a, b, c\)</span>, and <span class="math inline">\(d\)</span> types in the population, referring to <span class="math inline">\(X\)</span>’s effect on <span class="math inline">\(Y\)</span>. As we discussed in defining case-level causal-effect queries in <a href="04-causal-questions.html#sec-casequery" class="quarto-xref"><span>Section 4.1</span></a>, the mediation of this effect by <span class="math inline">\(M\)</span> means that this query is asking a question about both <span class="math inline">\(\lambda^M\)</span> and <span class="math inline">\(\lambda^Y\)</span>. The next three columns ask about the average effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(M\)</span>, <span class="math inline">\(M\)</span> on <span class="math inline">\(Y\)</span>, and <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>. And the final column poses a conditional query, asking for what share of cases that display <span class="math inline">\(X=1, Y=1\)</span> does <span class="math inline">\(X\)</span> have a positive effect on <span class="math inline">\(Y\)</span>. As we can see, two features of our posterior beliefs shift as we add data: the expected value of the query and our degree of uncertainty.</p>
<p>For instance, as we go from 2 cases to 10 cases, and from just <span class="math inline">\(X,Y\)</span> data to observing <span class="math inline">\(M\)</span> as well, our beliefs about the proportion of positive-effect cases (including conditional on <span class="math inline">\(X=1, Y=1\)</span>) go up, and our beliefs about the proportion of no-effect and negative-effect cases go down—sensibly, given the strong positive correlations in the data. Interestingly, more data does not necessarily generate less uncertainty; this is because, for some queries, the data and our priors are pulling in opposite directions, and when we are only analyzing 10 or fewer cases, there aren’t enough data to overwhelm our priors. Also, movements from extreme values toward 0.5 can come with increased uncertainty. Ultimately, we can see in the last row that, with sufficiently large amounts of data, these credibility intervals shrink, and the mean of our posterior on the query approaches the “true” value.</p>
</section></section><section id="wrinkles" class="level3" data-number="9.2.3"><h3 data-number="9.2.3" class="anchored" data-anchor-id="wrinkles">
<span class="header-section-number">9.2.3</span> Wrinkles</h3>
<p>The basic procedure described above goes through with only minor adjustments when we have unobserved confounding or more complex sampling processes. We describe here how to take account of these features.</p>
<section id="unobserved-confounding" class="level4" data-number="9.2.3.1"><h4 data-number="9.2.3.1" class="anchored" data-anchor-id="unobserved-confounding">
<span class="header-section-number">9.2.3.1</span> Unobserved Confounding</h4>
<p></p>
<p>When there is unobserved confounding, we need parameter sets that allow for a joint distribution over nodal types. Unobserved confounding, put simply, means that there is confounding across nodes that is not captured by edges represented on the DAG. More formally, in the absence of unobserved confounding, we can treat the distribution of nodal types for a given node as independent of the distribution of nodal types for every other node. Unobserved confounding means that we believe that nodal types may be correlated across nodes. Thus, for instance, we might believe that those units assigned to <span class="math inline">\(X=1\)</span> have different potential outcomes for <span class="math inline">\(Y\)</span> than those assigned to <span class="math inline">\(X=0\)</span>—that is, that the probability of <span class="math inline">\(X=1\)</span> is not independent of whether or not <span class="math inline">\(X\)</span> has an effect on <span class="math inline">\(Y\)</span>. To allow for this, we have to allow <span class="math inline">\(\theta^X\)</span> and <span class="math inline">\(\theta^Y\)</span> to have a joint distribution. There are different ways to do this in practice, but a simple approach is to split the parameter set corresponding to the <span class="math inline">\(Y\)</span> node into two: We specify one distribution for <span class="math inline">\(\theta^Y\)</span> when <span class="math inline">\(X=0\)</span> and a separate distribution for <span class="math inline">\(\theta^Y\)</span> when <span class="math inline">\(X=1\)</span>. For each of these parameter sets, we specify four <span class="math inline">\(\alpha\)</span> parameters representing our priors. We can draw <span class="math inline">\(\lambda\)</span> values for these conditional nodal types from the resulting Dirichlet distributions, as above, and can then calculate causal type probabilities in the usual way. Note that if we do this in an <span class="math inline">\(X  \rightarrow Y\)</span> model, we have one two-dimensional Dirichlet distribution corresponding to <span class="math inline">\(X\)</span> and two four-dimensional distributions corresponding to <span class="math inline">\(Y\)</span>. In all, with 1+3+3 degrees of freedom: Exactly the number needed to represent a joint distribution over all eight <span class="math inline">\(\theta^X, \theta^Y\)</span> combinations. </p>
<p>In <a href="#fig-HJ-F-9-5" class="quarto-xref">Figure&nbsp;<span>9.5</span></a>, we represent this confounding for a model with direct and indirect effects by indicating parameters values <span class="math inline">\(\lambda_{MY}\)</span> that determine the joint distribution over <span class="math inline">\(\theta_M\)</span> and <span class="math inline">\(\theta_Y\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-HJ-F-9-5" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-HJ-F-9-5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="09-mixing-methods_files/figure-html/fig-HJ-F-9-5-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:95.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-HJ-F-9-5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.5: Types parameters and priors with confounding
</figcaption></figure>
</div>
</div>
</div>
</section><section id="sampling-and-the-likelihood-principle" class="level4" data-number="9.2.3.2"><h4 data-number="9.2.3.2" class="anchored" data-anchor-id="sampling-and-the-likelihood-principle">
<span class="header-section-number">9.2.3.2</span> Sampling and the Likelihood Principle</h4>
<p> </p>
<p>When we constructed the likelihood function—the probability of observing data given model parameters—we did not say much about how data were gathered. But surely <em>how</em> cases are sampled affects the probability of seeing different types of data and so affects the likelihood function. Ought we have different likelihood functions, for instance, if we decided to look for data on <span class="math inline">\(K\)</span> only in places in which we have already observed <span class="math inline">\(X=1\)</span> and <span class="math inline">\(Y=1\)</span>, or if we selected cases in which to examine <span class="math inline">\(K\)</span> without taking into account known values of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>? Do we need to take account of such details when making inference?</p>
<p>The answer depends on whether and how details of sampling affect the likelihood of seeing different data patterns. In general, we can invoke the “likelihood principle,” which is the principle that the relevant information for inference is contained in the likelihood. If sampling strategies don’t alter the likelihood of observing data, then we can ignore them. In fact, since what matters is the relative likelihoods, we can treat two likelihood functions as equivalent if they are scalar multiples of each other. Thus, for instance, we can think of <span class="math inline">\(\lambda^X\lambda^Y\)</span> as equivalent to <span class="math inline">\(2\lambda^X\lambda^Y\)</span>. </p>
<p>Here are two general rules of thumb:</p>
<ul>
<li>Strategies in which a unit’s probability of selection into a sample is not related to its own potential outcomes can likely be ignored.</li>
<li>Sampling strategies in which a unit’s probability of selection into the sample is related to its own potential outcomes likely cannot be ignored.</li>
</ul>
<p>To illustrate, let’s consider a set of strategies that can be treated equivalently. We imagine an <span class="math inline">\(X \rightarrow M \rightarrow Y\)</span> model and suppose we have data on two cases: one case in which we see data on <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> only, observing <span class="math inline">\(X=0, Y=0\)</span>, and another in which in we have data on <span class="math inline">\(X,M\)</span>, and <span class="math inline">\(Y\)</span>, observing <span class="math inline">\(X=1, M = 0\)</span>, and <span class="math inline">\(Y=1\)</span>. Further, let <span class="math inline">\(P(X=x, Y=y)\)</span> denote the probability that we find <span class="math inline">\(X=x\)</span> and <span class="math inline">\(Y=y\)</span> when we seek data on <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>Consider now three strategies that we might have used to gather these data.</p>
<p><strong>Strategy 1:</strong> <em>Single multinomial draw.</em> For each case, we could have randomly decided, with equal probability, whether or not to select data on <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> only or on <span class="math inline">\(X, M\)</span>, and <span class="math inline">\(Y\)</span>. Each case then had 12 possible data types (4 possible <span class="math inline">\(X,Y\)</span> data types and 8 possible <span class="math inline">\(X,M,Y\)</span> data types). The probability of data type <span class="math inline">\(X=0, Y=0\)</span>, for instance, is <span class="math inline">\(0.5P(X=0, Y=0)\)</span>. The probability of observing the data we do observe is then:</p>
<p><span class="math display">\[2\times\frac12P(X=0, Y=0)\times\frac12P(X=1, M=0, Y=1)\]</span></p>
<p><strong>Strategy 2.</strong> <em>Conditional (sequential) multinomial draws.</em></p>
<p>We could have collected data on <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> in two cases, and we then measured <span class="math inline">\(M\)</span> in every case in which we observed <span class="math inline">\(X=1, Y=1\)</span>. For this strategy, the probability of observing the data that we do observe is the probability of observing exactly one case with <span class="math inline">\(X=1,  Y=1\)</span> and another with <span class="math inline">\(X=0,  Y=0\)</span>, times the probability of observing <span class="math inline">\(M=1\)</span> in the case in which we observed <span class="math inline">\(X=1, Y=1\)</span>.</p>
<p><span class="math display">\[2P(X=0, Y=0)P(X=1, Y=1)P(M=0 | X=1, Y=1)\]</span></p>
<p>which is equivalent to:</p>
<p><span class="math display">\[2P(X=0, Y=0)P(X=1, M=0, Y=1)\]</span></p>
<p><strong>Strategy 3:</strong> <em>Parallel multinomial draws</em></p>
<p>We could have sampled two cases and simultaneously examined <span class="math inline">\(X,Y\)</span> in the first case and <span class="math inline">\(X, M, Y\)</span> in the second case. The probability of seeing the data we see is then:</p>
<p><span class="math display">\[2P(X=0, Y=0)P(X=1, M=0, Y=1)\]</span></p>
<p>We can readily see that, for all three strategies, the probability of observing the data we do in fact observe has the same form, albeit with possibly different constants. In other words, the differences in sampling across these strategies can be ignored.</p>
<p>Some differences in sampling procedures do have to be taken into account, however: in particular, sampling—or more generally missingness—that is related to potential outcomes. For a simple illustration, consider an <span class="math inline">\(X \rightarrow Y\)</span> model where data are only recorded in cases in which <span class="math inline">\(Y=1\)</span>. Thus, the observed data can have variation on <span class="math inline">\(X\)</span> but not on <span class="math inline">\(Y\)</span>. Naive updating that ignored the sampling process here would lead us to infer that <span class="math inline">\(Y=1\)</span> regardless of <span class="math inline">\(X\)</span>, and thus that <span class="math inline">\(X\)</span> has no effect on <span class="math inline">\(Y\)</span>. The problem here is that the likelihood is not taking account of the process through which cases enter our dataset.</p>
<p>In this situation, the correct likelihood would use event probabilities that consider the possible data types under the strategy. Let <span class="math inline">\(D^*\)</span> denote the set of data types that are observable under the strategy (here <span class="math inline">\(D^*\)</span> is the set of data types involving <span class="math inline">\(Y=1\)</span>). Then event probabilities are: <span class="math display">\[w_d =  \left\{ \begin{array}{cc}
0 &amp; \text{if } d\not\in D^* \\
\frac{x_d}{\sum_{d^\prime\in D^*}x_{d^\prime}} &amp; \text{otherwise} \end{array} \right.\]</span></p>
<p>where <span class="math inline">\(x_d = \sum_{\theta \in \Theta(d)}p(\theta|\lambda)\)</span> is the uncensored data event probability.</p>
<p>An example of such sampling is the problem discussed in <span class="citation" data-cites="knox2020administrative">Knox, Lowe, and Mummolo (<a href="20-references.html#ref-knox2020administrative" role="doc-biblioref">2020</a>)</span> where reporting of police encounters depends on the outcome of those encounters.</p>
<p>While this kind of sampling can sometimes be handled relatively easily,<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> the general principle holds that sampling (missingness) that is related to potential outcomes is a part of the data-generating process and needs to be taken into account in the likelihood. On strategies for addressing nonrandom sampling by blocking, see <span class="citation" data-cites="bareinboim2016causal">Bareinboim and Pearl (<a href="20-references.html#ref-bareinboim2016causal" role="doc-biblioref">2016</a>)</span>.</p>
</section></section></section><section id="payoffs" class="level2" data-number="9.3"><h2 data-number="9.3" class="anchored" data-anchor-id="payoffs">
<span class="header-section-number">9.3</span> Payoffs</h2>
<p>The most straightforward payoff to this approach is that we can learn about causal relationships in a population of interest from any number of cases drawn from that population. We can then use the updated model to ask causal questions about the population of interest or about individual cases within that population. In this section, we elaborate on three additional things that a causal-model-based approach to multi-case causal inference allows us to do: to integrate information from extensive and intensive data strategies; to empirically derive and justify the probative value of our process-tracing clues; and to learn about causal relationships even when they are not identified.</p>
<section id="mmpayoff" class="level3" data-number="9.3.1"><h3 data-number="9.3.1" class="anchored" data-anchor-id="mmpayoff">
<span class="header-section-number">9.3.1</span> Mixing Methods</h3>
<p>Having described </p>
<p>Having described the basic procedure, it is relatively straightforward now to explain what we mean when we say we can use this approach to mix methods. The notion of “mixing methods” can, of course, imply many things. What we mean in particular is that we can mix <em>evidence drawn from any combination of data strategies.</em> One common mixed-method research design in the social sciences involves combining (1) “extensive” data, meaning observations of a few variables for a large set of cases with (2) “intensive” data, meaning more in-depth observations for a small set of cases, usually a subset of the larger sample. The approach we have outlined can readily handle this kind of data mixture, and this is the kind of mixed strategy we will usually address in this book. More generally, though, as long as all data involve observations of nodes represented in our model, the framework can handle any arbitrary mixture of data structures.</p>
<p>The key features of the approach that allow for mixing are that we need neither data on <em>all</em> nodes nor data on the <em>same</em> nodes for all cases in order to implement the procedure. Whatever the data structure, we simply update our beliefs using whatever information we have.</p>
<p>The <code>CausalQueries</code> package will automatically perform updating on any arbitrary mixture of data structures we provide it with, but here is the basic idea. The logic is akin to that which we employ with partial process-tracing data (see <a href="07-process-tracing-with-models.html#sec-updatingtypes" class="quarto-xref"><span>Section 7.2.4</span></a>). Suppose we have a data strategy <span class="math inline">\(s\)</span> under which we gather data on <span class="math inline">\(n_s\)</span> units for a subset of nodes, <span class="math inline">\(V_s\)</span>. In calculating the probability of a pattern of partial data, we use all columns (data types) in the ambiguities matrix that are consistent with the partial data in order to calculate the event probability <span class="math inline">\(w_s\)</span>. Our overall data strategy might involve multiple strategies like this.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> If units are randomly assigned to data strategies and the observed number of units for each data type under each data strategy, <span class="math inline">\(s\)</span>, is captured in the vector <span class="math inline">\(m_s\)</span>,<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> then the likelihood is: </p>
<p><span class="math display">\[L = \prod_s \text{Multinom}(m_s|n_s, w_s)\]</span> <!-- Illustration --> That is, the likelihood of a given mixed data pattern is simply the product, across strategies, of the likelihood of observing the number of units that we observe of each possible data type for each strategy, given the number of cases observed under each strategy and the likelihood of each data type emerging under each strategy.</p>
<p>To illustrate, consider a model with nodes <span class="math inline">\(X, M\)</span>, and <span class="math inline">\(Y\)</span>. Suppose we have collected <span class="math inline">\(X,Y\)</span> data for a set of cases, and have additionally collected data on <span class="math inline">\(M\)</span> for a random subset of these—akin to conducting quantitative analysis on a large sample while conducting in-depth process tracing on part of the large-<span class="math inline">\(N\)</span> sample. We can then summarize our data in two vectors, an eight-element <span class="math inline">\(n_{XYM}\)</span> vector <span class="math inline">\((n_{000},n_{001},\ldots n_{111})\)</span> for the cases with process-tracing (<span class="math inline">\(M\)</span>) observations, and a four-element vector <span class="math inline">\(n_{XY*} = (n_{00*},n_{10*},n_{01*},n_{11*})\)</span> for the partial data on those cases on for which we did not conduct process tracing. Likewise, we now have two sets of data probabilities: an eight-element vector for the set of cases with complete data, <span class="math inline">\(w_{XYM}\)</span>, and a four-element vector for those with partial data, <span class="math inline">\(w_{XY*}\)</span>.</p>
<p>Let <span class="math inline">\(n\)</span> denote the total number of cases examined, and <span class="math inline">\(k\)</span> the number for which we have data on <span class="math inline">\(M\)</span>. Assuming that each observed case represents an independent, random draw from the population, we form the likelihood function quite simply as:</p>
<p><span class="math display">\[\Pr(\mathcal{D}|\theta) =
  \text{Multinom}\left(n_{XY*}|n-k, w_{XY*}\right) \times \text{Multinom}\left(n_{XYM}|k, w_{XYM}\right)\]</span></p>
<p>That is, the likelihood of observing the mixed data pattern is the likelihood of observing the data we see in the non-process-traced cases (given the number of those cases and the event probability for each <span class="math inline">\(X,Y\)</span> data type) times the likelihood of observing the data we see in the process-traced cases (given the number of those cases and the event probability for each <span class="math inline">\(X,M,Y\)</span> data type).</p>
</section><section id="sec-casepop" class="level3" data-number="9.3.2"><h3 data-number="9.3.2" class="anchored" data-anchor-id="sec-casepop">
<span class="header-section-number">9.3.2</span> Deriving Probative Value from the Data</h3>
<p></p>
<p>In <a href="07-process-tracing-with-models.html" class="quarto-xref"><span>Chapter 7</span></a>, we discussed the fact that a DAG by itself is often insufficient to generate learning about causal effects from data on a single case. For many queries, a causal structure alone cannot make nodes on the graph informative as clues about causal relations. We also need to provide nonuniform prior beliefs about the population-level shares of nodal types.</p>
<p>When working with multiple cases, however, we can learn about causal relations starting with nothing more than the DAG and data. Learning about causal relations from the data can, in turn, <em>generate and justify the probative value of process-tracing clues</em>—that is, without the researcher having to <em>posit</em> any beliefs about nodal-type shares. In other words, we can simultaneously learn about population-level queries and empirically justify inferences we might make about new cases using case-level data.</p>
<p>For intuition, if we start with a simple model of the form <span class="math inline">\(X \rightarrow Y \leftarrow K\)</span>, and have flat priors over causal types, then knowledge of <span class="math inline">\(K\)</span> is uninformative about whether <span class="math inline">\(X\)</span> caused <span class="math inline">\(Y\)</span> in a case. But imagine that we observe data on <span class="math inline">\(X, K\)</span>, and <span class="math inline">\(Y\)</span> for multiple cases and find a strong correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> only when <span class="math inline">\(K=1\)</span>. Now an inference that <span class="math inline">\(X\)</span> mattered for <span class="math inline">\(Y\)</span> in a case after seeing <span class="math inline">\(K=1\)</span> can be justified by the updated model. That is, the model has <em>learned</em> that <span class="math inline">\(K\)</span> is <span class="math inline">\(1\)</span> more often in cases where it is likely that <span class="math inline">\(X\)</span> affected <span class="math inline">\(Y\)</span>. The data plus the DAG—without informative priors—have generated a probative value for our clue, <span class="math inline">\(K\)</span>, which we can then leverage for process tracing. With real data, we show an example of this kind of learning in our multi-case analysis of the institutions and growth model in <a href="10-mixed-application.html" class="quarto-xref"><span>Chapter 10</span></a>.</p>
<p>This represents a key integrative opportunity for model-based inference: A population-level model, updated on data from multiple cases, can allow us to empirically justify the causal inferences that we make about individual cases when we observe case-level data. To be clear, we imagine here that we first update our model using data from multiple cases, and then bring the updated model to an individual case—using the model to tell us what we should believe about the case given a set of observations from that case.</p>
<section id="two-types-of-case-level-conditional-inferences" class="level4" data-number="9.3.2.1"><h4 data-number="9.3.2.1" class="anchored" data-anchor-id="two-types-of-case-level-conditional-inferences">
<span class="header-section-number">9.3.2.1</span> Two Types of Case-Level Conditional Inferences</h4>
<p></p>
<p>We must be careful, however, about what we mean by case-level inference following model-updating. Generally speaking, case-level inference means asking about the probability that query <span class="math inline">\(Q\)</span> is true for a unit with observed characteristics <span class="math inline">\(D\)</span>. For instance, we might want to know about the probability that <span class="math inline">\(X\)</span> caused <span class="math inline">\(Y\)</span> in a case with <span class="math inline">\(X=1, Y=1\)</span>, and <span class="math inline">\(K=1\)</span>. But there are two ways in which we might interpret this question and seek an answer from an updated model. We will refer to these two similar-but-distinct types of questions as an <em>uninformative</em>-case query and an <em>informative</em>-case query. </p>
<p><em>Uninformative-case query</em>. With an updated model in hand, we can ask: What is the probability that <span class="math inline">\(Q\)</span> is true for a case of interest that has characteristics <span class="math inline">\(D\)</span>? In this setup, we have selected the case for inference <em>because</em> it has characteristics <span class="math inline">\(D\)</span><sub>–</sub>for example, we have randomly drawn the case from among those that have <span class="math inline">\(D\)</span>—and we have a question about this kind of case. If we can treat this case as undifferentiated in expectation from other units with <span class="math inline">\(D\)</span> in the population, then we can treat the <em>share</em> of cases in the population with <span class="math inline">\(D\)</span> for which the query is satisfied as the <em>probability</em> with which <span class="math inline">\(Q\)</span> is true for the case of interest. Thus, if our updated model tells us that <span class="math inline">\(X\)</span> causes <span class="math inline">\(Y\)</span> for 80% of cases in the population with <span class="math inline">\(X=1, Y=1\)</span>, and <span class="math inline">\(K=1\)</span>, then our best guess for any case with these observed features, absent other data, is that there is an 80% probability that <span class="math inline">\(X\)</span> causes <span class="math inline">\(Y\)</span> in this case.</p>
<p><em>Informative-case query</em>. Say that instead of randomly sampling a case <em>from among the cases that have <span class="math inline">\(D\)</span></em>, we were to randomly select a case from the population and <em>observe</em> that this new case has characteristics <span class="math inline">\(D\)</span>. Then, what should we believe about <span class="math inline">\(Q\)</span>?</p>
<p>Things are different now because the observation of <span class="math inline">\(D\)</span> in a randomly sampled case is now new information, and this additional data may lead us to update our causal model, even as we query it.</p>
<p>To calculate <em>uninformative case queries</em>, we make use of our posterior beliefs about the <em>share</em> of units in a population that satisfy the conditional query. This is quantity <span class="math inline">\(\pi(Q|D, p)\)</span> that we discussed in <a href="#sec-Qch9" class="quarto-xref"><span>Section 9.2.2.5</span></a>, where <span class="math inline">\(p\)</span> now is our posterior distribution over <span class="math inline">\(\lambda\)</span>. We use the <em>expected value</em> of this posterior distribution over the conditional query to answer the uninformative case query:</p>
<p><span class="math display">\[\hat{\pi}(Q|D, p) := \int \frac{\pi(Q  \&amp; D | \lambda_i)}{\pi(D | \lambda_i)}  p(\lambda_i)d\lambda_i\]</span></p>
<p>For the informative case query—what should we believe about <span class="math inline">\(Q\)</span> for a randomly selected case in which we <em>observe</em> <span class="math inline">\(D\)</span>—we need to take into account the new information that <span class="math inline">\(D\)</span>’s observation represents. That is, we need to allow for updating on our posterior distribution over <span class="math inline">\(\lambda\)</span> given the new observation. We thus use the law of conditional probability to calculate:</p>
<p><span class="math display">\[\hat{\phi}(Q|D, p) := \frac{\int \Pr(Q  \&amp; D |  \lambda_i)p(\lambda_i)d\lambda_i}{\int \Pr(D |  \lambda_i)p(\lambda_i)d\lambda_i}= \frac{\int \pi(Q  \&amp; D |  \lambda_i)p(\lambda_i)d\lambda_i}{\int \pi(D |  \lambda_i)p(\lambda_i)d\lambda_i}\]</span> Note we have made use of the fact that for a single case <span class="math inline">\(\Pr(Q  \&amp; D |  \lambda_i)= \pi(Q  \&amp; D |  \lambda_i)\)</span>.</p>
<p>In this calculation, we have a value for <span class="math inline">\(\pi(Q  \&amp; D)\)</span> for each possible <span class="math inline">\(\lambda_i\)</span>, as in the uninformative-case query. The key difference is that observing <span class="math inline">\(D\)</span> can now lead us to shift probability toward those <span class="math inline">\(\lambda_i\)</span>’s under which the observation of <span class="math inline">\(D\)</span> was more likely to occur—and in turn toward those answers to our query (those <span class="math inline">\(\pi(Q  \&amp; D)\)</span> values) implied by those now-more-probable <span class="math inline">\(\lambda_i\)</span>’s. Put differently, in an informative-case query, the case-level data do not just give us information about the kind of case we’re examining; they can also provide new information about the way causal relations operate in the world we’re in (i.e., about <span class="math inline">\(\lambda\)</span>), informing how we interpret the evidence we see in the case.</p>
<p>Formally, <span class="math inline">\(\hat{\phi}\)</span> and <span class="math inline">\(\hat{\pi}\)</span> look quite similar, and the differences between them are somewhat subtle. They relate to each other in a simple way, however. If we let <span class="math inline">\(p'(\lambda_i)\)</span> denote the posterior on <span class="math inline">\(\lambda\)</span> after seeing data <span class="math inline">\(D\)</span> on a new case, then:</p>
<p><span class="math display">\[p'(\lambda_i) =  \frac{\Pr(D|\lambda_i)p(\lambda_i)}{\int \Pr(D|\lambda_i')p(\lambda_i')d\lambda_i'}\]</span></p>
<p>And then:</p>
<p><span class="math display">\[\begin{eqnarray}
\hat{\phi}(Q|D, p) &amp;=&amp; \frac{\int \Pr(Q  \&amp; D |  \lambda_i)p(\lambda_i)d\lambda_i}{\int \Pr(D |  \lambda_i)p(\lambda_i)d\lambda_i} \\
&amp;=&amp; \int \frac{\Pr(Q  \&amp; D | \lambda_i)}{\Pr(D | \lambda_i)}  \frac{\Pr(D|\lambda_i)p(\lambda_i)}{\int \Pr(D|\lambda_i')p(\lambda_i')d\lambda_i'} d\lambda_i \\
&amp;=&amp; \int \frac{\Pr(Q  \&amp; D | \lambda_i)}{\Pr(D | \lambda_i)}  p'(\lambda_i)d\lambda_i \\
&amp;=&amp; \hat{\pi}(Q|D, p')
\end{eqnarray}\]</span></p>
<p>In other words, posing a <span class="math inline">\(\hat{\phi}\)</span> query about a new “informative” case is equivalent to first using that new case to update <span class="math inline">\(\lambda\)</span> and then posing a <span class="math inline">\(\hat{\pi}\)</span> query about the case using the posterior distribution on <span class="math inline">\(\lambda\)</span>. The one thing we have to be sure <em>not</em> to do is to first use the new case to update on <span class="math inline">\(\lambda\)</span> and then pose a <span class="math inline">\(\hat{\phi}\)</span> query about the same case—since that would effectively be updating <span class="math inline">\(\lambda\)</span> twice from the same case data.</p>
</section><section id="when-new-cases-carry-new-information" class="level4" data-number="9.3.2.2"><h4 data-number="9.3.2.2" class="anchored" data-anchor-id="when-new-cases-carry-new-information">
<span class="header-section-number">9.3.2.2</span> When New Cases Carry New Information</h4>
<p>The difference between uninformative- and informative-case queries turns on the nature of uncertainty over the conditioning information, <span class="math inline">\(D\)</span>. When undertaking an uninformative-case query, we have no uncertainty about whether we will observe <span class="math inline">\(D\)</span> in the case: The case of interest has been selected <em>for</em> its display of <span class="math inline">\(D\)</span>. In an informative-case query, because we don’t condition selection on <span class="math inline">\(D\)</span>, we don’t know whether we will observe <span class="math inline">\(D\)</span> in the case before we select it: Thus, observing <span class="math inline">\(D\)</span> can potentially tell us something about the world (about <span class="math inline">\(\lambda\)</span>).</p>
<p>By the same token, if the likelihood of observing <span class="math inline">\(D\)</span> in a randomly selected case is the same under all beliefs we might have about the world, then we will <em>not</em> update those beliefs when we observe <span class="math inline">\(D\)</span>. The informative-case query then collapses into an uninformative one. In fact, comparing expressions <span class="math inline">\(\hat{\pi}(Q|D)\)</span> and <span class="math inline">\(\hat{\phi}(Q|D)\)</span> above, we can see that if <span class="math inline">\(\Pr(D)\)</span> is constant over <span class="math inline">\(\lambda_i\)</span>, then <span class="math inline">\(\hat{\pi}(Q|D)=\hat{\phi}(Q|D)\)</span>.</p>
<p>For instance, we will not update on <span class="math inline">\(\lambda\)</span> from observing <span class="math inline">\(D\)</span> in a randomly selected case if <span class="math inline">\(D\)</span>’s distribution in the population is <em>known</em>. Suppose that we are interested in a subgroup effect—for instance, the conditional average treatment effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> for democracies and for non-democracies—and that the relative sizes of these two subgroups are already known. Then, when we randomly draw a case and observe that it is a democracy, we do not learn anything about the world: The likelihood of having observed a democracy in a randomly drawn case is the same under all values of <span class="math inline">\(\lambda_i\)</span>. So <span class="math inline">\(\hat{\pi}(Q|D)\)</span> and <span class="math inline">\(\hat{\phi}(Q|D)\)</span> are the same. Thus, if we think there is a positive effect in 50% of democracies in the population and in 30% of the non-democracies, then we think that the probability that there is an effect in a new random case is 0.5 if it is a democracy and 0.3 if it is a non-democracy.</p>
<p>While this seems like a straightforward equivalence, it depends crucially on the fact that we <em>know</em> the share of democracies and non-democracies in the population in advance of drawing the case. If we didn’t, then observing democracy in the new case <em>could</em> alter our beliefs about <span class="math inline">\(\lambda\)</span>.</p>
<p>Similarly, <span class="math inline">\(\hat{\pi}(Q|D)\)</span> and <span class="math inline">\(\hat{\phi}(Q|D)\)</span> will be the same for any case-level query for which <span class="math inline">\(D\)</span> is the empty set—that is, for which we condition on no observed characteristic. For instance, if our query is simply the probability that <span class="math inline">\(X\)</span> has a positive effect on <span class="math inline">\(Y\)</span> in a case, then <span class="math inline">\(\hat{\pi}(Q|D)\)</span> will simply be the share of cases in the population with a positive effect. For <span class="math inline">\(\hat{\phi}(Q|D)\)</span>, we are not making use of any information from the newly drawn case and so will not update on <span class="math inline">\(\lambda\)</span>, we will just use the very same population share used for <span class="math inline">\(\hat{\pi}(Q|D)\)</span>.</p>
<p>Conversely, if <span class="math inline">\(\Pr(D)\)</span> is not constant over <span class="math inline">\(\lambda_i\)</span> then <span class="math inline">\(\hat{\phi}(Q|D)\)</span> can differ from <span class="math inline">\(\hat{\pi}(Q|D)\)</span>: That is, our case-level inference may differ depending on whether we selected the case <em>because</em> it displayed <span class="math inline">\(D\)</span> or we discovered <span class="math inline">\(D\)</span> after randomly drawing the case. For this reason, it is in principle possible to arrive at two <em>seemingly</em> contradictory inferences at the same time: We can simultaneously figure that <span class="math inline">\(\hat{\pi}(X \text{ causes } Y|K=1, p)\)</span> is very small and that <span class="math inline">\(\hat{\phi}(X \text{ causes } Y|K=1,p)\)</span> is very large. In other words, you could believe that among units for which (K=1), it is unlikely that there is an effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>, while at the same time observing <span class="math inline">\(K=1\)</span> in a new case could be enough to convince you that <span class="math inline">\(X\)</span> caused <span class="math inline">\(Y\)</span> for the case.</p>
<p>A quite stark example can illustrate how this can be possible. Imagine that we have a model in which <span class="math inline">\(Y\)</span> is a function of both <span class="math inline">\(X\)</span> and <span class="math inline">\(K\)</span>: <span class="math inline">\(X \rightarrow Y \leftarrow K\)</span>. Suppose, further, we entertained just two possible nodal types for <span class="math inline">\(\theta^Y\)</span>:</p>
<ol type="1">
<li>
<span class="math inline">\(\theta^Y_{1}\)</span>: <span class="math inline">\(Y=1\)</span> if and only if both <span class="math inline">\(X=1\)</span> and <span class="math inline">\(K=1\)</span>; we let <span class="math inline">\(\lambda^Y_{1}\)</span> denote the share of cases with <span class="math inline">\(\theta^Y = \theta^Y_{1}\)</span>
</li>
<li>
<span class="math inline">\(\theta^Y_{2}\)</span>: <span class="math inline">\(Y=0\)</span> regardless of <span class="math inline">\(X\)</span> and <span class="math inline">\(K\)</span>; we let <span class="math inline">\(\lambda^Y_{2}\)</span> denote the share of cases with <span class="math inline">\(\theta^Y = \theta^Y_{2}\)</span>
</li>
</ol>
<p>We also let <span class="math inline">\(\lambda^K_1\)</span> denote the share of cases in which <span class="math inline">\(K=1\)</span>.</p>
<p>We then imagine two possible worlds that we might be in, <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span>, described in <a href="#tbl-HJ-T-9-states" class="quarto-xref">Table&nbsp;<span>9.5</span></a>.</p>
<div class="cell" data-layout-align="center">
<div id="tbl-HJ-T-9-states" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-HJ-T-9-states-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;9.5: Beliefs over two states of the world, where information on a new case leads to updating about the state of the world
</figcaption><div aria-describedby="tbl-HJ-T-9-states-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<thead><tr class="header">
<th style="text-align: center;"><span class="math inline">\(\lambda\)</span></th>
<th style="text-align: center;"><span class="math inline">\(p(\lambda)\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\lambda^Y_1\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\lambda^Y_2\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\lambda^K_0\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\lambda^K_1\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(\lambda_1\)</span></td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.999</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(\lambda_2\)</span></td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.999</td>
<td style="text-align: center;">0.001</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>Note that we start out believing there is a 0.01 probability that we are in <span class="math inline">\(\lambda_1\)</span> and a 0.99 probability that we are in <span class="math inline">\(\lambda_2\)</span>.</p>
<p>For the query <span class="math inline">\(\hat{\pi}(X \text{ causes } Y|K=1)\)</span>, we ask: <em>What is the probability that <span class="math inline">\(X\)</span> causes <span class="math inline">\(Y\)</span> in a case with <span class="math inline">\(K=1\)</span></em>? This is the same as asking for what share of <span class="math inline">\(K=1\)</span> cases in the population <span class="math inline">\(X\)</span> does cause <span class="math inline">\(Y\)</span>. The answer is 100% if <span class="math inline">\(\lambda=\lambda_1\)</span> and 0% if <span class="math inline">\(\lambda=\lambda_2\)</span>. So, given the probability our model places on the two worlds, the expected share is <span class="math inline">\(p(\lambda_1) = 0.01\)</span>. Thus, if we were to present you with a case randomly selected from those with <span class="math inline">\(K=1\)</span>, we would say it is very unlikely that <span class="math inline">\(X\)</span> causes <span class="math inline">\(Y\)</span> for that case.</p>
<p>For the query <span class="math inline">\(\hat{\phi}(X \text{ causes } Y|K=1)\)</span>, we ask: <em>What is the probability that <span class="math inline">\(X\)</span> causes <span class="math inline">\(Y\)</span> in a randomly drawn case in which we then observe <span class="math inline">\(K=1\)</span>?</em> The likelihoods of observing <span class="math inline">\(K=1\)</span> or <span class="math inline">\(K=0\)</span> for a randomly drawn case, given different beliefs about <span class="math inline">\(X\)</span>’s effect, are shown in <a href="#tbl-HJ-T-9-twoworlds" class="quarto-xref">Table&nbsp;<span>9.6</span></a>.</p>
<div id="tbl-HJ-T-9-twoworlds" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-HJ-T-9-twoworlds-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;9.6: Case level inference given new case data
</figcaption><div aria-describedby="tbl-HJ-T-9-twoworlds-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead><tr class="header">
<th></th>
<th style="text-align: center;"><span class="math inline">\(K = 0\)</span></th>
<th style="text-align: center;"><span class="math inline">\(K =1\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>
<span class="math inline">\(\lambda_1\)</span> (<span class="math inline">\(X\)</span> causes <span class="math inline">\(Y\)</span>)</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr class="even">
<td>
<span class="math inline">\(\lambda_2\)</span> (<span class="math inline">\(X\)</span> doesn’t cause <span class="math inline">\(Y\)</span>)</td>
<td style="text-align: center;">0.989</td>
<td style="text-align: center;">0.001</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>We can read <span class="math inline">\(\hat{\phi}(X \text{ causes } Y|K=1)\)</span> off of this table: The probability that <span class="math inline">\(X\)</span> causes <span class="math inline">\(Y\)</span> given the observation of <span class="math inline">\(K=1\)</span> is <span class="math inline">\(0.01/(0.01 + 0.001)=0.91\)</span>.</p>
<p>So, we think that the typical <span class="math inline">\(K=1\)</span> case has a very low probability of being one in which <span class="math inline">\(X\)</span> causes <span class="math inline">\(Y\)</span> because our model tells us we’re very likely in a world (that of <span class="math inline">\(\lambda_2\)</span>) in which <span class="math inline">\(X\)</span> never causes <span class="math inline">\(Y\)</span>. Yet the likely world (where <span class="math inline">\(\lambda_2\)</span>) is <em>also</em> a world in which we should almost never observe <span class="math inline">\(K=1\)</span>. Thus, <em>finding out</em> that <span class="math inline">\(K=1\)</span> in a randomly drawn case allows us to update to a belief that we are more likely in the world of <span class="math inline">\(\lambda_1\)</span>—where <span class="math inline">\(X\)</span> indeed causes <span class="math inline">\(Y\)</span> whenever <span class="math inline">\(K=1\)</span>. In other words, our prior beliefs about the world can be upended by what we see in new cases, in turn changing how we understand those cases.</p>
</section></section><section id="learning-without-identification" class="level3" data-number="9.3.3"><h3 data-number="9.3.3" class="anchored" data-anchor-id="learning-without-identification">
<span class="header-section-number">9.3.3</span> Learning without Identification</h3>
<p> A third payoff of this approach is that it allows us to engage in inference even when causal queries are not <em>identified</em>. When a query is identified, each true value for the query is associated with a unique distribution of data types. Thus, as we gather more and more data, our posterior on the query should converge on the true value. When a query is not identified, multiple true values of the query will be associated with the same distribution of data types. With a non-identified query, our posterior will never converge on a unique value regardless of how much data we collect since multiple answers will be equally consistent with the data. A key advantage of a causal model framework, however, is that we can <em>learn</em> about queries that are not identified but are still “partially identified,” even if we cannot remove all uncertainty over such queries.</p>
<p>We can illustrate the difference between identified and non-identified causal questions by comparing an ATE query to a probability of causation PC query for a simple <span class="math inline">\(X \rightarrow Y\)</span> model. When asking about the ATE, we are asking about the average effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>, or the difference between <span class="math inline">\(\lambda^Y_{01}\)</span> (the share of units with positive effects) and <span class="math inline">\(\lambda^Y_{10}\)</span> (share with negative effects). When asking about the PC, we are asking, for a case with given values of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, about the probability that <span class="math inline">\(X\)</span> caused <span class="math inline">\(Y\)</span> in that case. This PC query is defined by a different set of parameters. For, say, an <span class="math inline">\(X=1, Y=1\)</span> case and a <span class="math inline">\(X \rightarrow Y\)</span> model, the probability of causation is given by just <span class="math inline">\(\lambda^Y_{01}/(\lambda_{01}^Y + \lambda_{11}^Y)\)</span>.</p>
<p>Let us assume a “true” set of parameters, unknown to the researcher, such that <span class="math inline">\(\lambda^Y_{01} = 0.6,\)</span> and <span class="math inline">\(\lambda^Y_{10} = 0.1\)</span> while we set <span class="math inline">\(\lambda^Y_{00} = 0.2\)</span> and <span class="math inline">\(\lambda^Y_{11} = 0.1\)</span>. Thus, the true average causal effect is <span class="math inline">\(0.5\)</span>. We now use these parameters and the model to simulate a large amount of data (<span class="math inline">\(N=\)</span> 10,000). We then return to the model, set flat priors over nodal types, and update the model using the simulated data. We graph the posterior on our two queries, the ATE and the probability of positive causation in an <span class="math inline">\(X=1, Y=1\)</span> case, in <a href="#fig-HJ-F-9-6" class="quarto-xref">Figure&nbsp;<span>9.6</span></a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-HJ-F-9-6" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-HJ-F-9-6-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="09-mixing-methods_files/figure-html/fig-HJ-F-9-6-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-HJ-F-9-6-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.6: Posterior distributions. ATE is identified PC is not identified but has informative bounds
</figcaption></figure>
</div>
</div>
</div>
<p>The figure nicely illustrates the difference between an identified and non-identified query. While the ATE converges on the right answer, the probability of causation fails to converge even with a massive amount of data. We see instead a range of values for this query on which our updated model places roughly equal posterior probability.</p>
<p>Importantly, however, we see that we <em>do</em> learn about the probability of causation. Despite the lack of convergence, our posterior rules out a wide range of values. While our prior on the query was 0.5, we have correctly updated toward a range of values that includes (and happens to be fairly well centered over) the true value (<span class="math inline">\(\approx 0.86\)</span>).</p>
<p>A distinctive feature of updating a causal model is that it lets us learn about non-identified quantities in this manner. We might end up with “ridges” in our posterior distributions: ranges or combinations of parameter values that are equally likely given the data. But our posterior weight can nonetheless shift toward the right answer.</p>
<p>At the same time, for non-identified queries, we have to be cautious about the impact of our priors. As <span class="math inline">\(N\)</span> becomes large, the remaining curvature we see in our posteriors may simply be a function of those priors. One way to inspect for this is to simulate a very large dataset and see whether variance shrinks. A second approach is to do sensitivity analyses by updating the model on the same data with different sets of priors to see how this affects the shape of the posterior.</p>
<p>Finally, we note a nontrivial practical payoff. Whether quantities are identified or not, we calculate answers to queries in the same way: by defining a model, then updating it and querying it. We do not have to figure out the particular estimating equation that works to return a good estimate of an estimand. To illustrate the point, in a beautiful contribution, <span class="citation" data-cites="angrist1995identification">Angrist and Imbens (<a href="20-references.html#ref-angrist1995identification" role="doc-biblioref">1995</a>)</span> show that, under a small set of conditions, average treatment effects for compliers (or “CACE” for “complier average causal effects”) are identifiable, and then figure out what procedure one can use for estimating them (instrumental variables). Yet a researcher who believed that the conditions Angrist and Imbens stipulate held in their causal model, updated their model with a large amount of data, and queried for the complier average effect would get to the right answer with a low posterior variance. And they would get there even if they had never read <span class="citation" data-cites="angrist1995identification">Angrist and Imbens (<a href="20-references.html#ref-angrist1995identification" role="doc-biblioref">1995</a>)</span>, did not know beforehand that their quantity of interest was identified, and did not know what estimating equation they would need to estimate it consistently.</p>
</section></section><section id="extensions" class="level2" data-number="9.4"><h2 data-number="9.4" class="anchored" data-anchor-id="extensions">
<span class="header-section-number">9.4</span> Extensions</h2>
<p>In our presentation of the baseline approach so far, we have assumed that we are analyzing binary data on a set of cases with independent (potential) outcomes for the central purpose of estimating causal relationships. In this last section, we consider four extensions of this basic approach: a procedure for handling nonbinary data and applications of the framework to modeling and learning about measurement error and spillovers between units.</p>
<section id="beyond-binary-data" class="level3" data-number="9.4.1"><h3 data-number="9.4.1" class="anchored" data-anchor-id="beyond-binary-data">
<span class="header-section-number">9.4.1</span> Beyond Binary Data</h3>
<p> The approach we have described readily generalizes to nonbinary data. Moving beyond binary nodes allows for considerably greater flexibility in response functions. For instance, moving from binary to merely three-level ordinal <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> variables allows us to represent nonlinear and even non-monotonic relationships. It also allows us to pose more complex queries, such as, “What is the probability that <span class="math inline">\(Y\)</span> is linear in <span class="math inline">\(X\)</span>?,” “What is the probability that <span class="math inline">\(Y\)</span> is concave in <span class="math inline">\(X\)</span>?” or “What is the probability that <span class="math inline">\(Y\)</span> is monotonic in <span class="math inline">\(X\)</span>?”</p>
<p>To move to nonbinary nodes, we need to be able to expand the nodal-type space to accommodate the richer range of possible relations between nodes that can take on more than two possible values. Suppose, for instance, that we want to operate with variables with four ordinal categories. In an <span class="math inline">\(X \rightarrow Y\)</span> model, <span class="math inline">\(Y\)</span>’s nodal types have to accommodate four possible values that <span class="math inline">\(X\)</span> can take on, and four possible values that <span class="math inline">\(Y\)</span> can take on for any value of <span class="math inline">\(X\)</span>. This yields <span class="math inline">\(4^4 = 256\)</span> nodal types for <span class="math inline">\(Y\)</span> and 1024 causal types (compared to just eight in a binary setup).</p>
<p>The <code>CausalQueries</code> package, set up to work most naturally with binary nodes, can in principle, be used to represent nonbinary data as well.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<p>In the illustration below with two four-level variables, we generate data (<span class="math inline">\(N=100\)</span>) from a non-monotonic process with the following potential outcomes: <span class="math inline">\(Y(0)=0, Y(1)=1, Y(2)=3\)</span>, and <span class="math inline">\(Y(3)=2\)</span>. We then update and report on posteriors on potential outcomes.</p>
<p>Updating and querying are done in the usual way. In <a href="#tbl-HJ-T-9-5" class="quarto-xref">Table&nbsp;<span>9.7</span></a>, we show results for a simple set of queries in which we ask what <span class="math inline">\(Y\)</span>’s expected outcome is for each value of <span class="math inline">\(X\)</span>. We report the mean and standard deviation for the posterior on each query and as a benchmark, also show the “true” parameter value that we used to generate the data.</p>
<div class="cell" data-layout-align="center">
<div id="tbl-HJ-T-9-5" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-HJ-T-9-5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;9.7: Posteriors on potential outcomes for non binary model
</figcaption><div aria-describedby="tbl-HJ-T-9-5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<thead><tr class="header">
<th style="text-align: left;">Q</th>
<th style="text-align: left;">using</th>
<th style="text-align: right;">True value</th>
<th style="text-align: right;">mean</th>
<th style="text-align: right;">sd</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Y(0)</td>
<td style="text-align: left;">posteriors</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0.39</td>
<td style="text-align: right;">0.09</td>
</tr>
<tr class="even">
<td style="text-align: left;">Y(1)</td>
<td style="text-align: left;">posteriors</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.98</td>
<td style="text-align: right;">0.07</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Y(2)</td>
<td style="text-align: left;">posteriors</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">2.61</td>
<td style="text-align: right;">0.09</td>
</tr>
<tr class="even">
<td style="text-align: left;">Y(3)</td>
<td style="text-align: left;">posteriors</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">2.02</td>
<td style="text-align: right;">0.07</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>We see that the model performs well. As in the binary setup, the posterior reflects both the data and the priors. And, as usual, we have access to a full posterior distribution over all nodal types and can thus ask arbitrary queries of the updated model.</p>
<p>The greatest challenge posed by the move to nonbinary data is computational. If <span class="math inline">\(Y\)</span> takes on <span class="math inline">\(m\)</span> possible values and has <span class="math inline">\(k\)</span> parents, each taking on <span class="math inline">\(r\)</span> possible values, we then have <span class="math inline">\(m^{(r^k)}\)</span> nodal types for <span class="math inline">\(Y\)</span>. Thus, the cost of more granular measurement is complexity—an explosion of the parameter space—as the nodal type space expands rapidly with the granularity of measurement and the number of explanatory variables. With three-level ordinal variables pointing into the same outcome, for instance, we have <span class="math inline">\(3^{27} = 7.6\)</span> <em>trillion</em> nodal types.</p>
<p>We expect that, as measurement becomes more granular, researchers will want to manage the complexity by placing structure onto the possible patterns of causal effects. Structure, imposed through model restrictions, can quite rapidly tame the complexity. For some substantive problems, one form of structure we might be willing to impose is monotonicity. In a <span class="math inline">\(X \rightarrow Y\)</span> model with three-level variables, excluding non-monotonic effects brings down the number of nodal types from 27 to 17. Alternatively, we may have a strong reason to rule out effects in one direction: Disallowing negative effects, for instance, brings us down to 10 nodal types. If we are willing to assume linearity, the number of nodal types falls further to 5.</p>
</section><section id="measurement-error" class="level3" data-number="9.4.2"><h3 data-number="9.4.2" class="anchored" data-anchor-id="measurement-error">
<span class="header-section-number">9.4.2</span> Measurement Error</h3>
<p> One potential application of the approach we have described in this chapter to integrating differing forms of data is to address the problem of measurement error. The conceptual move to address measurement error in a causal model setup is quite simple: We incorporate the error-generating process into our model.</p>
<p>Consider, for instance, a model in which we build in a process generating measurement error on the dependent variable.</p>
<p><span class="math display">\[X \rightarrow Y  \rightarrow Y_\text{measured} \leftarrow \text{source of measurement error}\]</span></p>
<p>Here, <span class="math inline">\(X\)</span> has an effect on the true value of our outcome of interest, <span class="math inline">\(Y\)</span>. The true value of <span class="math inline">\(Y\)</span>, in turn, has an effect on the value of <span class="math inline">\(Y\)</span> that we measure, but so too does a potential problem with our coding process. Thus, the measured value of <span class="math inline">\(Y\)</span> is a function of both the true value and error.</p>
<p>To motivate the setup, imagine that we are interested in the effect of a rule restricting long-term care staff to working at a single site (<span class="math inline">\(X\)</span>) on outbreaks of the novel coronavirus in long-term care facilities (<span class="math inline">\(Y\)</span>), defined as infections among two or more staff or residents. We do not directly observe infections, however; rather, we observe positive results of PCR tests. We also know that testing is neither comprehensive nor uniform. For some units, regular random testing is carried out on staff and residents, while in others, only symptomatic individuals are tested. It is the latter arrangement that potentially introduces measurement error.</p>
<p>If we approach the problem naively, ignoring measurement error and treating <span class="math inline">\(Y_\text{measured}\)</span> as though it were identical to <span class="math inline">\(Y\)</span>, a differences-in-means approach might produce attenuation bias—insofar as we are averaging between the true relationship and 0.</p>
<p>We can do better with a causal model, however. Without any additional data, we can update on both <span class="math inline">\(\lambda^Y\)</span> and <span class="math inline">\(\lambda^{Y_\text{measured}}\)</span>, and our posterior uncertainty would reflect uncertainty in measurement. We could go further if, for instance, we could reasonably exclude negative effects of <span class="math inline">\(Y\)</span> on <span class="math inline">\(Y_\text{measured}\)</span>. Then, if we observe (say) a negative correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y_\text{measured}\)</span>, we can update on the substantive effect of interest—<span class="math inline">\(\lambda^Y\)</span>—in the direction of a larger share of negative effects: It is only <em>via</em> negative effects of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> that a negative correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y_\text{measured}\)</span> could emerge. At the same time, we learn about the measure itself as we update on <span class="math inline">\(\lambda^{Y_\text{measured}}\)</span>: The negative observed correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y_\text{measured}\)</span> is an indicator of the degree to which <span class="math inline">\(Y_\text{measured}\)</span> is picking up true <span class="math inline">\(Y\)</span>.</p>
<p>We can do better still if we can collect more detailed information on at least some units. One data strategy would be to invest in observing <span class="math inline">\(Y\)</span>, the true outbreak status of each unit, for a subset of units for which we already have data on <span class="math inline">\(X\)</span> and <span class="math inline">\(Y_\text{measured}\)</span> — perhaps by implementing a random-testing protocol at a subset of facilities. Getting better measures of <span class="math inline">\(Y\)</span> for some cases will allow us to update more directly on <span class="math inline">\(\lambda^Y\)</span>, and so the true effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>, for those cases. But just as importantly, observing true <span class="math inline">\(Y\)</span> will allow us to update on measurement <em>quality</em>, <span class="math inline">\(\lambda^{Y_\text{measured}}\)</span>, and thus help us make better use of the data we have for those cases where we only observe <span class="math inline">\(Y_\text{measured}\)</span>. This strategy, of course, parallels a commonly prescribed use of mixed methods, in which qualitative research takes place in a small set of units to generate more credible measures for large-<span class="math inline">\(n\)</span> analysis (see, e.g., <span class="citation" data-cites="seawrightbook">Seawright (<a href="20-references.html#ref-seawrightbook" role="doc-biblioref">2016</a>)</span>).</p>
<p>To illustrate, we posit a true average effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> of 0.6. We also posit an average “effect” of <span class="math inline">\(Y\)</span> on measured <span class="math inline">\(Y\)</span> of just 0.7, allowing for measurement error.</p>
<p>In this setup, with a large amount of data, we would arrive at a differences-in-means estimate of the effect of <span class="math inline">\(X\)</span> on <em>measured</em> <span class="math inline">\(Y\)</span> of about 0.42. Importantly, this would be the effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y_{\text{measured}}\)</span> — not the effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> — but if we were not thinking about the possibility of measurement error, we might conflate the two, arriving at an estimate far from the true value.</p>
<p>We can improve on this “naive” estimate in a number of ways using a causal model, as shown in <a href="#tbl-merror" class="quarto-xref">Table&nbsp;<span>9.8</span></a>. First, we can do much better simply by undertaking the estimation within a causal model framework, even if we simply make use of the exact same data. We write down the following simple model <span class="math inline">\(X \rightarrow Y \rightarrow Y_\text{measured}\)</span>, and we build in a monotonicity restriction that disallows negative effects of <span class="math inline">\(Y\)</span> on <span class="math inline">\(Y_{\text{measured}}\)</span>. As we can see from the first row in <a href="#tbl-merror" class="quarto-xref">Table&nbsp;<span>9.8</span></a>, our mean estimate of the <span class="math inline">\(ATE\)</span> moves much closer to the true value of 0.6, and it has an appropriately larger posterior standard deviation.</p>
<p>Second, we can add data by gathering measures of “true” <span class="math inline">\(Y\)</span> for 20% of our sample. As we can see from the second row in the table, this investment in additional data does not change our posterior mean much but yields a dramatic increase in precision. In fact, as we can see by comparison to the third row, partial data on “true” <span class="math inline">\(Y\)</span> yields an estimate that is almost the same and almost as precise as the one we would arrive at with data on “true” <span class="math inline">\(Y\)</span> for <em>all</em> cases.</p>
<div class="cell" data-layout-align="center">
<div id="tbl-merror" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-merror-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;9.8: Inferences on effects on true Y given measurement error (true ATE = .6)
</figcaption><div aria-describedby="tbl-merror-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<thead><tr class="header">
<th style="text-align: left;">Data</th>
<th style="text-align: left;">using</th>
<th style="text-align: right;">mean</th>
<th style="text-align: right;">sd</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Data on Y measured only</td>
<td style="text-align: left;">posteriors</td>
<td style="text-align: right;">0.64</td>
<td style="text-align: right;">0.09</td>
</tr>
<tr class="even">
<td style="text-align: left;">Data on true Y for 20% of units</td>
<td style="text-align: left;">posteriors</td>
<td style="text-align: right;">0.63</td>
<td style="text-align: right;">0.03</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Data on true Y</td>
<td style="text-align: left;">posteriors</td>
<td style="text-align: right;">0.61</td>
<td style="text-align: right;">0.02</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>An alternative strategy might involve gathering multiple measures of <span class="math inline">\(Y\)</span>, each with its own independent source of error. Consider the model, <span class="math inline">\(X \rightarrow Y \rightarrow Y_\text{measured[1]}; Y \rightarrow Y_\text{measured[2]}\)</span>. Assume again a true <span class="math inline">\(ATE\)</span> of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> of 0.6, that <span class="math inline">\(Y\)</span> has an average effect of 0.7 on both <span class="math inline">\(Y_\text{measured[1]}\)</span> and <span class="math inline">\(Y_\text{measured[2]}\)</span>, and no negative effects of true <span class="math inline">\(Y\)</span> on the measures.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> In this setup, updating the true <span class="math inline">\(Y\)</span> can be thought of as a Bayesian version of “triangulation,” or factor analysis. The results in <a href="#tbl-measurementerror2" class="quarto-xref">Table&nbsp;<span>9.9</span></a> are based on the same data as in the previous example but are now augmented with the second noisy measure for <span class="math inline">\(Y\)</span>.</p>
<div class="cell" data-layout-align="center">
<div id="tbl-measurementerror2" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-measurementerror2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;9.9: Inferences on effects on true Y given two noisy measures (true ATE = .6)
</figcaption><div aria-describedby="tbl-measurementerror2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<thead><tr class="header">
<th style="text-align: left;">Data</th>
<th style="text-align: left;">using</th>
<th style="text-align: right;">mean</th>
<th style="text-align: right;">sd</th>
</tr></thead>
<tbody><tr class="odd">
<td style="text-align: left;">Two noisy measures</td>
<td style="text-align: left;">posteriors</td>
<td style="text-align: right;">0.61</td>
<td style="text-align: right;">0.02</td>
</tr></tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>As we can see, two noisy measures perform in this example about as well as access to full data on the true <span class="math inline">\(Y\)</span> (as in <a href="#tbl-merror" class="quarto-xref">Table&nbsp;<span>9.8</span></a>).</p>
<p>The main point here is that measurement error matters for inference and can be taken directly into account within a causal model framework. Confusing measured variables for variables of interest will obviously lead to false conclusions. But if measurement concerns loom large, we can respond by making them part of our model and learning about them. We have illustrated this point for simple setups, but more complex structures could be just as well envisioned, such as those where the error is related to <span class="math inline">\(X\)</span> or, more perniciously, to the effects of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>.</p>
</section><section id="spillovers" class="level3" data-number="9.4.3"><h3 data-number="9.4.3" class="anchored" data-anchor-id="spillovers">
<span class="header-section-number">9.4.3</span> Spillovers</h3>
<p> A common threat to causal inference is the possibility of spillovers: a given unit’s outcome being affected by the treatment status of another (e.g., possibly neighboring) unit. We can readily set up a causal model to allow for the estimation of various quantities related to spillovers.</p>
<p>Consider, for instance, the causal model represented in <a href="#fig-HJ-F-9-7" class="quarto-xref">Figure&nbsp;<span>9.7</span></a>. We consider here groupings of pairs of units across which spillovers might occur. We might imagine, for instance, geographically proximate villages separated from other groups such that spillovers might occur between neighboring villages but can be ruled out across more distal villages. Here, <span class="math inline">\(X_i\)</span> and <span class="math inline">\(Y_i\)</span> represent village <span class="math inline">\(i\)</span>’s treatment status and outcome, respectively. The pattern of directed edges indicates that each village’s outcome might be affected both by its own and by its neighbors’ treatment status.</p>
<p>We now simulate data that allow for spillovers. Specifically, while independently assigning <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> to treatment <span class="math inline">\(50 \%\)</span> of the time, we (a) set <span class="math inline">\(Y_1\)</span> equal to <span class="math inline">\(X_1\)</span>, meaning that Unit 1 is affected only by its own treatment status and (b) set <span class="math inline">\(Y_2\)</span> equal to <span class="math inline">\(X_1 \times X_2\)</span>, meaning that Unit 2 is equally affected by its own treatment status and that of its neighbor, such that <span class="math inline">\(Y_2 = 1\)</span> only if both Unit 2 and its neighbor are assigned to treatment.</p>
<p>We simulate 100 observations from this data-generating process and then update a model (with flat priors over all nodal types).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-HJ-F-9-7" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-HJ-F-9-7-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="09-mixing-methods_files/figure-html/fig-HJ-F-9-7-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-HJ-F-9-7-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.7: Model with spillovers: the treatment status of two units each affect the outcomes for both.
</figcaption></figure>
</div>
</div>
</div>
<p>Now we can extract a number of spillover-relevant causal quantities from the updated model. First, we ask: What is the average effect of exposing a unit <em>directly</em> to treatment (“only_self_treated”) when the neighboring unit is untreated? Under the data-generating process that we have posited, we know that this effect will be <span class="math inline">\(1\)</span> for Unit 1 (which always has a positive treatment effect) and <span class="math inline">\(0\)</span> for Unit 2 (which sees a positive effect of <span class="math inline">\(X_2\)</span> only when <span class="math inline">\(X_1 = 1\)</span>), yielding an average across the two units of <span class="math inline">\(0.5\)</span>. In Table 9.10, see that we update, given our 100 observations, from a prior of 0 to a posterior mean of 0.371, approaching the right answer.</p>
<p>A second question we can ask is about the spillover itself: What is the average treatment effect for a unit of its neighbor being assigned to treatment when the unit itself is not assigned to treatment (“only_other_treated”)? We know that the correct answer is <span class="math inline">\(0\)</span> since Unit 1 responds only to its own treatment status, and Unit 2 requires that both units be assigned to treatment to see an effect. Our posterior estimate of this effect is right on target, at 0.</p>
<p>We can then ask about the average effect of <em>any</em> one unit being treated, as compared to no units being treated (“one_treated”). This is a more complex quantity. To estimate it, we have to consider what happens to the outcome in Unit 1 when only <span class="math inline">\(X_1\)</span> shifts from control to treatment, with <span class="math inline">\(X_2\)</span> at control (the true effect is <span class="math inline">\(1\)</span>); what happens to Unit 1 when only <span class="math inline">\(X_2\)</span> shifts from control to treatment, with <span class="math inline">\(X_1\)</span> at control (the true effect is <span class="math inline">\(0\)</span>); and the same two effects for Unit 2 (both true effects are <span class="math inline">\(0\)</span>). We then average across both the treatment conditions and units. We arrive at a posterior mean of <span class="math inline">\(0.186\)</span>, not far from the true value of <span class="math inline">\(0.25\)</span>.</p>
<p>Finally, we can ask about the average effect of both treatments going from control to treatment (“both_treated”). The true value of this effect is <span class="math inline">\(1\)</span> for both units, and the posterior has shifted quite far in the direction of this value.</p>
<p>Obviously, more complex setups are possible. The main idea however is that spillovers, often seen as a threat to inference, can just as well been seen as an opportunity to learn about an array of causal processes.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Spillover queries</caption>
<thead><tr class="header">
<th style="text-align: left;">label</th>
<th style="text-align: left;">using</th>
<th style="text-align: right;">mean</th>
<th style="text-align: right;">sd</th>
<th style="text-align: right;">cred.low</th>
<th style="text-align: right;">cred.high</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">only_self_treated</td>
<td style="text-align: left;">posteriors</td>
<td style="text-align: right;">0.38</td>
<td style="text-align: right;">0.05</td>
<td style="text-align: right;">0.28</td>
<td style="text-align: right;">0.47</td>
</tr>
<tr class="even">
<td style="text-align: left;">only_other_treated</td>
<td style="text-align: left;">posteriors</td>
<td style="text-align: right;">0.02</td>
<td style="text-align: right;">0.04</td>
<td style="text-align: right;">-0.06</td>
<td style="text-align: right;">0.11</td>
</tr>
<tr class="odd">
<td style="text-align: left;">one_treated</td>
<td style="text-align: left;">posteriors</td>
<td style="text-align: right;">0.20</td>
<td style="text-align: right;">0.04</td>
<td style="text-align: right;">0.13</td>
<td style="text-align: right;">0.27</td>
</tr>
<tr class="even">
<td style="text-align: left;">both_treated</td>
<td style="text-align: left;">posteriors</td>
<td style="text-align: right;">0.76</td>
<td style="text-align: right;">0.05</td>
<td style="text-align: right;">0.65</td>
<td style="text-align: right;">0.84</td>
</tr>
</tbody>
</table>
</div>
</div>
</section></section><section id="chapter-appendix-mixing-methods-with-causalqueries" class="level2" data-number="9.5"><h2 data-number="9.5" class="anchored" data-anchor-id="chapter-appendix-mixing-methods-with-causalqueries">
<span class="header-section-number">9.5</span> Chapter Appendix: Mixing Methods with <code>CausalQueries</code>
</h2>
<p></p>
<section id="an-illustration-in-code" class="level3" data-number="9.5.1"><h3 data-number="9.5.1" class="anchored" data-anchor-id="an-illustration-in-code">
<span class="header-section-number">9.5.1</span> An Illustration in Code</h3>
<p>We now demonstrate how to do model updating in <code>CausalQueries</code> when you have <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> data for many cases but “causal process observations” for only a smaller number of cases.</p>
<p>Imagine a simple model in which <span class="math inline">\(X\)</span> has a possible direct or indirect effect via <span class="math inline">\(M\)</span>. We can define the model thus:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu">make_model</span><span class="op">(</span><span class="st">"X -&gt; M -&gt; Y &lt;- X"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We do not provide any structure to priors or impose any monotonicity constraints. But we do imagine that we can access some data and update using these data. For this illustration, the data are consistent with effects running through <span class="math inline">\(M\)</span>; moreover, <span class="math inline">\(X,Y\)</span> data are available for all units, but <span class="math inline">\(M\)</span> is available for some units only. We now input data and update the model.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>  X <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">0</span>,<span class="fl">0</span>,<span class="fl">0</span>,<span class="fl">1</span>,<span class="fl">1</span>,<span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span>,</span>
<span>  M <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">NA</span>,<span class="fl">0</span>,<span class="fl">0</span>,<span class="fl">1</span>,<span class="fl">0</span>,<span class="fl">1</span>,<span class="fl">1</span>,<span class="cn">NA</span><span class="op">)</span>,</span>
<span>  Y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">0</span>,<span class="fl">0</span>,<span class="fl">1</span>,<span class="fl">0</span>,<span class="fl">1</span>,<span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">uncount</span><span class="op">(</span><span class="fl">10</span><span class="op">)</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu">update_model</span><span class="op">(</span><span class="va">model</span>, <span class="va">data</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now query the updated model to figure out how our inferences for a case depend on <span class="math inline">\(M\)</span>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">query_model</span><span class="op">(</span><span class="va">model</span>,</span>
<span>            query <span class="op">=</span> <span class="st">"Y[X=1]&gt; Y[X=0]"</span>,</span>
<span>            given <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"X==1 &amp; Y==1"</span>, </span>
<span>                      <span class="st">"X==1 &amp; Y==1 &amp; M==1"</span>,</span>
<span>                      <span class="st">"X==1 &amp; Y==1 &amp; M==0"</span><span class="op">)</span>,</span>
<span>            using <span class="op">=</span> <span class="st">"posteriors"</span>,</span>
<span>            case_level <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span> </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Querying an updated model</caption>
<thead><tr class="header">
<th style="text-align: left;">given</th>
<th style="text-align: right;">estimate</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">X==1 &amp; Y==1</td>
<td style="text-align: right;">0.765</td>
</tr>
<tr class="even">
<td style="text-align: left;">X==1 &amp; Y==1 &amp; M==0</td>
<td style="text-align: right;">0.581</td>
</tr>
<tr class="odd">
<td style="text-align: left;">X==1 &amp; Y==1 &amp; M==1</td>
<td style="text-align: right;">0.775</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Results are shown in Table 9.11. We see that <span class="math inline">\(M\)</span> is informative (particularly when <span class="math inline">\(M=0\)</span>) about causal effects in a new case, given our observation of processes in previous cases. The key thing here is that the informativeness of <span class="math inline">\(M\)</span> for the new case is justified by the updating of the original model<sub>–</sub>a model that itself contained no assumptions about whether or how effects passed through <span class="math inline">\(M\)</span>.</p>
</section><section id="replication-of-chickering1996clinician-lipid-analysis." class="level3" data-number="9.5.2"><h3 data-number="9.5.2" class="anchored" data-anchor-id="replication-of-chickering1996clinician-lipid-analysis.">
<span class="header-section-number">9.5.2</span> Replication of <span class="citation" data-cites="chickering1996clinician">Chickering and Pearl (<a href="20-references.html#ref-chickering1996clinician" role="doc-biblioref">1996</a>)</span> Lipid Analysis.</h3>
<p> </p>
<p><span class="citation" data-cites="chickering1996clinician">Chickering and Pearl (<a href="20-references.html#ref-chickering1996clinician" role="doc-biblioref">1996</a>)</span> assess the problem of drawing inferences in the presence of imperfect compliance. They use data that look like those in Table 9.12. </p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Lipid study data from Chickering and Pearl (1996). Note data are reported in compact form with counts of events, assuming a data strategy in which data are sought on all nodes (ZXY)</caption>
<thead><tr class="header">
<th style="text-align: left;">event</th>
<th style="text-align: left;">strategy</th>
<th style="text-align: right;">count</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Z0X0Y0</td>
<td style="text-align: left;">ZXY</td>
<td style="text-align: right;">158</td>
</tr>
<tr class="even">
<td style="text-align: left;">Z1X0Y0</td>
<td style="text-align: left;">ZXY</td>
<td style="text-align: right;">52</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Z0X1Y0</td>
<td style="text-align: left;">ZXY</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">Z1X1Y0</td>
<td style="text-align: left;">ZXY</td>
<td style="text-align: right;">23</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Z0X0Y1</td>
<td style="text-align: left;">ZXY</td>
<td style="text-align: right;">14</td>
</tr>
<tr class="even">
<td style="text-align: left;">Z1X0Y1</td>
<td style="text-align: left;">ZXY</td>
<td style="text-align: right;">12</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Z0X1Y1</td>
<td style="text-align: left;">ZXY</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">Z1X1Y1</td>
<td style="text-align: left;">ZXY</td>
<td style="text-align: right;">78</td>
</tr>
</tbody>
</table>
</div>
</div>
<p><span class="citation" data-cites="chickering1996clinician">Chickering and Pearl (<a href="20-references.html#ref-chickering1996clinician" role="doc-biblioref">1996</a>)</span> use a Gibbs sampler to update over 16 response types (and so 15 degrees of freedom). The parameterization in <code>CausalQueries</code> has four nodal types for <span class="math inline">\(X\)</span> and four parameters capturing the conditional distribution of the four nodal types for <span class="math inline">\(Y\)</span> given each nodal type for <span class="math inline">\(X\)</span>, giving <span class="math inline">\(3 + 4\times3 = 15\)</span> degrees of freedom. </p>
<p>In <code>CausalQueries</code> the complete code for model specification, updating, and querying is quite compact:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">results</span> <span class="op">&lt;-</span></span>
<span>  </span>
<span>  <span class="fu">make_model</span><span class="op">(</span><span class="st">"Z -&gt; X -&gt; Y; X &lt;-&gt; Y"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">update_model</span><span class="op">(</span><span class="va">data</span>, data_type <span class="op">=</span> <span class="st">"compact"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">query_model</span><span class="op">(</span>query <span class="op">=</span> <span class="st">"Y[X=1] - Y[X=0]"</span>, using <span class="op">=</span> <span class="st">"posteriors"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><a href="#tbl-HJ-T-9-12" class="quarto-xref">Table&nbsp;<span>9.10</span></a> reports the results while <a href="#fig-HJ-F-9-9" class="quarto-xref">Figure&nbsp;<span>9.8</span></a> shows the full posterior distribution for this query.</p>
<div id="tbl-HJ-T-9-12" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl quarto-uncaptioned" id="tbl-HJ-T-9-12-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;9.10
</figcaption><div aria-describedby="tbl-HJ-T-9-12-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>

</div>
</div>
</figure>
</div>
<p>The results agree with the findings in <span class="citation" data-cites="chickering1996clinician">Chickering and Pearl (<a href="20-references.html#ref-chickering1996clinician" role="doc-biblioref">1996</a>)</span>. We also show the posterior distribution for the average effects <em>among the compliers</em>—those for whom <span class="math inline">\(Z\)</span> has a positive effect on <span class="math inline">\(X\)</span>—which is tighter thanks to identification for this query. </p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-HJ-F-9-9" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-HJ-F-9-9-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="09-mixing-methods_files/figure-html/fig-HJ-F-9-9-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-HJ-F-9-9-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.8: Posterior distributions on average treatment effects and treatment effects for compliers. Left panel replicates Chickering and Pearl (1997).
</figcaption></figure>
</div>
</div>
</div>
<p><span class="citation" data-cites="chickering1996clinician">Chickering and Pearl (<a href="20-references.html#ref-chickering1996clinician" role="doc-biblioref">1996</a>)</span> also assess probabilities of counterfactual events for single cases. For instance, would there be a positive effect for someone with <span class="math inline">\(X=0, Y=0\)</span>. Our answers to this query, shown in Table 9.14, also agree with <span class="citation" data-cites="chickering1996clinician">Chickering and Pearl (<a href="20-references.html#ref-chickering1996clinician" role="doc-biblioref">1996</a>)</span>, see <a href="#tbl-HJ-T-9-13" class="quarto-xref">Table&nbsp;<span>9.11</span></a>. Note that when we calculate inferences for a single “new” case (“Case level”) our conclusion is a single number, a probability, and it does not have a confidence interval around it. </p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">make_model</span><span class="op">(</span><span class="st">"Z -&gt; X -&gt; Y; X &lt;-&gt; Y"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">update_model</span><span class="op">(</span><span class="va">data</span>, data_type <span class="op">=</span> <span class="st">"compact"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">query_model</span><span class="op">(</span></span>
<span>    query <span class="op">=</span> <span class="st">"Y[X=1] - Y[X=0]"</span>,</span>
<span>    given <span class="op">=</span> <span class="st">"X==0 &amp; Y==0"</span>,</span>
<span>    case_level <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">FALSE</span>, <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>    using <span class="op">=</span> <span class="st">"posteriors"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div id="tbl-HJ-T-9-13" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-HJ-T-9-13-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;9.11: Case level counterfactual inference following model updating (replication of Chickering and Pearl 1997).
</figcaption><div aria-describedby="tbl-HJ-T-9-13-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<thead><tr class="header">
<th style="text-align: left;">Case level</th>
<th style="text-align: right;">mean</th>
<th style="text-align: right;">sd</th>
<th style="text-align: right;">cred.low</th>
<th style="text-align: right;">cred.high</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">FALSE</td>
<td style="text-align: right;">0.634</td>
<td style="text-align: right;">0.151</td>
<td style="text-align: right;">0.371</td>
<td style="text-align: right;">0.894</td>
</tr>
<tr class="even">
<td style="text-align: left;">TRUE</td>
<td style="text-align: right;">0.634</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">0.634</td>
<td style="text-align: right;">0.634</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
</section><section id="sec-pvpopdag" class="level3" data-number="9.5.3"><h3 data-number="9.5.3" class="anchored" data-anchor-id="sec-pvpopdag">
<span class="header-section-number">9.5.3</span> Probative value arising from correlations in the posterior distribution over parameters</h3>
<p></p>
<p>In <a href="07-process-tracing-with-models.html" class="quarto-xref"><span>Chapter 7</span></a>, we showed how you can use rules of <span class="math inline">\(d\)</span>-separation to assess whether data on a node has probative value for a query given a case-level DAG.. In that discussion, we were conditioning on <span class="math inline">\(\lambda\)</span> (or assuming that nonindependencies arising from the joint distribution of <span class="math inline">\(\lambda\)</span> were already captured by the DAG). How do things change when we update over the distribution of <span class="math inline">\(\lambda\)</span>?</p>
<p>In that case, it is possible that when we update over <span class="math inline">\(\lambda\)</span> we have dependencies in our beliefs that call for a reassessment of the case-level DAG and so a reassessment of when case-level data have probative value for a query.</p>
<p>Thus, we may have a case-level DAG where two nodes, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are <span class="math inline">\(d\)</span>-separated given <span class="math inline">\(C\)</span> conditional on <span class="math inline">\(\lambda\)</span>. In other words, we are convinced that in the data generating process, whatever it is, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are <span class="math inline">\(d\)</span>-separated given <span class="math inline">\(C\)</span> and so <span class="math inline">\(A\)</span> has no probative value for learning about <span class="math inline">\(B\)</span> given <span class="math inline">\(C\)</span>. We might even specify prior beliefs over <span class="math inline">\(\lambda\)</span> such that beliefs over <span class="math inline">\(\lambda^A\)</span> and <span class="math inline">\(\lambda^B\)</span> are independent and so <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are also <span class="math inline">\(d\)</span>-separated for each case in the population DAG (<a href="#fig-HJ-F-9-2" class="quarto-xref">Figure&nbsp;<span>9.2</span></a>. However, after updating, beliefs over <span class="math inline">\(\lambda^A\)</span> and <span class="math inline">\(\lambda^B\)</span> may no longer be independent and, in consequence, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> may no longer be <span class="math inline">\(d\)</span>-separated given <span class="math inline">\(C\)</span>.</p>
<p>We illustrate by imagining a chain model of the form <span class="math inline">\(X \rightarrow M \rightarrow Y\)</span>. Note that as written <span class="math inline">\(X\)</span> is <span class="math inline">\(d\)</span>-separated from <span class="math inline">\(Y\)</span> given <span class="math inline">\(M\)</span>. Say, however, that we have the following (<a href="#tbl-HJ-T-9-14" class="quarto-xref">Table&nbsp;<span>9.12</span></a>) joint distribution of beliefs over model parameters (where, as before, subscript <span class="math inline">\(01\)</span> indicates a positive effect and <span class="math inline">\(10\)</span> a negative effect):</p>
<div class="cell" data-layout-align="center">
<div id="tbl-HJ-T-9-14" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-HJ-T-9-14-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;9.12: Probability distribution over two parameter vectors
</figcaption><div aria-describedby="tbl-HJ-T-9-14-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<thead><tr class="header">
<th style="text-align: center;"><span class="math inline">\(\lambda\)</span></th>
<th style="text-align: center;"><span class="math inline">\(p(\lambda)\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\lambda^X_0\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\lambda^X_1\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\lambda^M_{01}\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\lambda^M_{10}\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\lambda^Y_{01}\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\lambda^Y_{10}\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(\lambda_1\)</span></td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.99</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(\lambda_2\)</span></td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.01</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>Beliefs like this might arise if you observe a lot of strongly correlated data on <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> but never get to observe <span class="math inline">\(M\)</span>.</p>
<p>Say we now ask about <span class="math inline">\(\hat{\pi}(X=0|M=1, Y=1)\)</span>. This is given by <span class="math inline">\(\frac{\lambda^X_0\lambda^M_{10}\lambda^Y_{01}}{\lambda^X_0\lambda^M_{10}\lambda^Y_{01} + \lambda^X_1\lambda^M_{01}\lambda^Y_{01}} = \lambda^M_{01}\)</span> which has expected value <span class="math inline">\(0.5\times0.01 + 0.5\times0.99 = 0.5\)</span>. Similarly <span class="math inline">\(\hat{\pi}(X=1|M=1, Y=0) = 0.5\)</span>. The reason is that in the model, <em>conditional on <span class="math inline">\(\lambda\)</span></em>, <span class="math inline">\(Y\)</span> is <span class="math inline">\(d\)</span>-separated from <span class="math inline">\(X\)</span> by <span class="math inline">\(M\)</span>.</p>
<p>However, when we now ask about <span class="math inline">\(\hat{\phi}(X=0|M=1, Y=1)\)</span> we are not conditioning on <span class="math inline">\(\lambda\)</span>. We have</p>
<p><span class="math inline">\(\hat{\phi}(X=1|M=1, Y=0) =\)</span> 0.02 and <span class="math inline">\(\hat{\phi}(X=1|M=1, Y=1)=\)</span> 0.98. Thus, we do not have conditional independence. Referring back to <a href="#fig-HJ-F-9-2" class="quarto-xref">Figure&nbsp;<span>9.2</span></a>, if we were to include double-headed arrows between the <span class="math inline">\(\lambda^M\)</span> and <span class="math inline">\(\lambda^Y\)</span> terms and then focus on the DAG for unit 1, we would then have to include double-headed arrows between <span class="math inline">\(M\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>For this reason, when drawing DAGs we need to be careful to specify either that the DAG represents the independence of <span class="math inline">\(\theta\)</span> terms <em>given</em> <span class="math inline">\(\lambda\)</span> or make sure that the DAG is faithful to violations of independence that arise from correlated beliefs over <span class="math inline">\(\lambda\)</span>. <a href="#fig-HJ-F-9-10" class="quarto-xref">Figure&nbsp;<span>9.9</span></a> illustrates. If we mean only the former, then we cannot use the rules of <span class="math inline">\(d\)</span>-separation to determine whether a clue has probative value for our beliefs on causal quantities.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-HJ-F-9-10" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-HJ-F-9-10-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="09-mixing-methods_files/figure-html/fig-HJ-F-9-10-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-HJ-F-9-10-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.9: <span class="math inline">\(d\)</span>-connectedness via correlations in beliefs over <span class="math inline">\(\lambda\)</span>
</figcaption></figure>
</div>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-angrist1995identification" class="csl-entry" role="listitem">
Angrist, Joshua D, and Guido W Imbens. 1995. <span>“Identification and Estimation of Local Average Treatment Effects.”</span> National Bureau of Economic Research.
</div>
<div id="ref-bareinboim2016causal" class="csl-entry" role="listitem">
Bareinboim, Elias, and Judea Pearl. 2016. <span>“Causal Inference and the Data-Fusion Problem.”</span> <em>Proceedings of the National Academy of Sciences</em> 113 (27): 7345–52.
</div>
<div id="ref-chickering1996clinician" class="csl-entry" role="listitem">
Chickering, David M, and Judea Pearl. 1996. <span>“A Clinician’s Tool for Analyzing Non-Compliance.”</span> In <em>Proceedings of the National Conference on Artificial Intelligence</em>, 1269–76.
</div>
<div id="ref-king1994designing" class="csl-entry" role="listitem">
King, Gary, Robert Keohane, and Sidney Verba. 1994. <em>Designing Social Inquiry: Scientific Inference in Qualitative Research</em>. Princeton University Press. <a href="http://books.google.de/books?id=A7VFF-JR3b8C">http://books.google.de/books?id=A7VFF-JR3b8C</a>.
</div>
<div id="ref-knox2020administrative" class="csl-entry" role="listitem">
Knox, Dean, Will Lowe, and Jonathan Mummolo. 2020. <span>“Administrative Records Mask Racially Biased Policing.”</span> <em>American Political Science Review</em> 114 (3): 619–37.
</div>
<div id="ref-seawrightbook" class="csl-entry" role="listitem">
Seawright, Jason. 2016. <em>Multi-Method Social Science: Combining Qualitative and Quantitative Tools</em>. New York: Cambridge University Press.
</div>
</div>
</section></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr>
<ol>
<li id="fn1"><p>A final subtlety that we discuss later is that, with this larger structure, the DAG for a single case (e.g.&nbsp;<span class="math inline">\(X_1 \rightarrow M_1 \rightarrow Y_1 \leftarrow M_1\)</span>) can be extracted <em>as is</em> from this larger DAG provided that we condition on the <span class="math inline">\(\lambda\)</span>s (or the <span class="math inline">\(\theta\)</span>s ) <em>or</em> the <span class="math inline">\(\lambda\)</span>s are independent of each other, as here. If the <span class="math inline">\(\lambda\)</span>s are not independent of each other then the DAG no longer captures all relations of conditional independence.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>As discussed in <a href="11-fusion.html#sec-multilevel" class="quarto-xref"><span>Section 11.4</span></a>, if we want to model heterogeneity across different populations we might use the Dirichlet distribution to capture the variation in <span class="math inline">\(\lambda\)</span> across populations (rather than our uncertainty over <span class="math inline">\(\lambda\)</span>). The <span class="math inline">\(\alpha\)</span> terms then become parameters that we want to learn about, and we need to provide a prior distribution for these, captured, perhaps, by an inverse Gamma distribution.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>The calculation is simplified by the fact that the information on <span class="math inline">\(M\)</span> is uninformative in this chain model. For the full calculation the denominator—the probability that <span class="math inline">\(M=1 \&amp; Y=1\)</span>—is <span class="math inline">\((\lambda^X_0\lambda^M_{10} + \lambda^X_1\lambda^M_{01} \lambda^M_{11})(\lambda^Y_{01} + \lambda^Y_{11})\)</span>. The numerator—the probability that <span class="math inline">\((M=1) \&amp; (Y=1) \&amp; (M \text{ causes } Y)\)</span>—is <span class="math inline">\((\lambda^X_0\lambda^M_{10} + \lambda^X_1\lambda^M_{01} \lambda^M_{11})(\lambda^Y_{01})\)</span>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>It is worth noting that the flat priors over nodal types in this chain model do <em>not</em> imply flat priors over the nodal types in a reduced <span class="math inline">\(X\rightarrow Y\)</span> model. For intuition: whereas in the simple model, flat priors imply that there is some causal effect (positive or negative) half the time, in the chain model, a causal effect occurs only if there are causal effects in <em>both</em> stages, and so, only one quarter of the time.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Such sampling is also implemented in the <code>CausalQueries</code> package.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>For example, data may be gathered through three strategies: <span class="math inline">\(S_1\)</span> in which data are gathered on nodes <span class="math inline">\(V_1\)</span> only in <span class="math inline">\(n_1\)</span> units for; <span class="math inline">\(S_2\)</span> in which data are gathered on nodes <span class="math inline">\(V_2\)</span> only in <span class="math inline">\(n_2\)</span> units; and <span class="math inline">\(S_3\)</span> in which data are gathered on nodes <span class="math inline">\(V_3\)</span> only in <span class="math inline">\(n_3\)</span> units.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Specifically, <span class="math inline">\(m_s\)</span> is a vector containing, for each strategy, <span class="math inline">\(s\)</span>, the number of observed units that are of each data type that can possibly be observed under that strategy.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>The trick, as it were, is to express integers in base-2 and then represent the integer as a series of 0’s and 1’s on multiple nodes. In base-2 counting we would represent four integer values for <span class="math inline">\(X\)</span> (say, 0, 1, 2,3) using <span class="math inline">\(00, 01, 10, 11\)</span>. If we use one binary node, <span class="math inline">\(X_1\)</span> to represent the first digit, and a second node <span class="math inline">\(X_2\)</span> to represent the second, we have enough information to capture the four values of <span class="math inline">\(X\)</span>. The mapping then is: <span class="math inline">\(X_1 = 0, X_2 = 0\)</span> represents <span class="math inline">\(X=0\)</span>; <span class="math inline">\(X_1 = 0, X_2 = 1\)</span> represents <span class="math inline">\(X=1\)</span>; <span class="math inline">\(X_1 = 1, X_2 = 0\)</span> represents <span class="math inline">\(X=2\)</span>; and <span class="math inline">\(X_1 = 1, X_2 = 1\)</span> represents <span class="math inline">\(X=3\)</span>. We construct <span class="math inline">\(Y\)</span> in the same way. We can then represent a simple <span class="math inline">\(X \rightarrow Y\)</span> relation as a model with two <span class="math inline">\(X\)</span> nodes each pointing into two <span class="math inline">\(Y\)</span> nodes: <span class="math inline">\(Y_1 \leftarrow X_1 \rightarrow Y_2, Y_1 \leftarrow X_2 \rightarrow Y_2\)</span>. To allow for the full range of nodal types we need to allow a joint distribution over <span class="math inline">\(\theta^{X_1}\)</span> and <span class="math inline">\(\theta^{X_2}\)</span> and over <span class="math inline">\(\theta^{Y_1}\)</span> and <span class="math inline">\(\theta^{Y_2}\)</span>, which results in three degrees of freedom for <span class="math inline">\(X\)</span> and 255 for <span class="math inline">\(Y\)</span>, as required.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>Importantly, this model assumes nodal types for <span class="math inline">\(Y_\text{measured[1]}\)</span> and <span class="math inline">\(Y_\text{measured[2]}\)</span> are independent of one another (no unobserved confounding), implying independent sources of measurement error in this setup.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./08-PT-application.html" class="pagination-link" aria-label="Process Tracing Applications">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Process Tracing Applications</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./10-mixed-application.html" class="pagination-link" aria-label="Integrated Inferences Applications">
        <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Integrated Inferences Applications</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>